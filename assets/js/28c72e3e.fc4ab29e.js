"use strict";(globalThis.webpackChunkbook_source=globalThis.webpackChunkbook_source||[]).push([[1968],{980:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing.summary","title":"Summary: Lesson 2 - Depth Sensing Technologies","description":"Module: Module 2 - Sensors and Perception for Humanoid Robots","source":"@site/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/02-depth-sensing.summary.md","sourceDirName":"13-Physical-AI-Humanoid-Robotics/02-sensors-perception","slug":"/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing.summary","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing.summary","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/02-depth-sensing.summary.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2: Depth Sensing Technologies","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing"},"next":{"title":"IMU & Proprioception","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception"}}');var r=s(4848),t=s(8453);const o={},l="Summary: Lesson 2 - Depth Sensing Technologies",a={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Key Concepts Covered",id:"key-concepts-covered",level:2},{value:"Depth Sensing Technologies (Section 3.1)",id:"depth-sensing-technologies-section-31",level:3},{value:"Point Cloud Data Representation (Section 3.2)",id:"point-cloud-data-representation-section-32",level:3},{value:"LiDAR Principles (Section 3.3)",id:"lidar-principles-section-33",level:3},{value:"ROS2 Messages (Section 3.4)",id:"ros2-messages-section-34",level:3},{value:"SLAM Integration (Section 3.5)",id:"slam-integration-section-35",level:3},{value:"Real-World Examples",id:"real-world-examples",level:2},{value:"Boston Dynamics Spot",id:"boston-dynamics-spot",level:3},{value:"Agility Robotics Digit",id:"agility-robotics-digit",level:3},{value:"PR-2 Robot (Willow Garage)",id:"pr-2-robot-willow-garage",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Example 1: LaserScan Obstacle Detection",id:"example-1-laserscan-obstacle-detection",level:3},{value:"Example 2: PointCloud2 Binary Data Access",id:"example-2-pointcloud2-binary-data-access",level:3},{value:"Practice Exercises",id:"practice-exercises",level:2},{value:"Common Pitfalls (Expert Insights)",id:"common-pitfalls-expert-insights",level:2},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Metadata",id:"metadata",level:2},{value:"Validation Status",id:"validation-status",level:2},{value:"Technical Corrections Applied",id:"technical-corrections-applied",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"summary-lesson-2---depth-sensing-technologies",children:"Summary: Lesson 2 - Depth Sensing Technologies"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Module"}),": Module 2 - Sensors and Perception for Humanoid Robots\r\n",(0,r.jsx)(n.strong,{children:"Lesson"}),": 02-depth-sensing.md\r\n",(0,r.jsx)(n.strong,{children:"Target Audience"}),": CS students with Python + Module 1 (ROS2) + Lesson 1 (Camera Systems) knowledge\r\n",(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 40-50 minutes\r\n",(0,r.jsx)(n.strong,{children:"Difficulty"}),": Beginner-Intermediate"]}),"\n",(0,r.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this lesson, students will be able to:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Understand"})," how depth sensing technologies measure distance and enable spatial awareness"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Apply"})," sensor_msgs/LaserScan and PointCloud2 message formats to process depth data in ROS2"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Analyze"})," trade-offs between 2D LiDAR, 3D LiDAR, and depth cameras for specific tasks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Understand"})," point cloud representation, filtering, and segmentation for 3D scene understanding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Evaluate"})," depth sensor integration with SLAM and navigation costmaps"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"key-concepts-covered",children:"Key Concepts Covered"}),"\n",(0,r.jsx)(n.h3,{id:"depth-sensing-technologies-section-31",children:"Depth Sensing Technologies (Section 3.1)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Comparison of 4 Technologies"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"2D LiDAR"}),": Planar scanning, 10-30m range, 5-40 Hz, sensor_msgs/LaserScan"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3D LiDAR"}),": Volumetric scanning, 50-100m range, 16-64 channels, sensor_msgs/PointCloud2"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Structured Light"}),": IR pattern projection, 0.5-4m range, indoor only (Kinect-style)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time-of-Flight (ToF)"}),": IR pulse timing, 0.5-10m range, moderate outdoor performance"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Trade-off Matrix"}),": Range vs Accuracy vs Cost vs Indoor/Outdoor capability"]}),"\n",(0,r.jsx)(n.h3,{id:"point-cloud-data-representation-section-32",children:"Point Cloud Data Representation (Section 3.2)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Structure"}),": Unordered collection of (x, y, z) 3D points"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Attributes"}),": Color (RGB), intensity, normal vectors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Operations"}),": Filtering, downsampling, segmentation, clustering, registration"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Coordinate Systems"}),": Cartesian (x,y,z) vs Cylindrical (r,\u03b8,z)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"lidar-principles-section-33",children:"LiDAR Principles (Section 3.3)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"2D LiDAR"}),": Single rotating laser, planar sweep, obstacle avoidance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3D LiDAR"}),": Multiple laser beams at different vertical angles, 3D mapping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time-of-Flight"}),": Speed of light \xd7 (round-trip time / 2)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hybrid Approaches"}),": Tilting 2D LiDAR for pseudo-3D coverage"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"ros2-messages-section-34",children:"ROS2 Messages (Section 3.4)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"sensor_msgs/LaserScan"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Fields: angle_min, angle_max, angle_increment, ranges[], intensities[]"}),"\n",(0,r.jsx)(n.li,{children:"Use case: 2D obstacle detection, floor-level navigation"}),"\n",(0,r.jsx)(n.li,{children:"Invalid measurements: infinity (out of range) or NaN (no return)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"sensor_msgs/PointCloud2"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Fields: header, height, width, fields[], point_step, row_step, data (binary)"}),"\n",(0,r.jsx)(n.li,{children:"Use case: 3D mapping, object segmentation, manipulation planning"}),"\n",(0,r.jsx)(n.li,{children:"Complexity: Binary format requires struct unpacking or pcl_ros tools"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"slam-integration-section-35",children:"SLAM Integration (Section 3.5)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SLAM Pipeline"}),": sensor data \u2192 feature extraction \u2192 map building \u2192 localization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Occupancy Grids"}),": 2D probabilistic map for navigation costmaps"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Loop Closure"}),": Detect revisited locations to correct drift"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"TF2 Integration"}),": Transform depth data between robot frames"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"real-world-examples",children:"Real-World Examples"}),"\n",(0,r.jsx)(n.h3,{id:"boston-dynamics-spot",children:"Boston Dynamics Spot"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor"}),": Velodyne VLP-16 (16-channel 3D LiDAR, 100m range, 300k points/sec)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Application"}),": Outdoor SLAM in GPS-denied industrial environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Key Insight"}),": 3D LiDAR's cost justified for unstructured outdoor autonomy"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"agility-robotics-digit",children:"Agility Robotics Digit"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor"}),": Intel RealSense D435i (active stereo, 1280\xd7720, 0.3-10m optimal 0.3-3m)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Application"}),": Bipedal terrain detection for stair navigation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Key Insight"}),": High-resolution close-range depth enables safe bipedal locomotion"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"pr-2-robot-willow-garage",children:"PR-2 Robot (Willow Garage)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor"}),": Microsoft Kinect v1 (structured light RGB-D, 640\xd7480, 0.4-4m)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Application"}),": Object grasping in cluttered household scenes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Key Insight"}),": RGB-depth fusion enables texture-independent manipulation"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,r.jsx)(n.h3,{id:"example-1-laserscan-obstacle-detection",children:"Example 1: LaserScan Obstacle Detection"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Functionality"}),": Subscribe to /scan, detect obstacles within 1-meter danger zone"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Key Techniques"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"numpy array conversion for efficient processing"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"np.linspace()"})," for angle generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"np.isfinite()"})," for filtering invalid measurements"]}),"\n",(0,r.jsx)(n.li,{children:"Boolean masking for danger zone identification"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lines"}),": 73 lines (comprehensive with error handling and logging)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-2-pointcloud2-binary-data-access",children:"Example 2: PointCloud2 Binary Data Access"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Functionality"}),": Subscribe to /camera/depth/points, extract x,y,z coordinates"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Key Techniques"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"struct.unpack_from('fff')"})," for binary unpacking"]}),"\n",(0,r.jsx)(n.li,{children:"Height \xd7 width calculation for total points"}),"\n",(0,r.jsx)(n.li,{children:"Error handling for empty clouds and malformed data"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lines"}),": 68 lines (production-ready with try-except blocks)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"practice-exercises",children:"Practice Exercises"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Sensor Design"}),": Design depth sensing for home-navigating humanoid (navigation + object recognition + safety)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"2D vs 3D LiDAR Analysis"}),": Compare coverage, cost, and compute for warehouse vs outdoor tasks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"AI Colearning Prompt"}),": Explore why 2D LiDAR fails to detect overhanging obstacles (ceiling beams, tree branches)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"common-pitfalls-expert-insights",children:"Common Pitfalls (Expert Insights)"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:'"More is Always Better" Fallacy'}),": 3D LiDAR isn't always better than 2D; consider compute budget, power, and task requirements"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Point Cloud Coordinate Confusion"}),": Laser scanner frame \u2260 base_link frame; always use TF2 for transformations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PointCloud2 Binary Parsing"}),": Hardcoded formats break on sensors with different field layouts; inspect msg.fields dynamically"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,r.jsx)(n.p,{children:"Students demonstrate mastery when they can:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Explain time-of-flight principles for LiDAR distance measurement"}),"\n",(0,r.jsx)(n.li,{children:"Differentiate 2D LiDAR, 3D LiDAR, structured light, and ToF by range/accuracy/environment"}),"\n",(0,r.jsx)(n.li,{children:"Subscribe to LaserScan and PointCloud2 topics with correct ROS2 patterns"}),"\n",(0,r.jsx)(n.li,{children:"Process point cloud binary data using struct or pcl_ros libraries"}),"\n",(0,r.jsx)(n.li,{children:"Design multi-sensor configurations with justified trade-offs for specific humanoid tasks"}),"\n",(0,r.jsx)(n.li,{children:"Describe SLAM pipeline integration with depth sensors and TF2"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Module 1: ROS2 Basics (nodes, topics, publishers, subscribers, message types)"}),"\n",(0,r.jsx)(n.li,{children:"Lesson 1: Camera Systems (sensor_msgs/Image, CameraInfo, camera types)"}),"\n",(0,r.jsx)(n.li,{children:"Python 3.11+ with type hints, numpy for array operations"}),"\n",(0,r.jsx)(n.li,{children:"Basic 3D coordinate systems (Cartesian coordinates, transformations)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lesson 3"}),": IMU and Proprioception (accelerometer, gyroscope, magnetometer for balance)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Connection"}),": Depth sensors provide external spatial awareness; IMUs provide internal body state awareness"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Combined"}),": Sensor fusion of cameras, depth, and IMU creates robust humanoid perception"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"metadata",children:"Metadata"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Generated by"}),": Agent Pipeline (9-agent system)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Created"}),": 2025-12-11"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tags"}),": ros2, sensors, depth-sensing, lidar, point-cloud"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cognitive Load"}),": Moderate-High (7 new concepts: 4 depth technologies, point clouds, 2 ROS2 messages, SLAM)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Word Count"}),": ~6,800 words (comprehensive coverage with 3 case studies)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sections"}),": 7 (What Is, Why Matters, Key Principles [5 subsections], Callouts [6 total], 2 Code Examples, Summary, Next Steps)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"validation-status",children:"Validation Status"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Technical Review: PASS WITH REVISIONS (RealSense tech corrected from structured light to active stereo)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Structure & Style: CONDITIONAL PASS (code examples comprehensive but exceed length guideline)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Frontmatter: COMPLETE (13 fields generated with 3 skills, 5 learning objectives)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Code Quality: PASS (type hints, docstrings, error handling, numpy operations validated)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Case Studies: 3 detailed examples (Spot 3D LiDAR, Digit active stereo, PR-2 Kinect RGB-D)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Callouts: 1 AI Colearning, 1 Expert Insight, 1 Practice Exercise, 3 Case Studies (\ud83d\udcca)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"technical-corrections-applied",children:"Technical Corrections Applied"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RealSense D435i Technology"}),': Changed from "structured light" to "active stereo" (IR pattern + stereo matching)']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Range Specification"}),': Updated to "0.3-10m range (optimal 0.3-3m)" for accurate expectations']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Outdoor Performance"}),": Clarified that active stereo performs better outdoors than traditional structured light"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>l});var i=s(6540);const r={},t=i.createContext(r);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);