"use strict";(globalThis.webpackChunkbook_source=globalThis.webpackChunkbook_source||[]).push([[8029],{2520:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"Physical-AI-Humanoid-Robotics/sensors-perception/README","title":"Module 2: Sensors and Perception for Humanoid Robots","description":"Focus: How humanoid robots sense and understand their environment","source":"@site/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/README.md","sourceDirName":"13-Physical-AI-Humanoid-Robotics/02-sensors-perception","slug":"/Physical-AI-Humanoid-Robotics/sensors-perception/","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/README.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 1 Quiz: The Robotic Nervous System (ROS 2)","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/ros2-nervous-system/quiz"},"next":{"title":"Lesson 1: Camera Systems and Computer Vision","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems"}}');var i=n(4848),r=n(8453);const t={},a="Module 2: Sensors and Perception for Humanoid Robots",c={},l=[{value:"Learning Goals",id:"learning-goals",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"Lesson 1: Camera Systems and Computer Vision",id:"lesson-1-camera-systems-and-computer-vision",level:3},{value:"Lesson 2: Depth Sensing Technologies",id:"lesson-2-depth-sensing-technologies",level:3},{value:"Lesson 3: IMU and Proprioception",id:"lesson-3-imu-and-proprioception",level:3},{value:"Lesson 4: Sensor Fusion Techniques",id:"lesson-4-sensor-fusion-techniques",level:3},{value:"Capstone Project: Multi-Sensor Perception System Design",id:"capstone-project-multi-sensor-perception-system-design",level:3},{value:"Quiz: Module 2 Assessment",id:"quiz-module-2-assessment",level:3},{value:"Learning Approach",id:"learning-approach",level:2},{value:"Hands-On Practice",id:"hands-on-practice",level:2},{value:"Success Criteria",id:"success-criteria",level:2},{value:"Next Module",id:"next-module",level:2}];function d(e){const s={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"module-2-sensors-and-perception-for-humanoid-robots",children:"Module 2: Sensors and Perception for Humanoid Robots"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Focus"}),": How humanoid robots sense and understand their environment"]}),"\n",(0,i.jsx)(s.p,{children:"This module builds on Module 1 (ROS 2 fundamentals) to explore how humanoid robots perceive the world through multiple sensor modalities. You'll learn about cameras, depth sensors, IMUs, and how to combine them through sensor fusion for robust perception."}),"\n",(0,i.jsx)(s.h2,{id:"learning-goals",children:"Learning Goals"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Understand camera systems and computer vision basics for humanoid robots"}),"\n",(0,i.jsx)(s.li,{children:"Learn LiDAR and depth sensing technologies"}),"\n",(0,i.jsx)(s.li,{children:"Master IMU (Inertial Measurement Unit) for balance and orientation"}),"\n",(0,i.jsx)(s.li,{children:"Apply sensor fusion techniques to combine multiple sensor inputs"}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Module 1 Complete"}),": Understanding of ROS 2 nodes, topics, publishers, subscribers, and message types"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Python 3.11+"}),": Object-oriented programming, type hints, imports"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Basic Linear Algebra"}),": Vectors, matrices, transformations (for sensor fusion)"]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,i.jsx)(s.h3,{id:"lesson-1-camera-systems-and-computer-vision",children:(0,i.jsx)(s.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems",children:"Lesson 1: Camera Systems and Computer Vision"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Time"}),": 30-45 minutes | ",(0,i.jsx)(s.strong,{children:"Level"}),": Beginner"]}),"\n",(0,i.jsx)(s.p,{children:"Learn how humanoid robots use cameras to perceive their environment, including camera types (monocular, stereo, RGB-D), ROS 2 Image messages, and camera placement strategies for humanoid tasks."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Key Concepts"}),": Camera types, sensor_msgs/Image, field of view, resolution"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h3,{id:"lesson-2-depth-sensing-technologies",children:(0,i.jsx)(s.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing",children:"Lesson 2: Depth Sensing Technologies"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Time"}),": 30-45 minutes | ",(0,i.jsx)(s.strong,{children:"Level"}),": Beginner-Intermediate"]}),"\n",(0,i.jsx)(s.p,{children:"Explore how humanoid robots measure distances using LiDAR, structured light cameras, and time-of-flight sensors. Understand point clouds and depth data representation in ROS 2."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Key Concepts"}),": LiDAR, depth cameras, sensor_msgs/PointCloud2, sensor_msgs/LaserScan"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h3,{id:"lesson-3-imu-and-proprioception",children:(0,i.jsx)(s.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception",children:"Lesson 3: IMU and Proprioception"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Time"}),": 30-45 minutes | ",(0,i.jsx)(s.strong,{children:"Level"}),": Intermediate"]}),"\n",(0,i.jsx)(s.p,{children:"Understand how humanoid robots maintain balance and know their body position using Inertial Measurement Units (IMUs). Learn about accelerometers, gyroscopes, magnetometers, and proprioception."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Key Concepts"}),": IMU components, sensor_msgs/Imu, drift mitigation, balance control"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h3,{id:"lesson-4-sensor-fusion-techniques",children:(0,i.jsx)(s.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion",children:"Lesson 4: Sensor Fusion Techniques"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Time"}),": 30-45 minutes | ",(0,i.jsx)(s.strong,{children:"Level"}),": Intermediate"]}),"\n",(0,i.jsx)(s.p,{children:"Discover how humanoid robots combine data from multiple sensors (cameras, LiDAR, IMU) to create robust perception. Learn Kalman filtering, complementary filtering, and visual-inertial odometry concepts."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Key Concepts"}),": Sensor fusion, Kalman filter, visual-inertial odometry, robot_localization"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h3,{id:"capstone-project-multi-sensor-perception-system-design",children:(0,i.jsx)(s.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project",children:"Capstone Project: Multi-Sensor Perception System Design"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Time"}),": 60-90 minutes | ",(0,i.jsx)(s.strong,{children:"Level"}),": Intermediate"]}),"\n",(0,i.jsx)(s.p,{children:"Apply your knowledge from all 4 lessons to design a multi-sensor perception system for a humanoid robot task. Select sensors, justify fusion strategies, and sketch ROS 2 node architecture."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Integration"}),": Cameras + Depth Sensors + IMU + Sensor Fusion"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h3,{id:"quiz-module-2-assessment",children:(0,i.jsx)(s.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/quiz",children:"Quiz: Module 2 Assessment"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Time"}),": 15-20 minutes | ",(0,i.jsx)(s.strong,{children:"Questions"}),": 15-20"]}),"\n",(0,i.jsx)(s.p,{children:"Test your understanding of sensor perception concepts across all lessons. Includes conceptual questions, code-reading exercises, and scenario-based problems."}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"learning-approach",children:"Learning Approach"}),"\n",(0,i.jsx)(s.p,{children:"Each lesson follows a consistent structure:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"\ud83d\udcac AI Colearning Prompts"}),": Explore concepts with AI assistants (Claude, ChatGPT)"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"\ud83c\udf93 Expert Insights"}),": Learn best practices and common pitfalls"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"\ud83e\udd1d Practice Exercises"}),": Apply concepts through design challenges"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Real-World Examples"}),": Case studies from Boston Dynamics, Tesla Optimus, Agility Robotics"]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"hands-on-practice",children:"Hands-On Practice"}),"\n",(0,i.jsx)(s.p,{children:"While this module focuses on conceptual understanding, you can enhance learning with:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"RViz Visualization"}),": Visualize sensor data (camera images, point clouds, IMU axes)"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Gazebo Simulation"}),": Generate synthetic sensor data without hardware"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"ROS 2 Tools"}),": Use ",(0,i.jsx)(s.code,{children:"ros2 topic echo"})," to inspect sensor messages"]}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"success-criteria",children:"Success Criteria"}),"\n",(0,i.jsx)(s.p,{children:"After completing this module, you should be able to:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"\u2705 Differentiate between monocular, stereo, and RGB-D cameras"}),"\n",(0,i.jsx)(s.li,{children:"\u2705 Explain LiDAR, structured light, and time-of-flight depth sensing"}),"\n",(0,i.jsx)(s.li,{children:"\u2705 Describe IMU components and their role in balance control"}),"\n",(0,i.jsx)(s.li,{children:"\u2705 Design a sensor fusion strategy for a humanoid robot task"}),"\n",(0,i.jsx)(s.li,{children:"\u2705 Score 80%+ on the module quiz"}),"\n"]}),"\n",(0,i.jsx)(s.h2,{id:"next-module",children:"Next Module"}),"\n",(0,i.jsx)(s.p,{children:"After mastering sensor perception, you'll be ready for:"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Module 3"}),": Motion Planning and Control (coming soon)\r\nLearn how humanoid robots plan collision-free paths and execute stable locomotion using sensor feedback."]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Estimated Total Time"}),": 3-4 hours (including capstone and quiz)\r\n",(0,i.jsx)(s.strong,{children:"Target Audience"}),": CS students with Python + ROS 2 Module 1 knowledge"]})]})}function h(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>t,x:()=>a});var o=n(6540);const i={},r=o.createContext(i);function t(e){const s=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),o.createElement(r.Provider,{value:s},e.children)}}}]);