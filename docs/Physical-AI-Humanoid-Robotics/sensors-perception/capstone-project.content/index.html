<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Module 2 Capstone Project: Integrated Sensor System Design | Physical AI &amp; Humanoid Robotics Textbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://anusbutt.github.io/hackathon-phase-01/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://anusbutt.github.io/hackathon-phase-01/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Module 2 Capstone Project: Integrated Sensor System Design | Physical AI &amp; Humanoid Robotics Textbook"><meta data-rh="true" name="description" content="Introduction"><meta data-rh="true" property="og:description" content="Introduction"><link data-rh="true" rel="icon" href="/hackathon-phase-01/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content"><link data-rh="true" rel="alternate" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content" hreflang="en"><link data-rh="true" rel="alternate" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Physical AI & Humanoid Robotics","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"},{"@type":"ListItem","position":2,"name":"Module 2: Sensors and Perception for Humanoid Robots","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"},{"@type":"ListItem","position":3,"name":"Module 2 Capstone Project: Integrated Sensor System Design","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content"}]}</script><link rel="alternate" type="application/rss+xml" href="/hackathon-phase-01/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics Textbook RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/hackathon-phase-01/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Textbook Atom Feed"><link rel="stylesheet" href="/hackathon-phase-01/assets/css/styles.5903934b.css">
<script src="/hackathon-phase-01/assets/js/runtime~main.94161a1c.js" defer="defer"></script>
<script src="/hackathon-phase-01/assets/js/main.ab534ac3.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/hackathon-phase-01/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/hackathon-phase-01/"><div class="navbar__logo"><img src="/hackathon-phase-01/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/hackathon-phase-01/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics Textbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/">Textbook</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/anusbutt/hackathon-phase-01" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"><span title="Physical AI &amp; Humanoid Robotics" class="categoryLinkLabel_W154">Physical AI &amp; Humanoid Robotics</span></a><button aria-label="Collapse sidebar category &#x27;Physical AI &amp; Humanoid Robotics&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/ros2-nervous-system/"><span title="Module 1: The Robotic Nervous System (ROS 2)" class="categoryLinkLabel_W154">Module 1: The Robotic Nervous System (ROS 2)</span></a><button aria-label="Expand sidebar category &#x27;Module 1: The Robotic Nervous System (ROS 2)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"><span title="Module 2: Sensors and Perception for Humanoid Robots" class="categoryLinkLabel_W154">Module 2: Sensors and Perception for Humanoid Robots</span></a><button aria-label="Collapse sidebar category &#x27;Module 2: Sensors and Perception for Humanoid Robots&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems"><span title="Lesson 1: Camera Systems and Computer Vision" class="linkLabel_WmDU">Lesson 1: Camera Systems and Computer Vision</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems.summary"><span title="Summary: Lesson 1 - Camera Systems and Computer Vision" class="linkLabel_WmDU">Summary: Lesson 1 - Camera Systems and Computer Vision</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing"><span title="Lesson 2: Depth Sensing Technologies" class="linkLabel_WmDU">Lesson 2: Depth Sensing Technologies</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing.summary"><span title="Summary: Lesson 2 - Depth Sensing Technologies" class="linkLabel_WmDU">Summary: Lesson 2 - Depth Sensing Technologies</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception"><span title="IMU &amp; Proprioception" class="linkLabel_WmDU">IMU &amp; Proprioception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception.summary"><span title="Summary: Lesson 3 - IMU and Proprioception" class="linkLabel_WmDU">Summary: Lesson 3 - IMU and Proprioception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion"><span title="Sensor Fusion" class="linkLabel_WmDU">Sensor Fusion</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion.summary"><span title="Lesson 4 Summary: Sensor Fusion Techniques for Humanoid Robots" class="linkLabel_WmDU">Lesson 4 Summary: Sensor Fusion Techniques for Humanoid Robots</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.architecture"><span title="Multi-Sensor Fusion Capstone: Architecture Documentation" class="linkLabel_WmDU">Multi-Sensor Fusion Capstone: Architecture Documentation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.casestudies"><span title="Case Studies: Real-World Multi-Sensor Integration" class="linkLabel_WmDU">Case Studies: Real-World Multi-Sensor Integration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content"><span title="Module 2 Capstone Project: Integrated Sensor System Design" class="linkLabel_WmDU">Module 2 Capstone Project: Integrated Sensor System Design</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project"><span title="Capstone Project: Design a Multi-Sensor Humanoid Perception System" class="linkLabel_WmDU">Capstone Project: Design a Multi-Sensor Humanoid Perception System</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.summary"><span title="Capstone Summary" class="linkLabel_WmDU">Capstone Summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/quiz"><span title="Module 2 Quiz: Sensors and Perception for Humanoid Robots" class="linkLabel_WmDU">Module 2 Quiz: Sensors and Perception for Humanoid Robots</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/isaac-ai-brain/"><span title="Module 3: The AI-Robot Brain (NVIDIA Isaac™)" class="categoryLinkLabel_W154">Module 3: The AI-Robot Brain (NVIDIA Isaac™)</span></a><button aria-label="Expand sidebar category &#x27;Module 3: The AI-Robot Brain (NVIDIA Isaac™)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/"><span title="Module 4: Vision-Language-Action (VLA)" class="categoryLinkLabel_W154">Module 4: Vision-Language-Action (VLA)</span></a><button aria-label="Expand sidebar category &#x27;Module 4: Vision-Language-Action (VLA)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/hackathon-phase-01/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"><span>Physical AI &amp; Humanoid Robotics</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"><span>Module 2: Sensors and Perception for Humanoid Robots</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Module 2 Capstone Project: Integrated Sensor System Design</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Module 2 Capstone Project: Integrated Sensor System Design</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">​</a></h2>
<p>Professional roboticists design sensor systems through iterative refinement, balancing performance, cost, reliability, and computational constraints. This capstone challenges you to design a complete perception system for a humanoid robot, integrating camera systems (Lesson 1), depth sensing (Lesson 2), IMU and proprioception (Lesson 3), and sensor fusion (Lesson 4).</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="why-a-design-document">Why a Design Document?<a href="#why-a-design-document" class="hash-link" aria-label="Direct link to Why a Design Document?" title="Direct link to Why a Design Document?" translate="no">​</a></h3>
<p>Professional robotics begins with architecture and justification before implementation. Design documents force explicit decision-making about trade-offs—you can&#x27;t hide behind &quot;I&#x27;ll try different sensors and see what works.&quot; This mirrors industry practice: technical design reviews, RFP responses, and architectural documentation all precede development.</p>
<p>Individual lessons covered sensors in isolation, but real humanoids require coordinated multi-sensor systems. Trade-offs become explicit: camera resolution vs. compute budget, LiDAR range vs. cost, IMU drift vs. fusion complexity. Your design demonstrates how fusion strategies address sensor limitations from Lessons 1-4.</p>
<p>This capstone develops design documentation skills critical for robotics engineers. Whether writing grant proposals, presenting technical reviews, or collaborating with teams, justifying decisions with evidence—not arbitrary choices—distinguishes professional engineering.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning Objectives" title="Direct link to Learning Objectives" translate="no">​</a></h3>
<p>By completing this capstone, you will:</p>
<ul>
<li class=""><strong>Apply</strong> camera, depth, IMU, and fusion concepts to realistic humanoid robotics challenges</li>
<li class=""><strong>Justify</strong> sensor selection with technical trade-offs (resolution vs. latency, range vs. accuracy, cost vs. redundancy)</li>
<li class=""><strong>Design</strong> ROS2 architectures with appropriate message types and data flow</li>
<li class=""><strong>Analyze</strong> failure modes and propose graceful degradation strategies</li>
<li class=""><strong>Communicate</strong> technical decisions through structured documentation</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="capstone-scenario-options">Capstone Scenario Options<a href="#capstone-scenario-options" class="hash-link" aria-label="Direct link to Capstone Scenario Options" title="Direct link to Capstone Scenario Options" translate="no">​</a></h2>
<p>Select <strong>one</strong> scenario for your design. Each requires applying all Module 2 concepts.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="scenario-a-indoor-home-assistant-robot">Scenario A: Indoor Home Assistant Robot<a href="#scenario-a-indoor-home-assistant-robot" class="hash-link" aria-label="Direct link to Scenario A: Indoor Home Assistant Robot" title="Direct link to Scenario A: Indoor Home Assistant Robot" translate="no">​</a></h3>
<p><strong>Scenario</strong>: Design a humanoid robot for household tasks in urban apartments. Navigate autonomously through varying lighting (bright kitchen, dim bedroom), avoid furniture/pets/humans, identify and manipulate objects (pick up books, retrieve mugs from shelves, place items in drawers), and maintain balance on different surfaces (tile, carpet, rugs) while carrying 5 kg loads.</p>
<p><strong>Constraints</strong>: Limited compute (embedded GPU or CPU-only), glass surfaces and mirrors, $5,000 sensor budget.</p>
<p><strong>Camera Systems (Lesson 1)</strong>: Object recognition for household items, visual servoing for grasping precision, landmark-based localization. <strong>Challenge</strong>: Lighting variability from sunlight to lamp lighting affects exposure and detection.</p>
<p><strong>Depth Sensing (Lesson 2)</strong>: Obstacle avoidance (0.5-5m), grasp distance measurement (0.3-2m), surface detection for floors/tables/shelves. <strong>Challenge</strong>: IR-based sensors (structured light, ToF) fail on glass/mirrors—requires stereo vision or fusion strategies.</p>
<p><strong>IMU (Lesson 3)</strong>: Balance control detecting tilt while walking/carrying, fall detection for emergency stops, motion prediction for stable gait. <strong>Challenge</strong>: Drift accumulation after 10+ minutes requires re-initialization or fusion.</p>
<p><strong>Fusion (Lesson 4)</strong>: Visual-Inertial Odometry for GPS-free indoor localization, depth+IMU fusion for gait adjustment, redundancy enabling navigation with camera failure. <strong>Challenge</strong>: Synchronizing different rates (camera 15 Hz, IMU 100 Hz, depth 30 Hz) using <code>message_filters</code>.</p>
<p><strong>Success</strong>: Sensor suite enables detection, distance measurement, balance, localization within $5,000. VIO fusion for drift-free navigation. Address camera glare, depth failures on glass, IMU drift. ROS2 uses <code>sensor_msgs/Image</code>, <code>PointCloud2</code>, <code>Imu</code>.</p>
<hr>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="scenario-b-outdoor-delivery-robot">Scenario B: Outdoor Delivery Robot<a href="#scenario-b-outdoor-delivery-robot" class="hash-link" aria-label="Direct link to Scenario B: Outdoor Delivery Robot" title="Direct link to Scenario B: Outdoor Delivery Robot" translate="no">​</a></h3>
<p><strong>Scenario</strong>: Design a humanoid for package delivery on university campuses and office parks. Walk on uneven terrain (grass, gravel, curbs, stairs), avoid dynamic obstacles (pedestrians, bicyclists, vehicles), navigate 2 km trips using GPS-aided localization, operate in variable weather (sunny, cloudy, light rain) during daylight, and carry 15 kg packages while maintaining stability.</p>
<p><strong>Constraints</strong>: Outdoor lighting (direct sunlight, shadows), long-range detection (5-30m), battery-limited compute, $10,000 budget.</p>
<p><strong>Camera Systems (Lesson 1)</strong>: Semantic navigation (sidewalks, crosswalks, building entrances), visual landmark recognition, texture analysis for curb detection. <strong>Challenge</strong>: Direct sunlight causes overexposure; shadows create high contrast requiring HDR or exposure bracketing.</p>
<p><strong>Depth Sensing (Lesson 2)</strong>: Long-range obstacle detection (5-30m) for reaction time, terrain mapping for footstep planning, curb/stair detection for gait adaptation. <strong>Challenge</strong>: Sunlight interferes with IR-based sensors—requires LiDAR for outdoor reliability (higher cost/power).</p>
<p><strong>IMU (Lesson 3)</strong>: Tilt detection on slopes for posture adjustment, vibration damping from uneven surfaces, package load sensing for center-of-mass shifts. <strong>Challenge</strong>: Magnetic interference near buildings affects magnetometer calibration.</p>
<p><strong>Fusion (Lesson 4)</strong>: LiDAR+IMU SLAM for outdoor localization, GPS (10m accuracy) + visual odometry (cm-level relative) for global+local precision, multi-modal obstacle fusion merging LiDAR and stereo depth. <strong>Challenge</strong>: GPS dropout in tree cover requires seamless VIO fallback.</p>
<p><strong>Success</strong>: Handle outdoor lighting, long-range obstacles, uneven terrain, loads within $10,000. Justify LiDAR over RGB-D using sunlight limitations from Lesson 2. Fusion uses LiDAR+IMU SLAM with GPS fallback. Address GPS dropout, sun glare, magnetic interference. ROS2 includes <code>LaserScan</code> or <code>PointCloud2</code>, <code>NavSatFix</code>, <code>Imu</code>.</p>
<hr>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="scenario-c-human-interaction-robot-healthcare">Scenario C: Human Interaction Robot (Healthcare)<a href="#scenario-c-human-interaction-robot-healthcare" class="hash-link" aria-label="Direct link to Scenario C: Human Interaction Robot (Healthcare)" title="Direct link to Scenario C: Human Interaction Robot (Healthcare)" translate="no">​</a></h3>
<p><strong>Scenario</strong>: Design a humanoid for healthcare settings (hospitals, nursing homes). Approach humans safely without startling (0.5-1.5m social distance), track faces/gestures for non-verbal communication and eye contact, hand off objects (medication, water bottles) safely, maintain stable head orientation for natural gaze, and detect human distress (falls, sudden movements) to alert caregivers.</p>
<p><strong>Constraints</strong>: Human safety paramount (zero collision tolerance), privacy (minimal data storage), indoor only, $7,500 budget.</p>
<p><strong>Camera Systems (Lesson 1)</strong>: Face detection/tracking for gaze direction, gesture recognition (waving, pointing, reaching), visual servoing for object handoff. <strong>Challenge</strong>: Privacy requires edge-only processing without cloud storage, impacting compute and algorithm selection.</p>
<p><strong>Depth Sensing (Lesson 2)</strong>: Safe approach distance measurement (0.5-3m) for personal space, 3D person tracking combining RGB face detection with depth, obstacle detection for medical equipment (IV poles, wheelchairs). <strong>Challenge</strong>: Wheelchair users/children require lower sensor placement—chest-mounted depth may miss low obstacles.</p>
<p><strong>IMU (Lesson 3)</strong>: Stable head orientation keeping head level during walking for natural gaze, gentle motion control with smooth acceleration to avoid startling, collision reaction detecting unexpected contact via accelerometer spikes. <strong>Challenge</strong>: Distinguish intentional touch (patting) from collisions.</p>
<p><strong>Fusion (Lesson 4)</strong>: Camera+depth 3D person tracking, IMU+vision head stabilization using IMU to predict head motion and compensate camera tracking, multi-sensor safety layer with redundant human detection. <strong>Challenge</strong>: Synchronize camera face detection (15 Hz) with depth (30 Hz) using <code>message_filters</code>.</p>
<p><strong>Success</strong>: Enable face tracking, safe distance, stable head, gesture recognition within $7,500. Fusion uses camera+depth 3D tracking with IMU head stabilization. Address privacy via edge processing and no persistent storage. Failure modes: camera occlusion, depth close-range limits, redundant human detection. ROS2 shows <code>ApproximateTimeSynchronizer</code> for camera+depth sync.</p>
<hr>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="scenario-d-warehouse-logistics-robot">Scenario D: Warehouse Logistics Robot<a href="#scenario-d-warehouse-logistics-robot" class="hash-link" aria-label="Direct link to Scenario D: Warehouse Logistics Robot" title="Direct link to Scenario D: Warehouse Logistics Robot" translate="no">​</a></h3>
<p><strong>Scenario</strong>: Design a humanoid for high-speed warehouse logistics (Amazon fulfillment centers). Navigate aisles at 3 m/s avoiding forklifts/workers/robots, identify shelf locations using barcode/QR scanning, pick items from floor to 2.5m overhead, track rapid acceleration during start/stop cycles for balance, and operate in structured environments with known floor plans but dynamic obstacles.</p>
<p><strong>Constraints</strong>: High-speed requires low latency (&lt;50 ms), precise localization (&lt;5 cm error), harsh fluorescent lighting, $12,000 budget.</p>
<p><strong>Camera Systems (Lesson 1)</strong>: High-resolution (1080p+) barcode/QR scanning at 0.5-2m, multi-camera visual odometry for precise localization, shelf edge detection for reaching. <strong>Challenge</strong>: Overhead fluorescent flicker (60 Hz) affects exposure—global shutter cameras may be needed for fast motion.</p>
<p><strong>Depth Sensing (Lesson 2)</strong>: High-speed obstacle detection via LiDAR with &lt;50 ms latency for safe 3 m/s navigation, precise shelf localization via 3D mapping (±2 cm accuracy), item dimension measurement. <strong>Challenge</strong>: Fast motion causes depth camera motion blur—LiDAR less affected.</p>
<p><strong>IMU (Lesson 3)</strong>: Rapid acceleration tracking (200+ Hz) for sudden starts/stops, lateral acceleration detection for high-speed turns and dynamic stability, vibration isolation filtering concrete floor vibration. <strong>Challenge</strong>: High update rate (200 Hz) generates significant data throughput (bandwidth constraint).</p>
<p><strong>Fusion (Lesson 4)</strong>: Multi-camera visual odometry fusing 3+ cameras for redundant drift-free localization, LiDAR+Visual SLAM combining structure with features (LOAM, LIO-SAM), IMU-aided camera stabilization using IMU prediction to compensate motion blur. <strong>Challenge</strong>: Fuse high-rate IMU (200 Hz) with lower-rate cameras (30 Hz) via predictive filtering.</p>
<p><strong>Success</strong>: Enable high-speed navigation, barcode scanning, precise shelf localization, rapid acceleration tracking within $12,000. Fusion uses multi-camera visual odometry with LiDAR and IMU. Latency analysis shows &lt;50 ms obstacle detection pipeline. Failure modes: camera motion blur, LiDAR occlusion by forklifts, IMU saturation during emergency stops. ROS2 addresses data throughput via QoS settings (best-effort for cameras, reliable for critical control), compression, prioritization.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="design-requirements">Design Requirements<a href="#design-requirements" class="hash-link" aria-label="Direct link to Design Requirements" title="Direct link to Design Requirements" translate="no">​</a></h2>
<p>Your design document must address the following:</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="sensor-selection-and-justification">Sensor Selection and Justification<a href="#sensor-selection-and-justification" class="hash-link" aria-label="Direct link to Sensor Selection and Justification" title="Direct link to Sensor Selection and Justification" translate="no">​</a></h3>
<p><strong>Camera System</strong>: Specify type (monocular RGB, stereo, RGB-D, combination), resolution (640×480 to 1920×1080), frame rate (15-60 FPS), field of view (60-120°). Example models: Intel RealSense D435 (RGB-D 640×480), ZED 2 (stereo 1080p), generic USB webcam. <strong>Justification must reference Lesson 1 trade-offs</strong>. Example: &quot;Selected 640×480 over 1080p because navigation requires only basic shape recognition, and lower resolution reduces latency from 150ms to 50ms on embedded GPU.&quot;</p>
<p><strong>Depth Sensor</strong>: Specify technology (LiDAR mechanical/solid-state, structured light RGB-D, ToF), range (0.5-30m), accuracy (±2 to ±10 cm), update rate (10-100 Hz). Examples: Velodyne Puck (mechanical LiDAR, 100m), Intel RealSense L515 (solid-state LiDAR, 9m), Azure Kinect (ToF, 5.5m). <strong>Justify using Lesson 2 sensor limitations</strong>. Example: &quot;Selected LiDAR over RGB-D for outdoor scenario because structured light/ToF fail in sunlight due to IR interference (Lesson 2). Though LiDAR costs $8,000 vs. $400 for RGB-D, outdoor reliability justifies expense.&quot;</p>
<p><strong>IMU</strong>: Specify update rate (50-200 Hz), accelerometer range (±2g to ±16g), gyroscope range (±250°/s to ±2000°/s). Examples: MPU-9250 (9-axis, 100 Hz, $15), VectorNav VN-100 (200 Hz, $2,000), Xsens MTi ($1,500). <strong>Justify using Lesson 3 trade-offs</strong>. Example: &quot;Selected 200 Hz IMU for warehouse to track rapid acceleration during emergency stops at 3 m/s. Lower 50 Hz would miss acceleration peaks &lt;20ms.&quot;</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="sensor-placement-and-coverage">Sensor Placement and Coverage<a href="#sensor-placement-and-coverage" class="hash-link" aria-label="Direct link to Sensor Placement and Coverage" title="Direct link to Sensor Placement and Coverage" translate="no">​</a></h3>
<p>Diagram sensor placement: <strong>Head-mounted</strong> cameras for navigation/face tracking with pan/tilt; <strong>chest-mounted</strong> LiDAR/depth for stable SLAM with minimal arm occlusion; <strong>wrist-mounted</strong> cameras for manipulation visual servoing; <strong>torso IMU</strong> for whole-body balance. Show FOV cones, identify coverage gaps (e.g., &quot;blind spot behind robot, mitigated by 180° rotation before backing&quot;), demonstrate redundancy (overlapping camera+LiDAR FOV forward). Hand-drawn diagrams acceptable if legible.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="ros2-integration-architecture">ROS2 Integration Architecture<a href="#ros2-integration-architecture" class="hash-link" aria-label="Direct link to ROS2 Integration Architecture" title="Direct link to ROS2 Integration Architecture" translate="no">​</a></h3>
<p>Specify node structure: <strong>Sensor drivers</strong> (<code>camera_driver_node</code>, <code>lidar_driver_node</code>, <code>imu_driver_node</code>) publish raw data. <strong>Processing nodes</strong> (<code>object_detector_node</code>, <code>slam_node</code>, <code>balance_controller_node</code>) subscribe and publish processed outputs. <strong>Fusion node</strong> (<code>sensor_fusion_node</code>) subscribes to multiple sensors, publishes fused state estimates.</p>
<p><strong>Topics and message types</strong>: Camera <code>/camera/image_raw</code> (<code>sensor_msgs/Image</code>), <code>/camera/camera_info</code> (<code>sensor_msgs/CameraInfo</code>); depth <code>/depth/points</code> (<code>sensor_msgs/PointCloud2</code>) or <code>/scan</code> (<code>sensor_msgs/LaserScan</code>); IMU <code>/imu/data</code> (<code>sensor_msgs/Imu</code>); fused <code>/odometry/filtered</code> (<code>nav_msgs/Odometry</code>) from <code>robot_localization</code>.</p>
<p><strong>QoS policies</strong>: Cameras use best-effort for real-time priority; queue depth 5-10 for high-rate sensors, 1-2 for control commands; critical safety uses reliable QoS.</p>
<p>Include diagram (hand-drawn or digital) showing nodes, topics, message types with data flow arrows.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="sensor-fusion-strategy">Sensor Fusion Strategy<a href="#sensor-fusion-strategy" class="hash-link" aria-label="Direct link to Sensor Fusion Strategy" title="Direct link to Sensor Fusion Strategy" translate="no">​</a></h3>
<p><strong>Algorithm selection</strong>: Complementary Filter (IMU + low-rate orientation, simple/low-compute), Extended Kalman Filter (camera + IMU VIO, nonlinear motion), <code>robot_localization</code> (pre-built EKF for IMU/odometry/GPS), VIO algorithms (ORB-SLAM, VINS-Mono for camera+IMU), LiDAR+IMU SLAM (LOAM, LIO-SAM for outdoor 3D mapping).</p>
<p><strong>State variables</strong>: Position (x, y, z), orientation (roll, pitch, yaw), linear velocity (vx, vy, vz), angular velocity (ωx, ωy, ωz).</p>
<p><strong>Sensor contributions</strong>: Example: &quot;Camera provides position via visual odometry at 15 Hz; IMU provides orientation and angular velocity at 100 Hz; depth provides obstacle constraints preventing position drift into walls; fusion uses EKF with IMU prediction and camera correction.&quot;</p>
<p>Include fusion block diagram: sensor inputs → fusion algorithm (with rates) → state estimate → consuming nodes.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="failure-modes-and-graceful-degradation">Failure Modes and Graceful Degradation<a href="#failure-modes-and-graceful-degradation" class="hash-link" aria-label="Direct link to Failure Modes and Graceful Degradation" title="Direct link to Failure Modes and Graceful Degradation" translate="no">​</a></h3>
<p>Analyze 3-5 scenarios:</p>
<p><strong>Camera Failure</strong> (lens obscured, sun glare): Detection via no new messages &gt;1 sec or &gt;95% pixels saturated. Degradation: fall back to depth+IMU for obstacle avoidance and dead reckoning, disable object recognition. Impact: cannot identify objects/landmarks, navigation limited to mapped areas.</p>
<p><strong>Depth Sensor Failure</strong> (glass false readings, hardware failure): Detection via all NaN values in PointCloud2 or implausible ranges (&lt;0.1m away from walls). Degradation: use stereo vision for depth (if available) or rely on IMU+known map. Impact: reduced obstacle detection range, cannot safely approach unknown objects.</p>
<p><strong>IMU Drift</strong> (bias accumulation &gt;10 min): Detection via orientation divergence from visual horizon or gravity by &gt;5°. Degradation: re-initialize IMU with camera-based horizon or magnetometer. Impact: temporary disorientation 1-2 sec during re-init, reduced balance accuracy.</p>
<p><strong>Sensor Sync Failure</strong> (camera and depth timestamps differ &gt;100 ms): Detection via <code>ApproximateTimeSynchronizer</code> failure to match messages. Degradation: use sensors independently or increase slop from 50ms to 100ms. Impact: reduced fusion accuracy, potential camera-depth mismatch causing incorrect 3D positions.</p>
<p><strong>Complete Suite Failure</strong> (safety-critical): Detection via all sensor topics silent &gt;2 sec. Degradation: emergency stop, audible alarm, await manual intervention. Impact: cannot operate autonomously, requires human recovery.</p>
<p>Present as table: Failure Mode | Detection Method | Degradation Strategy | Performance Impact.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="performance-criteria">Performance Criteria<a href="#performance-criteria" class="hash-link" aria-label="Direct link to Performance Criteria" title="Direct link to Performance Criteria" translate="no">​</a></h3>
<p><strong>Latency</strong>: Define sensor-to-decision budget. Example: &quot;Obstacle detection to path replanning &lt;200 ms: camera capture 30 ms + network 10 ms + processing 100 ms + planning 60 ms.&quot;</p>
<p><strong>Accuracy</strong>: Set localization tolerances. Example: &quot;±5 cm for warehouse shelf alignment, ±50 cm for outdoor waypoints. Depth ±2 cm at 1m for grasping.&quot;</p>
<p><strong>Update Rates</strong>: Minimum sensor rates. Example: &quot;Camera 15 Hz for object recognition, 30 Hz for visual odometry; IMU 100 Hz for balance; depth 30 Hz for obstacles.&quot;</p>
<p><strong>Compute Cost</strong>: Estimate load. Example: &quot;Stereo depth 30% GPU, object detection 40% GPU, fusion 10% CPU = 80% total, 20% margin. If exceeded, reduce camera resolution 1080p→720p, saving 15% GPU.&quot;</p>
<p><strong>Cost Budget</strong>: Break down total. Example: &quot;LiDAR $8,000 + cameras $1,500 + IMU $500 = $10,000 total, meeting outdoor scenario budget.&quot;</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="deliverables">Deliverables<a href="#deliverables" class="hash-link" aria-label="Direct link to Deliverables" title="Direct link to Deliverables" translate="no">​</a></h2>
<p>Submit a design document (PDF or Markdown, 2-3 pages excluding diagrams) with:</p>
<p><strong>1. Scenario Selection</strong> (0.25 pages): Which scenario (A/B/C/D), why it interests you, brief overview of robot&#x27;s primary task.</p>
<p><strong>2. Sensor Specifications</strong> (0.5 pages): Table with Sensor Type | Model Example | Specifications | Placement | Update Rate | Cost. Justification paragraphs for each sensor referencing Lessons 1-4 trade-offs.</p>
<p><strong>3. ROS2 Node Architecture</strong> (0.5 pages): Visual diagram (hand-drawn or digital) with node/topic/message type labels. 1-2 paragraphs describing data flow from drivers through processing to fusion.</p>
<p><strong>4. Sensor Fusion Strategy</strong> (0.5 pages): Algorithm selection with rationale. Block diagram showing sensor inputs → fusion → state outputs. 2-3 paragraphs explaining which sensors contribute to which state variables, referencing Lesson 4.</p>
<p><strong>5. Failure Mode Analysis</strong> (0.5 pages): Table with 3-5 scenarios (Failure Mode | Detection | Degradation | Impact). 1 paragraph discussing most critical failure and mitigation.</p>
<p><strong>6. Trade-off Justifications</strong> (0.25 pages): Summarize 2-3 major design trade-offs. Example: &quot;Chose 640×480 over 1080p to reduce latency 150ms→50ms, accepting reduced detail for real-time performance.&quot;</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="optional-code-sketch">Optional Code Sketch<a href="#optional-code-sketch" class="hash-link" aria-label="Direct link to Optional Code Sketch" title="Direct link to Optional Code Sketch" translate="no">​</a></h3>
<p>Not required to run, but may include ROS2 pseudocode showing subscription setup, <code>message_filters</code> synchronization, fusion logic structure (commented outline, no full implementation). This demonstrates ROS2 integration understanding from Module 2.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="submission-guidelines">Submission Guidelines<a href="#submission-guidelines" class="hash-link" aria-label="Direct link to Submission Guidelines" title="Direct link to Submission Guidelines" translate="no">​</a></h3>
<p><strong>File Naming</strong>: <code>module2_capstone_[YourName].pdf</code> or <code>.md</code></p>
<p><strong>Submission</strong>: Upload to course platform per instructor.</p>
<p><strong>Academic Integrity</strong>: Discuss scenarios and approaches with classmates, but sensor selection and architecture must be individual work. Use AI tools (Claude, ChatGPT) for brainstorming and review—encouraged. Copying designs from online sources without attribution violates academic integrity.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="resources">Resources<a href="#resources" class="hash-link" aria-label="Direct link to Resources" title="Direct link to Resources" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="module-2-lessons">Module 2 Lessons<a href="#module-2-lessons" class="hash-link" aria-label="Direct link to Module 2 Lessons" title="Direct link to Module 2 Lessons" translate="no">​</a></h3>
<ul>
<li class=""><a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/01-camera-systems">Lesson 1: Camera Systems</a> - Camera types, resolution/frame rate trade-offs, FOV, <code>sensor_msgs/Image</code>, <code>CameraInfo</code></li>
<li class=""><a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/02-depth-sensing">Lesson 2: Depth Sensing</a> - LiDAR, structured light, ToF, <code>PointCloud2</code>, <code>LaserScan</code>, outdoor vs. indoor, sunlight interference</li>
<li class=""><a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/03-imu-proprioception">Lesson 3: IMU and Proprioception</a> - Accelerometer, gyroscope, magnetometer, <code>sensor_msgs/Imu</code>, balance, drift, calibration</li>
<li class=""><a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/04-sensor-fusion">Lesson 4: Sensor Fusion</a> - Complementary filters, Kalman filters, VIO, <code>robot_localization</code>, multi-sensor integration</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="case-studies">Case Studies<a href="#case-studies" class="hash-link" aria-label="Direct link to Case Studies" title="Direct link to Case Studies" translate="no">​</a></h3>
<ul>
<li class=""><strong>Boston Dynamics Atlas</strong>: Multi-sensor SLAM with LiDAR + stereo + IMU for disaster response</li>
<li class=""><strong>Agility Robotics Digit</strong>: VIO using stereo + IMU for GPS-free warehouse navigation</li>
<li class=""><strong>Tesla Optimus</strong>: Multi-camera fusion (8+ cameras) without LiDAR, vision-only perception</li>
<li class=""><strong>ANYbotics ANYmal</strong>: LiDAR + stereo + IMU for outdoor legged navigation on rough terrain</li>
</ul>
<p>Use as conceptual references, not blueprints. Tailor designs to your scenario constraints.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="ros2-documentation">ROS2 Documentation<a href="#ros2-documentation" class="hash-link" aria-label="Direct link to ROS2 Documentation" title="Direct link to ROS2 Documentation" translate="no">​</a></h3>
<ul>
<li class=""><strong>robot_localization</strong>: <a href="http://docs.ros.org/en/humble/p/robot_localization/" target="_blank" rel="noopener noreferrer" class="">ROS2 Humble docs</a> - EKF for multi-sensor fusion</li>
<li class=""><strong>message_filters</strong>: <a href="http://docs.ros.org/en/humble/p/message_filters/" target="_blank" rel="noopener noreferrer" class="">ApproximateTimeSynchronizer</a> - Timestamp matching for camera+depth+IMU</li>
<li class=""><strong>sensor_msgs</strong>: <a href="http://docs.ros.org/en/humble/p/sensor_msgs/" target="_blank" rel="noopener noreferrer" class="">Message definitions</a> - <code>Image</code>, <code>PointCloud2</code>, <code>LaserScan</code>, <code>Imu</code>, <code>CameraInfo</code>, <code>NavSatFix</code></li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="design-tools">Design Tools<a href="#design-tools" class="hash-link" aria-label="Direct link to Design Tools" title="Direct link to Design Tools" translate="no">​</a></h3>
<ul>
<li class=""><strong>draw.io</strong> (diagrams.net): Free web-based diagramming with ROS2 shapes</li>
<li class=""><strong>ROS2 rqt_graph</strong>: Generates real-time ROS2 architecture diagrams (if ROS2 installed)</li>
<li class=""><strong>Lucidchart</strong>: Web-based diagramming (free tier available)</li>
<li class=""><strong>Paper and pencil</strong>: Hand-drawn diagrams fully acceptable if legible</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="expert-tips">Expert Tips<a href="#expert-tips" class="hash-link" aria-label="Direct link to Expert Tips" title="Direct link to Expert Tips" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="ai-colearning-prompt-1-scenario-exploration">AI Colearning Prompt 1: Scenario Exploration<a href="#ai-colearning-prompt-1-scenario-exploration" class="hash-link" aria-label="Direct link to AI Colearning Prompt 1: Scenario Exploration" title="Direct link to AI Colearning Prompt 1: Scenario Exploration" translate="no">​</a></h3>
<p>Before starting, ask your AI assistant:</p>
<blockquote>
<p>&quot;I&#x27;m designing a sensor system for [chosen scenario]. Compare trade-offs between:</p>
<ol>
<li class="">Stereo cameras + IMU (passive depth, lower cost)</li>
<li class="">LiDAR + monocular camera + IMU (active depth, higher cost)</li>
</ol>
<p>Consider: indoor vs. outdoor, sunlight interference, compute requirements, cost. Which would you recommend and why?&quot;</p>
</blockquote>
<p><strong>Learning outcome</strong>: Explore sensor trade-offs specific to your scenario. Compare AI reasoning with Lessons 1-2 concepts to validate understanding.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="ai-colearning-prompt-2-fusion-strategy-validation">AI Colearning Prompt 2: Fusion Strategy Validation<a href="#ai-colearning-prompt-2-fusion-strategy-validation" class="hash-link" aria-label="Direct link to AI Colearning Prompt 2: Fusion Strategy Validation" title="Direct link to AI Colearning Prompt 2: Fusion Strategy Validation" translate="no">​</a></h3>
<p>After drafting fusion strategy, ask:</p>
<blockquote>
<p>&quot;I&#x27;m using [fusion algorithm] to combine [sensors]. My state variables are [position, orientation, velocity, etc.]. Which sensors should contribute to which state variables, and why? Are there nonsensical sensor combinations?&quot;</p>
</blockquote>
<p><strong>Learning outcome</strong>: Validate that fusion matches sensor characteristics. Cameras don&#x27;t directly measure velocity (estimate via optical flow). IMUs measure acceleration/angular velocity, not position (requires integration). Understanding these prevents design mistakes.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="expert-insight-real-world-iteration">Expert Insight: Real-World Iteration<a href="#expert-insight-real-world-iteration" class="hash-link" aria-label="Direct link to Expert Insight: Real-World Iteration" title="Direct link to Expert Insight: Real-World Iteration" translate="no">​</a></h3>
<p><strong>Sensor Systems Are Never Perfect on First Try</strong></p>
<p>Professional teams iterate sensor configurations 5-10 times before finalizing. Boston Dynamics&#x27; Atlas went through multiple camera/LiDAR combinations over years. Tesla Optimus initially tested LiDAR before switching to camera-only for cost and form factor.</p>
<p>Your capstone design won&#x27;t be perfect—that&#x27;s expected. Demonstrate:</p>
<ol>
<li class=""><strong>Explicit reasoning</strong>: Why sensor X over Y? What trade-off did you prioritize (cost vs. accuracy, latency vs. resolution)?</li>
<li class=""><strong>Awareness of limitations</strong>: What fails in your design? How do you mitigate with fusion or degradation?</li>
<li class=""><strong>Realistic constraints</strong>: Cost, compute, latency budgets force hard choices—acknowledging and working within constraints is core engineering.</li>
</ol>
<p>Grading prioritizes justification quality over &quot;optimal&quot; selection. Two students can choose different sensors for the same scenario and both earn full marks if reasoning is sound and grounded in Module 2.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="practice-exercise-ros2-message-synchronization">Practice Exercise: ROS2 Message Synchronization<a href="#practice-exercise-ros2-message-synchronization" class="hash-link" aria-label="Direct link to Practice Exercise: ROS2 Message Synchronization" title="Direct link to Practice Exercise: ROS2 Message Synchronization" translate="no">​</a></h3>
<p>Challenge: Sketch <code>message_filters.ApproximateTimeSynchronizer</code> setup for your sensor suite.</p>
<p>Consider:</p>
<ol>
<li class=""><strong>Which sensors need synchronization?</strong> (e.g., camera + depth for 3D object detection, camera + IMU for VIO)</li>
<li class=""><strong>What slop parameter?</strong> (Maximum time difference for &quot;synchronized&quot; messages). Example: Camera 15 Hz (66 ms period), IMU 100 Hz (10 ms period) → slop ~50 ms allows 1-2 camera frame drift.</li>
<li class=""><strong>What if synchronization fails?</strong> (e.g., one sensor stops publishing due to hardware failure)</li>
</ol>
<p>Pseudocode template:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> message_filters </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> ApproximateTimeSynchronizer</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> Subscriber</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sync </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> ApproximateTimeSynchronizer</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">camera_sub</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> depth_sub</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> imu_sub</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    queue_size</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">10</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    slop</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0.05</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># 50 ms tolerance</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sync</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">registerCallback</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">fusion_callback</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p><strong>Optional</strong>: Ask Claude to review your slop parameter choice and suggest improvements based on sensor update rates and scenario latency requirements.</p>
<hr>
<p><strong>Good luck with your capstone design! Focus on clear justifications, realistic trade-offs, and thoughtful failure analysis. Your design documentation skills will serve you throughout your robotics career.</strong></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/05-capstone-project.content.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.casestudies"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Case Studies: Real-World Multi-Sensor Integration</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Capstone Project: Design a Multi-Sensor Humanoid Perception System</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a><ul><li><a href="#why-a-design-document" class="table-of-contents__link toc-highlight">Why a Design Document?</a></li><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning Objectives</a></li></ul></li><li><a href="#capstone-scenario-options" class="table-of-contents__link toc-highlight">Capstone Scenario Options</a><ul><li><a href="#scenario-a-indoor-home-assistant-robot" class="table-of-contents__link toc-highlight">Scenario A: Indoor Home Assistant Robot</a></li><li><a href="#scenario-b-outdoor-delivery-robot" class="table-of-contents__link toc-highlight">Scenario B: Outdoor Delivery Robot</a></li><li><a href="#scenario-c-human-interaction-robot-healthcare" class="table-of-contents__link toc-highlight">Scenario C: Human Interaction Robot (Healthcare)</a></li><li><a href="#scenario-d-warehouse-logistics-robot" class="table-of-contents__link toc-highlight">Scenario D: Warehouse Logistics Robot</a></li></ul></li><li><a href="#design-requirements" class="table-of-contents__link toc-highlight">Design Requirements</a><ul><li><a href="#sensor-selection-and-justification" class="table-of-contents__link toc-highlight">Sensor Selection and Justification</a></li><li><a href="#sensor-placement-and-coverage" class="table-of-contents__link toc-highlight">Sensor Placement and Coverage</a></li><li><a href="#ros2-integration-architecture" class="table-of-contents__link toc-highlight">ROS2 Integration Architecture</a></li><li><a href="#sensor-fusion-strategy" class="table-of-contents__link toc-highlight">Sensor Fusion Strategy</a></li><li><a href="#failure-modes-and-graceful-degradation" class="table-of-contents__link toc-highlight">Failure Modes and Graceful Degradation</a></li><li><a href="#performance-criteria" class="table-of-contents__link toc-highlight">Performance Criteria</a></li></ul></li><li><a href="#deliverables" class="table-of-contents__link toc-highlight">Deliverables</a><ul><li><a href="#optional-code-sketch" class="table-of-contents__link toc-highlight">Optional Code Sketch</a></li><li><a href="#submission-guidelines" class="table-of-contents__link toc-highlight">Submission Guidelines</a></li></ul></li><li><a href="#resources" class="table-of-contents__link toc-highlight">Resources</a><ul><li><a href="#module-2-lessons" class="table-of-contents__link toc-highlight">Module 2 Lessons</a></li><li><a href="#case-studies" class="table-of-contents__link toc-highlight">Case Studies</a></li><li><a href="#ros2-documentation" class="table-of-contents__link toc-highlight">ROS2 Documentation</a></li><li><a href="#design-tools" class="table-of-contents__link toc-highlight">Design Tools</a></li></ul></li><li><a href="#expert-tips" class="table-of-contents__link toc-highlight">Expert Tips</a><ul><li><a href="#ai-colearning-prompt-1-scenario-exploration" class="table-of-contents__link toc-highlight">AI Colearning Prompt 1: Scenario Exploration</a></li><li><a href="#ai-colearning-prompt-2-fusion-strategy-validation" class="table-of-contents__link toc-highlight">AI Colearning Prompt 2: Fusion Strategy Validation</a></li><li><a href="#expert-insight-real-world-iteration" class="table-of-contents__link toc-highlight">Expert Insight: Real-World Iteration</a></li><li><a href="#practice-exercise-ros2-message-synchronization" class="table-of-contents__link toc-highlight">Practice Exercise: ROS2 Message Synchronization</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Learn</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/">Textbook</a></li><li class="footer__item"><a class="footer__link-item" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/ros2-nervous-system/">Module 1: ROS2</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/anusbutt/hackathon-phase-01" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://linkedin.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">X (Twitter)<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://instagram.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://panaversity.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Panaversity<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Physical AI & Humanoid Robotics Textbook. Built for Panaversity Hackathon.</div></div></div></footer><button class="floatingChatButton__Uyc" aria-label="Open AI Assistant" title="Ask questions about Physical AI &amp; Humanoid Robotics"><span class="chatIcon_GGtK">💬</span><span class="chatLabel_InO3">AI Assistant</span></button></div>
</body>
</html>