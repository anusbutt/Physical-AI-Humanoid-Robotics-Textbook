<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Capstone Project: Design a Multi-Sensor Humanoid Perception System | Physical AI &amp; Humanoid Robotics Textbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://anusbutt.github.io/hackathon-phase-01/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://anusbutt.github.io/hackathon-phase-01/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Capstone Project: Design a Multi-Sensor Humanoid Perception System | Physical AI &amp; Humanoid Robotics Textbook"><meta data-rh="true" name="description" content="Integrative project combining camera systems, depth sensing, IMU integration, and sensor fusion to design a complete perception system for a humanoid warehouse robot."><meta data-rh="true" property="og:description" content="Integrative project combining camera systems, depth sensing, IMU integration, and sensor fusion to design a complete perception system for a humanoid warehouse robot."><link data-rh="true" rel="icon" href="/hackathon-phase-01/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project"><link data-rh="true" rel="alternate" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project" hreflang="en"><link data-rh="true" rel="alternate" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Physical AI & Humanoid Robotics","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"},{"@type":"ListItem","position":2,"name":"Module 2: Sensors and Perception for Humanoid Robots","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"},{"@type":"ListItem","position":3,"name":"Capstone Project: Design a Multi-Sensor Humanoid Perception System","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project"}]}</script><link rel="alternate" type="application/rss+xml" href="/hackathon-phase-01/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics Textbook RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/hackathon-phase-01/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Textbook Atom Feed"><link rel="stylesheet" href="/hackathon-phase-01/assets/css/styles.5903934b.css">
<script src="/hackathon-phase-01/assets/js/runtime~main.94161a1c.js" defer="defer"></script>
<script src="/hackathon-phase-01/assets/js/main.ab534ac3.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/hackathon-phase-01/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/hackathon-phase-01/"><div class="navbar__logo"><img src="/hackathon-phase-01/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/hackathon-phase-01/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics Textbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/">Textbook</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/anusbutt/hackathon-phase-01" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"><span title="Physical AI &amp; Humanoid Robotics" class="categoryLinkLabel_W154">Physical AI &amp; Humanoid Robotics</span></a><button aria-label="Collapse sidebar category &#x27;Physical AI &amp; Humanoid Robotics&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/ros2-nervous-system/"><span title="Module 1: The Robotic Nervous System (ROS 2)" class="categoryLinkLabel_W154">Module 1: The Robotic Nervous System (ROS 2)</span></a><button aria-label="Expand sidebar category &#x27;Module 1: The Robotic Nervous System (ROS 2)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"><span title="Module 2: Sensors and Perception for Humanoid Robots" class="categoryLinkLabel_W154">Module 2: Sensors and Perception for Humanoid Robots</span></a><button aria-label="Collapse sidebar category &#x27;Module 2: Sensors and Perception for Humanoid Robots&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems"><span title="Lesson 1: Camera Systems and Computer Vision" class="linkLabel_WmDU">Lesson 1: Camera Systems and Computer Vision</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems.summary"><span title="Summary: Lesson 1 - Camera Systems and Computer Vision" class="linkLabel_WmDU">Summary: Lesson 1 - Camera Systems and Computer Vision</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing"><span title="Lesson 2: Depth Sensing Technologies" class="linkLabel_WmDU">Lesson 2: Depth Sensing Technologies</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing.summary"><span title="Summary: Lesson 2 - Depth Sensing Technologies" class="linkLabel_WmDU">Summary: Lesson 2 - Depth Sensing Technologies</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception"><span title="IMU &amp; Proprioception" class="linkLabel_WmDU">IMU &amp; Proprioception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception.summary"><span title="Summary: Lesson 3 - IMU and Proprioception" class="linkLabel_WmDU">Summary: Lesson 3 - IMU and Proprioception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion"><span title="Sensor Fusion" class="linkLabel_WmDU">Sensor Fusion</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion.summary"><span title="Lesson 4 Summary: Sensor Fusion Techniques for Humanoid Robots" class="linkLabel_WmDU">Lesson 4 Summary: Sensor Fusion Techniques for Humanoid Robots</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.architecture"><span title="Multi-Sensor Fusion Capstone: Architecture Documentation" class="linkLabel_WmDU">Multi-Sensor Fusion Capstone: Architecture Documentation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.casestudies"><span title="Case Studies: Real-World Multi-Sensor Integration" class="linkLabel_WmDU">Case Studies: Real-World Multi-Sensor Integration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content"><span title="Module 2 Capstone Project: Integrated Sensor System Design" class="linkLabel_WmDU">Module 2 Capstone Project: Integrated Sensor System Design</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project"><span title="Capstone Project: Design a Multi-Sensor Humanoid Perception System" class="linkLabel_WmDU">Capstone Project: Design a Multi-Sensor Humanoid Perception System</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.summary"><span title="Capstone Summary" class="linkLabel_WmDU">Capstone Summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/quiz"><span title="Module 2 Quiz: Sensors and Perception for Humanoid Robots" class="linkLabel_WmDU">Module 2 Quiz: Sensors and Perception for Humanoid Robots</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/isaac-ai-brain/"><span title="Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)" class="categoryLinkLabel_W154">Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)</span></a><button aria-label="Expand sidebar category &#x27;Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/"><span title="Module 4: Vision-Language-Action (VLA)" class="categoryLinkLabel_W154">Module 4: Vision-Language-Action (VLA)</span></a><button aria-label="Expand sidebar category &#x27;Module 4: Vision-Language-Action (VLA)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/hackathon-phase-01/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"><span>Physical AI &amp; Humanoid Robotics</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"><span>Module 2: Sensors and Perception for Humanoid Robots</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Capstone Project: Design a Multi-Sensor Humanoid Perception System</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Capstone Project: Design a Multi-Sensor Humanoid Perception System</h1></header>
<p>Integrate everything you&#x27;ve learned‚Äîcamera systems, depth sensing, IMU integration, and sensor fusion‚Äîinto a complete perception system for a humanoid warehouse robot.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="project-overview">Project Overview<a href="#project-overview" class="hash-link" aria-label="Direct link to Project Overview" title="Direct link to Project Overview" translate="no">‚Äã</a></h2>
<p>You&#x27;re tasked with designing the <strong>perception system for a humanoid warehouse robot</strong> that picks items from shelves, navigates between aisles, and delivers packages to packing stations. This robot must recognize objects, measure distances, maintain balance while carrying loads, and robustly perceive its environment despite varying lighting, cluttered scenes, and dynamic obstacles.</p>
<p>Your goal is to create a comprehensive sensor configuration and fusion architecture that demonstrates your understanding of multi-modal perception, not to implement production code.</p>
<p><strong>The Scenario</strong>: A logistics company is deploying humanoid robots (similar to Tesla Optimus or Agility Digit) in a warehouse environment. The robot must:</p>
<ul>
<li class=""><strong>Navigate autonomously</strong> through narrow aisles with moving forklifts and people</li>
<li class=""><strong>Locate and identify</strong> specific items on shelves at various heights (floor to 2 meters)</li>
<li class=""><strong>Reach and grasp</strong> objects while maintaining balance on two legs</li>
<li class=""><strong>Carry loads</strong> up to 10kg while walking, requiring continuous balance control</li>
<li class=""><strong>Operate reliably</strong> in varying lighting (bright windows + dark corners) and handle transparent/reflective packaging</li>
</ul>
<p><strong>What You&#x27;ll Design</strong>:</p>
<ol>
<li class=""><strong>Sensor Configuration</strong>: Which cameras, depth sensors, and IMUs? Where are they placed on the robot?</li>
<li class=""><strong>Sensor Selection Justification</strong>: Why did you choose each sensor type? What are the trade-offs?</li>
<li class=""><strong>ROS2 Integration Architecture</strong>: How do sensor data streams flow through ROS2 nodes?</li>
<li class=""><strong>Sensor Fusion Strategy</strong>: How do you combine camera, depth, and IMU data for robust perception?</li>
</ol>
<p>This capstone integrates all four lessons. It&#x27;s conceptual‚Äîyou&#x27;re designing the perception architecture and justifying decisions, not implementing every algorithm. Think like a robotics perception engineer: what sensors are needed, where should they be placed, how should data be fused, and why is this architecture appropriate for the task?</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="project-requirements">Project Requirements<a href="#project-requirements" class="hash-link" aria-label="Direct link to Project Requirements" title="Direct link to Project Requirements" translate="no">‚Äã</a></h2>
<p>Your multi-sensor perception system design must include:</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-sensor-configuration-30-points">1. Sensor Configuration (30 points)<a href="#1-sensor-configuration-30-points" class="hash-link" aria-label="Direct link to 1. Sensor Configuration (30 points)" title="Direct link to 1. Sensor Configuration (30 points)" translate="no">‚Äã</a></h3>
<p>Design a complete sensor suite for the humanoid warehouse robot. Specify <strong>camera(s), depth sensor(s), and IMU placement</strong>.</p>
<p><strong>Required Sensors</strong>:</p>
<ul>
<li class=""><strong>At least 2 cameras</strong> (specify type: monocular, stereo, RGB-D; resolution; FOV; placement)</li>
<li class=""><strong>At least 1 depth sensor</strong> (specify type: LiDAR, structured light, ToF; range; accuracy; placement)</li>
<li class=""><strong>At least 1 IMU</strong> (specify location: torso, head, limbs; purpose: balance, odometry, vibration detection)</li>
</ul>
<p>For each sensor, provide:</p>
<ul>
<li class=""><strong>Sensor Type and Model</strong> (e.g., &quot;Stereo camera - ZED 2&quot;, &quot;2D LiDAR - SICK TiM551&quot;, &quot;6-axis IMU - Bosch BMI088&quot;)</li>
<li class=""><strong>Key Specifications</strong> (resolution/FOV for cameras, range/accuracy for depth, update rate for IMU)</li>
<li class=""><strong>Physical Placement</strong> (where on the robot: head-mounted, torso-mounted, wrist-mounted, etc.)</li>
<li class=""><strong>Primary Purpose</strong> (what perception task: object recognition, obstacle avoidance, grasp planning, balance control, etc.)</li>
</ul>
<p><strong>Example Sensor Entry</strong>:</p>
<ul>
<li class=""><strong>Type</strong>: RGB-D camera (Intel RealSense D435i)</li>
<li class=""><strong>Specs</strong>: 1280√ó720 RGB, 1280√ó720 depth, 87¬∞ FOV, 0.3-3m range</li>
<li class=""><strong>Placement</strong>: Head-mounted, forward-facing, 1.6m height (eye level)</li>
<li class=""><strong>Purpose</strong>: Object identification (RGB) + grasp distance estimation (depth) for shelf items</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-sensor-selection-justification-25-points">2. Sensor Selection Justification (25 points)<a href="#2-sensor-selection-justification-25-points" class="hash-link" aria-label="Direct link to 2. Sensor Selection Justification (25 points)" title="Direct link to 2. Sensor Selection Justification (25 points)" translate="no">‚Äã</a></h3>
<p>For your sensor configuration, explain <strong>why you chose each sensor type</strong> and analyze the <strong>trade-offs</strong>.</p>
<p>Answer these questions for each sensor category:</p>
<p><strong>Camera Selection</strong>:</p>
<ul>
<li class="">Why monocular, stereo, or RGB-D? What does this choice optimize for (cost, processing, accuracy)?</li>
<li class="">How does camera placement (head vs wrists vs torso) support the warehouse task?</li>
<li class="">How do you handle lighting challenges (bright windows, dark corners, backlit objects)?</li>
</ul>
<p><strong>Depth Sensor Selection</strong>:</p>
<ul>
<li class="">Why LiDAR, structured light, ToF, or stereo vision? What task requirements drove this choice?</li>
<li class="">What are the range and accuracy trade-offs? (e.g., long-range navigation vs short-range manipulation)</li>
<li class="">How does your depth sensor perform indoors with transparent/reflective packaging materials?</li>
</ul>
<p><strong>IMU Selection and Placement</strong>:</p>
<ul>
<li class="">Where is the IMU located (torso center-of-mass, head, feet)? Why this placement for a bipedal robot?</li>
<li class="">What does the IMU measure that cameras and depth sensors cannot provide?</li>
<li class="">How does IMU data support balance control when the robot carries variable loads?</li>
</ul>
<p><strong>Example Justification</strong>:</p>
<blockquote>
<p><strong>Camera Choice</strong>: RGB-D camera on head for integrated color+depth. Trade-off: Limited range (3m) vs stereo, but simpler calibration and processing. Good for shelf items at 0.5-2m distance. Added wrist camera (monocular) for close-range grasp verification (&lt;0.3m) where RGB-D depth fails.</p>
</blockquote>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-ros2-integration-architecture-25-points">3. ROS2 Integration Architecture (25 points)<a href="#3-ros2-integration-architecture-25-points" class="hash-link" aria-label="Direct link to 3. ROS2 Integration Architecture (25 points)" title="Direct link to 3. ROS2 Integration Architecture (25 points)" translate="no">‚Äã</a></h3>
<p>Design the <strong>ROS2 node architecture</strong> showing how sensor data flows through your perception system.</p>
<p><strong>Required Nodes (minimum 4-6)</strong>:</p>
<ul>
<li class=""><strong>Camera Processing Node</strong>: Subscribes to camera images, performs object detection/recognition</li>
<li class=""><strong>Depth Processing Node</strong>: Subscribes to depth data (point cloud or depth image), extracts obstacle information</li>
<li class=""><strong>IMU Monitor Node</strong>: Subscribes to IMU data, estimates orientation and detects balance issues</li>
<li class=""><strong>Sensor Fusion Node</strong>: Combines camera, depth, and IMU data for unified perception</li>
<li class=""><strong>Localization Node</strong>: Fuses visual and inertial data for odometry (where is the robot?)</li>
<li class=""><strong>Object Recognition Node</strong>: Identifies specific items on shelves using camera + depth</li>
</ul>
<p>For each node, specify:</p>
<ul>
<li class=""><strong>Node Name</strong> (descriptive, like <code>object_detector</code>, <code>depth_processor</code>, <code>balance_monitor</code>)</li>
<li class=""><strong>Input Topics</strong> (what sensor_msgs messages: Image, PointCloud2, Imu, LaserScan?)</li>
<li class=""><strong>Output Topics</strong> (what processed data: detected objects, obstacles, orientation, fused state?)</li>
<li class=""><strong>Processing</strong> (brief description: &quot;runs YOLOv5 on RGB images&quot;, &quot;filters point cloud for obstacles &lt;2m&quot;)</li>
</ul>
<p><strong>Example Node</strong>:</p>
<ul>
<li class=""><strong>Node</strong>: <code>visual_inertial_odometry_node</code></li>
<li class=""><strong>Inputs</strong>: <code>/camera/image_raw</code> (sensor_msgs/Image), <code>/imu/data</code> (sensor_msgs/Imu)</li>
<li class=""><strong>Outputs</strong>: <code>/odom</code> (nav_msgs/Odometry) - robot pose estimate</li>
<li class=""><strong>Processing</strong>: Tracks visual features frame-to-frame, fuses with IMU using EKF for drift-free odometry</li>
</ul>
<p><strong>Communication Design</strong>:
For each data flow between nodes, specify:</p>
<ul>
<li class=""><strong>Topic Name</strong> (e.g., <code>/camera/rgb/image_raw</code>, <code>/depth/points</code>, <code>/imu/data</code>)</li>
<li class=""><strong>Message Type</strong> (sensor_msgs/Image, sensor_msgs/PointCloud2, sensor_msgs/Imu, custom messages)</li>
<li class=""><strong>Frequency</strong> (30 Hz for cameras, 100 Hz for IMU, 10 Hz for fused state)</li>
<li class=""><strong>Why this pattern?</strong> (continuous stream = topic; request-response = service; synchronization needed?)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="4-sensor-fusion-strategy-20-points">4. Sensor Fusion Strategy (20 points)<a href="#4-sensor-fusion-strategy-20-points" class="hash-link" aria-label="Direct link to 4. Sensor Fusion Strategy (20 points)" title="Direct link to 4. Sensor Fusion Strategy (20 points)" translate="no">‚Äã</a></h3>
<p>Describe <strong>how you combine data from multiple sensors</strong> to achieve robust perception. Address synchronization, fusion algorithms, and failure handling.</p>
<p><strong>Required Fusion Strategies (choose at least 2)</strong>:</p>
<p><strong>Visual-Inertial Fusion (Camera + IMU)</strong>:</p>
<ul>
<li class="">How do you fuse camera motion estimates with IMU acceleration/gyroscope data?</li>
<li class="">What fusion algorithm (complementary filter, Kalman filter, robot_localization)?</li>
<li class="">Why is this fusion beneficial? (e.g., camera provides scale, IMU prevents drift between frames)</li>
</ul>
<p><strong>Depth-Enhanced Object Detection (Camera + Depth)</strong>:</p>
<ul>
<li class="">How do you combine RGB object detection with depth measurements?</li>
<li class="">Do you use early fusion (RGB-D input to detector), late fusion (separate detection + depth lookup), or hybrid?</li>
<li class="">What does depth add to pure camera detection? (e.g., reject false positives on 2D posters, measure actual grasp distance)</li>
</ul>
<p><strong>Multi-Sensor Obstacle Avoidance (Depth + Camera + IMU)</strong>:</p>
<ul>
<li class="">How do you merge LiDAR/depth camera obstacle maps with visual obstacle detection?</li>
<li class="">How does IMU orientation inform which sensor data is most reliable (e.g., camera unreliable if robot tilted)?</li>
<li class="">What happens when sensors disagree (camera sees clear path, LiDAR detects transparent glass)?</li>
</ul>
<p><strong>Timestamp Synchronization</strong>:</p>
<ul>
<li class="">How do you handle different sensor frequencies (cameras 30Hz, IMU 100Hz, LiDAR 10Hz)?</li>
<li class="">Do you use message_filters for exact timestamp matching or approximate time policies?</li>
</ul>
<p><strong>Sensor Failure Handling</strong>:</p>
<ul>
<li class="">What happens if a camera is blinded by sunlight or a depth sensor fails?</li>
<li class="">How does the system degrade gracefully (e.g., fall back to pure visual navigation if LiDAR fails)?</li>
</ul>
<p><strong>Example Fusion Strategy</strong>:</p>
<blockquote>
<p><strong>Visual-Inertial Odometry (VIO)</strong>: Camera tracks visual features for position changes, IMU measures acceleration and rotation. Fusion: Extended Kalman Filter (EKF) predicts pose from IMU high-rate data, corrects with camera visual measurements at 30Hz. Benefit: IMU fills gaps between camera frames (high-rate smooth motion), camera prevents IMU drift over time (absolute position). Implementation: ROS2 <code>robot_localization</code> package with dual EKF nodes.</p>
</blockquote>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="deliverables">Deliverables<a href="#deliverables" class="hash-link" aria-label="Direct link to Deliverables" title="Direct link to Deliverables" translate="no">‚Äã</a></h2>
<p>Submit your capstone project with the following components:</p>
<ol>
<li class="">
<p><strong>Sensor Configuration Diagram</strong> (physical layout):</p>
<ul>
<li class="">Drawing or description showing sensor placement on humanoid robot body</li>
<li class="">Label each sensor with type and purpose</li>
<li class="">Include field-of-view overlays for cameras, range indicators for depth sensors</li>
</ul>
</li>
<li class="">
<p><strong>Sensor Selection Justification</strong> (written report):</p>
<ul>
<li class="">2-3 paragraphs per sensor category (cameras, depth, IMU)</li>
<li class="">Explain trade-offs and why your choices fit the warehouse task</li>
<li class="">Address environmental challenges (lighting, transparent objects, dynamic obstacles)</li>
</ul>
</li>
<li class="">
<p><strong>ROS2 Node Architecture Diagram</strong>:</p>
<ul>
<li class="">Block diagram showing nodes as boxes, topics as arrows</li>
<li class="">Label each topic with message type and frequency</li>
<li class="">Include at least 4-6 nodes with clear data flow</li>
</ul>
</li>
<li class="">
<p><strong>ROS2 Code Outline</strong> (Python pseudocode):</p>
<ul>
<li class="">Skeleton code for 1-2 key nodes (sensor fusion node, multi-sensor subscriber)</li>
<li class="">Show rclpy structure: subscribers, message_filters synchronization, callback processing</li>
<li class="">Include type hints and comments explaining sensor integration</li>
</ul>
</li>
<li class="">
<p><strong>Sensor Fusion Strategy</strong> (written description):</p>
<ul>
<li class="">1-2 paragraphs describing your fusion approach</li>
<li class="">Specify algorithms (complementary filter, Kalman filter, weighted averaging)</li>
<li class="">Explain how fusion improves robustness over single sensors</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="evaluation-rubric">Evaluation Rubric<a href="#evaluation-rubric" class="hash-link" aria-label="Direct link to Evaluation Rubric" title="Direct link to Evaluation Rubric" translate="no">‚Äã</a></h2>
<p>Your capstone project will be evaluated on:</p>
<table><thead><tr><th>Category</th><th>Excellent (90-100%)</th><th>Good (80-89%)</th><th>Needs Work (&lt;80%)</th></tr></thead><tbody><tr><td><strong>Sensor Configuration</strong> (30 pts)</td><td>Complete sensor suite with detailed specs, appropriate placement for all tasks, addresses environmental challenges</td><td>Sensor suite covers main tasks, reasonable placement, some environmental factors considered</td><td>Missing key sensors or unclear placement, environmental challenges not addressed</td></tr><tr><td><strong>Justification &amp; Trade-offs</strong> (25 pts)</td><td>Thorough analysis of sensor trade-offs, clear rationale for each choice tied to task requirements, addresses failure modes</td><td>Good justification for most sensors, some trade-offs analyzed, task fit explained</td><td>Weak justification, trade-offs not analyzed, unclear why sensors fit task</td></tr><tr><td><strong>ROS2 Architecture</strong> (25 pts)</td><td>Clear node diagram with 5+ nodes, appropriate topics/frequencies, demonstrates understanding of ROS2 message flow</td><td>Node architecture covers main functions, 4+ nodes, topics mostly correct</td><td>Unclear architecture, missing key nodes, ROS2 message types incorrect</td></tr><tr><td><strong>Sensor Fusion Strategy</strong> (20 pts)</td><td>Detailed fusion approach with algorithms specified, synchronization handled, failure modes addressed</td><td>Fusion strategy described for 1-2 sensor pairs, some algorithm detail</td><td>Vague fusion description, no specific algorithms or synchronization</td></tr></tbody></table>
<p><strong>Total: 100 points</strong></p>
<p><strong>Pass Threshold</strong>: 80 points (demonstrates integration of all 4 lessons)</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="resources-and-references">Resources and References<a href="#resources-and-references" class="hash-link" aria-label="Direct link to Resources and References" title="Direct link to Resources and References" translate="no">‚Äã</a></h2>
<p><strong>Review These Lessons</strong>:</p>
<ul>
<li class=""><a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems">Lesson 1: Camera Systems</a> - Camera types, sensor_msgs/Image, placement strategies</li>
<li class=""><a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing">Lesson 2: Depth Sensing</a> - LiDAR vs depth cameras, sensor_msgs/PointCloud2, range/accuracy trade-offs</li>
<li class=""><a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception">Lesson 3: IMU &amp; Proprioception</a> - Accelerometer/gyroscope, sensor_msgs/Imu, balance control</li>
<li class=""><a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion">Lesson 4: Sensor Fusion</a> - Kalman filters, VIO, robot_localization, message_filters</li>
</ul>
<p><strong>Real-World Examples</strong>:</p>
<ul>
<li class=""><strong>Tesla Optimus</strong>: Multi-camera configuration (head + wrists), vision-centric fusion for manipulation</li>
<li class=""><strong>Agility Robotics Digit</strong>: Stereo cameras + IMU for VIO, LiDAR for navigation, depth for grasp planning</li>
<li class=""><strong>Boston Dynamics Atlas</strong>: High-grade IMU + stereo vision for SLAM, depth sensors for obstacle avoidance</li>
</ul>
<p><strong>ROS2 Packages to Reference</strong>:</p>
<ul>
<li class=""><code>image_transport</code> - Efficient image streaming</li>
<li class=""><code>depth_image_proc</code> - Depth image processing (point cloud conversion, obstacle extraction)</li>
<li class=""><code>robot_localization</code> - Sensor fusion (EKF, UKF) for camera+IMU+wheel odometry</li>
<li class=""><code>message_filters</code> - Timestamp synchronization for multi-sensor topics</li>
</ul>
<p><strong>Optional Setup</strong> (for advanced students who want to implement):
If you want to test your design in simulation:</p>
<ol>
<li class="">Install ROS2 Humble and Gazebo</li>
<li class="">Use pre-built humanoid robot models (e.g., Robonaut, NASA Valkyrie URDF)</li>
<li class="">Add simulated sensors to URDF (camera, depth camera, IMU plugins)</li>
<li class="">Implement your ROS2 nodes and test with simulated sensor data</li>
<li class="">Visualize in RViz to see camera images, point clouds, TF frames</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="tips-for-success">Tips for Success<a href="#tips-for-success" class="hash-link" aria-label="Direct link to Tips for Success" title="Direct link to Tips for Success" translate="no">‚Äã</a></h2>
<p>üí¨ <strong>AI Colearning Prompt</strong>: &quot;Ask Claude to critique your sensor configuration for the warehouse task. Prompt: &#x27;I&#x27;m designing a humanoid perception system with [your sensors]. What are the biggest weaknesses in this configuration for indoor warehouse navigation and manipulation? What environmental conditions might cause failures?&#x27;&quot;</p>
<p>üéì <strong>Expert Insight</strong>: Real perception systems layer redundancy. Tesla Optimus uses 8+ cameras with overlapping fields-of-view so losing one camera doesn&#x27;t create blind spots. Consider: if your primary depth sensor fails or is occluded, can the robot still navigate safely? If the head camera is blinded by a flashlight, can wrist cameras take over for manipulation?</p>
<p>ü§ù <strong>Practice Exercise</strong>: Before designing your full system, sketch 3 different sensor configurations:</p>
<ol>
<li class=""><strong>Minimal</strong> (cheapest, fewest sensors)</li>
<li class=""><strong>Robust</strong> (redundancy, multiple modalities)</li>
<li class=""><strong>Performance</strong> (best accuracy, no cost constraints)</li>
</ol>
<p>Then analyze: What does each configuration optimize for? Which is most appropriate for a real warehouse deployment? This trade-off analysis is the core of perception system design.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="submission-notes">Submission Notes<a href="#submission-notes" class="hash-link" aria-label="Direct link to Submission Notes" title="Direct link to Submission Notes" translate="no">‚Äã</a></h2>
<ul>
<li class=""><strong>Format</strong>: PDF or Markdown document with diagrams (hand-drawn or digital tools like draw.io, Lucidchart)</li>
<li class=""><strong>Length</strong>: Expect 4-6 pages including diagrams and code snippets</li>
<li class=""><strong>Collaboration</strong>: Discuss ideas with classmates, but submit your own design</li>
<li class=""><strong>Questions</strong>: If you need clarification on warehouse task requirements or sensor specifications, ask in the course forum</li>
</ul>
<p>This capstone demonstrates your ability to integrate camera systems, depth sensing, IMU data, and sensor fusion into a practical humanoid robot application. Focus on clear justifications and thoughtful trade-off analysis‚Äîthere&#x27;s no single &quot;correct&quot; sensor configuration, but your reasoning should show deep understanding of each sensor&#x27;s strengths and limitations.</p>
<p>Good luck! ü§ñ‚ú®</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-tags-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/sensors">sensors</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/perception">perception</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/capstone">capstone</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/project">project</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/integration">integration</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/multi-sensor">multi-sensor</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/camera">camera</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/depth">depth</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/imu">imu</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/sensor-fusion">sensor-fusion</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/humanoid">humanoid</a></li></ul></div></div><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/05-capstone-project.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Module 2 Capstone Project: Integrated Sensor System Design</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.summary"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Capstone Summary</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#project-overview" class="table-of-contents__link toc-highlight">Project Overview</a></li><li><a href="#project-requirements" class="table-of-contents__link toc-highlight">Project Requirements</a><ul><li><a href="#1-sensor-configuration-30-points" class="table-of-contents__link toc-highlight">1. Sensor Configuration (30 points)</a></li><li><a href="#2-sensor-selection-justification-25-points" class="table-of-contents__link toc-highlight">2. Sensor Selection Justification (25 points)</a></li><li><a href="#3-ros2-integration-architecture-25-points" class="table-of-contents__link toc-highlight">3. ROS2 Integration Architecture (25 points)</a></li><li><a href="#4-sensor-fusion-strategy-20-points" class="table-of-contents__link toc-highlight">4. Sensor Fusion Strategy (20 points)</a></li></ul></li><li><a href="#deliverables" class="table-of-contents__link toc-highlight">Deliverables</a></li><li><a href="#evaluation-rubric" class="table-of-contents__link toc-highlight">Evaluation Rubric</a></li><li><a href="#resources-and-references" class="table-of-contents__link toc-highlight">Resources and References</a></li><li><a href="#tips-for-success" class="table-of-contents__link toc-highlight">Tips for Success</a></li><li><a href="#submission-notes" class="table-of-contents__link toc-highlight">Submission Notes</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Learn</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/">Textbook</a></li><li class="footer__item"><a class="footer__link-item" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/ros2-nervous-system/">Module 1: ROS2</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/anusbutt/hackathon-phase-01" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://linkedin.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">X (Twitter)<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://instagram.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://panaversity.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Panaversity<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2026 Physical AI & Humanoid Robotics Textbook. Built for Panaversity Hackathon.</div></div></div></footer><button class="floatingChatButton__Uyc" aria-label="Open AI Assistant" title="Ask questions about Physical AI &amp; Humanoid Robotics"><span class="chatIcon_GGtK">üí¨</span><span class="chatLabel_InO3">AI Assistant</span></button></div>
</body>
</html>