"use strict";(globalThis.webpackChunkbook_source=globalThis.webpackChunkbook_source||[]).push([[5777],{4513:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Physical-AI-Humanoid-Robotics/vision-language-action/cognitive-planning","title":"Lesson 2 - Cognitive Planning with LLMs","description":"What Is Cognitive Planning?","source":"@site/docs/13-Physical-AI-Humanoid-Robotics/04-vision-language-action/02-cognitive-planning.md","sourceDirName":"13-Physical-AI-Humanoid-Robotics/04-vision-language-action","slug":"/Physical-AI-Humanoid-Robotics/vision-language-action/cognitive-planning","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/04-vision-language-action/02-cognitive-planning.md","tags":[{"inline":true,"label":"llm","permalink":"/hackathon-phase-01/docs/tags/llm"},{"inline":true,"label":"cognitive-planning","permalink":"/hackathon-phase-01/docs/tags/cognitive-planning"},{"inline":true,"label":"task-decomposition","permalink":"/hackathon-phase-01/docs/tags/task-decomposition"},{"inline":true,"label":"robotics","permalink":"/hackathon-phase-01/docs/tags/robotics"},{"inline":true,"label":"natural-language","permalink":"/hackathon-phase-01/docs/tags/natural-language"}],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Lesson 2 - Cognitive Planning with LLMs","sidebar_position":2,"skills":["Large Language Models","Task Decomposition","Natural Language Understanding","Prompt Engineering","ROS2 Action Planning"],"learning_objectives":["Understand how LLMs decompose high-level natural language commands into action sequences","Learn effective prompting strategies for robotics task planning","Implement task decomposition algorithms for natural language to ROS2 actions","Handle ambiguous or underspecified commands with LLMs","Design error recovery strategies for LLM-generated action sequences"],"cognitive_load":6,"differentiation":"AI Colearning, Expert Insight, Practice Exercise","tags":["llm","cognitive-planning","task-decomposition","robotics","natural-language"],"created":"2025-12-23","last_modified":"2025-12-23","ros2_version":"humble"},"sidebar":"tutorialSidebar","previous":{"title":"Summary - Lesson 1 - Voice-to-Action Systems","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/voice-to-action.summary"},"next":{"title":"Summary - Lesson 2 - Cognitive Planning with LLMs","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/cognitive-planning.summary"}}');var a=i(4848),o=i(8453);const s={title:"Lesson 2 - Cognitive Planning with LLMs",sidebar_position:2,skills:["Large Language Models","Task Decomposition","Natural Language Understanding","Prompt Engineering","ROS2 Action Planning"],learning_objectives:["Understand how LLMs decompose high-level natural language commands into action sequences","Learn effective prompting strategies for robotics task planning","Implement task decomposition algorithms for natural language to ROS2 actions","Handle ambiguous or underspecified commands with LLMs","Design error recovery strategies for LLM-generated action sequences"],cognitive_load:6,differentiation:"AI Colearning, Expert Insight, Practice Exercise",tags:["llm","cognitive-planning","task-decomposition","robotics","natural-language"],created:"2025-12-23",last_modified:"2025-12-23",ros2_version:"humble"},r="Lesson 2: Cognitive Planning with LLMs",l={},c=[{value:"What Is Cognitive Planning?",id:"what-is-cognitive-planning",level:2},{value:"Why Cognitive Planning Matters",id:"why-cognitive-planning-matters",level:2},{value:"Key Principles",id:"key-principles",level:2},{value:"Chain-of-Thought Prompting",id:"chain-of-thought-prompting",level:3},{value:"Task Decomposition Hierarchies",id:"task-decomposition-hierarchies",level:3},{value:"World Modeling and Context Awareness",id:"world-modeling-and-context-awareness",level:3},{value:"Grounding in Physical Reality",id:"grounding-in-physical-reality",level:3},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:3},{value:"\ud83d\udcac AI Colearning Prompt",id:"-ai-colearning-prompt",level:2},{value:"\ud83c\udf93 Expert Insight",id:"-expert-insight",level:2},{value:"Practical Example: LLM-Based Task Planning for Cleaning",id:"practical-example-llm-based-task-planning-for-cleaning",level:2},{value:"\ud83e\udd1d Practice Exercise",id:"-practice-exercise",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"lesson-2-cognitive-planning-with-llms",children:"Lesson 2: Cognitive Planning with LLMs"})}),"\n",(0,a.jsx)(n.h2,{id:"what-is-cognitive-planning",children:"What Is Cognitive Planning?"}),"\n",(0,a.jsx)(n.p,{children:"Cognitive planning represents the AI brain of modern robotic systems, where Large Language Models (LLMs) serve as the reasoning engine that translates high-level human intentions into executable robotic actions. Unlike traditional programming approaches that require explicit step-by-step instructions, cognitive planning uses LLMs to understand natural language commands and automatically decompose them into sequences of specific robot behaviors."}),"\n",(0,a.jsx)(n.p,{children:"At its core, cognitive planning involves several key components: natural language understanding to interpret the command, task decomposition to break complex goals into simpler subtasks, spatial reasoning to understand the environment and object relationships, and action mapping to connect abstract concepts to concrete ROS2 actions. This process mimics human cognitive planning, where we naturally decompose complex goals into manageable steps."}),"\n",(0,a.jsx)(n.p,{children:'The LLM acts as an intelligent intermediary between human intent and robot execution. When given a command like "Clean the room," the LLM must understand that this involves multiple subtasks: identifying what constitutes "cleaning," determining what areas need attention, recognizing objects that might need to be moved or disposed of, and sequencing these actions appropriately. This requires not just language understanding, but also commonsense reasoning about the physical world.'}),"\n",(0,a.jsx)(n.p,{children:"Modern cognitive planning systems often employ techniques like Chain-of-Thought (CoT) prompting, where the LLM is guided to think through the problem step-by-step before providing the final action sequence. This approach improves the reliability and interpretability of the planning process, making it easier to debug when the robot behaves unexpectedly."}),"\n",(0,a.jsx)(n.h2,{id:"why-cognitive-planning-matters",children:"Why Cognitive Planning Matters"}),"\n",(0,a.jsx)(n.p,{children:"Cognitive planning bridges the critical gap between human communication and robot execution, enabling complex autonomous behaviors from simple natural language commands. Without this capability, robots would require explicit programming for every possible task, making them inflexible and difficult to deploy in dynamic environments where new tasks emerge regularly."}),"\n",(0,a.jsx)(n.p,{children:'For humanoid robots specifically, cognitive planning enables true collaboration with humans. Instead of requiring users to learn robot-specific commands or programming languages, cognitive planning allows natural communication. A user can say "Please set the table for dinner" and the robot can decompose this into: identifying table location, determining appropriate place settings, retrieving plates, utensils, and glasses, and arranging them appropriately.'}),"\n",(0,a.jsx)(n.p,{children:"The scalability aspect is crucial for practical robotics deployment. Traditional programming approaches require an exponential increase in complexity as new tasks are added. Cognitive planning, however, can generalize from existing knowledge to handle novel commands that follow similar patterns. This makes robots more adaptable and cost-effective to deploy."}),"\n",(0,a.jsx)(n.p,{children:'Furthermore, cognitive planning enables robots to handle ambiguous or underspecified commands gracefully. When a user says "Clean up over there," the robot can ask clarifying questions or make reasonable assumptions based on context, rather than failing completely as a traditional programmed system might.'}),"\n",(0,a.jsx)(n.h2,{id:"key-principles",children:"Key Principles"}),"\n",(0,a.jsx)(n.h3,{id:"chain-of-thought-prompting",children:"Chain-of-Thought Prompting"}),"\n",(0,a.jsx)(n.p,{children:"This technique guides LLMs to reason step-by-step before providing the final answer. For robotics, this might involve asking the LLM to first identify the goal, then list required subtasks, consider constraints, and finally provide the action sequence. This approach improves reliability and provides interpretable reasoning paths."}),"\n",(0,a.jsx)(n.h3,{id:"task-decomposition-hierarchies",children:"Task Decomposition Hierarchies"}),"\n",(0,a.jsx)(n.p,{children:'Complex tasks are broken down into hierarchies of subtasks. For example, "Clean the room" decomposes into "Identify dirty items," "Categorize items," "Dispose of trash," "Put items in proper places," and so on. Each subtask can be further decomposed until reaching primitive actions executable by the robot.'}),"\n",(0,a.jsx)(n.h3,{id:"world-modeling-and-context-awareness",children:"World Modeling and Context Awareness"}),"\n",(0,a.jsx)(n.p,{children:"Effective cognitive planning requires the LLM to maintain a model of the world state and how actions affect it. This includes understanding object affordances (what can be done with an object), spatial relationships, and the effects of actions on the environment."}),"\n",(0,a.jsx)(n.h3,{id:"grounding-in-physical-reality",children:"Grounding in Physical Reality"}),"\n",(0,a.jsx)(n.p,{children:"LLM outputs must be grounded in the physical capabilities and constraints of the robot. A plan that works in simulation or abstract reasoning must be feasible given the robot's mobility, manipulation capabilities, and sensor limitations."}),"\n",(0,a.jsx)(n.h3,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,a.jsx)(n.p,{children:"Cognitive planning systems must handle cases where LLM-generated plans fail or where the physical world differs from the LLM's assumptions. This includes fallback strategies, clarification requests, and graceful degradation when perfect execution isn't possible."}),"\n",(0,a.jsx)(n.h2,{id:"-ai-colearning-prompt",children:"\ud83d\udcac AI Colearning Prompt"}),"\n",(0,a.jsx)(n.p,{children:'Ask Claude to demonstrate how "Clean the room" gets decomposed into robotic actions. Consider the intermediate reasoning steps that would be important for a cognitive planning system to consider.'}),"\n",(0,a.jsx)(n.h2,{id:"-expert-insight",children:"\ud83c\udf93 Expert Insight"}),"\n",(0,a.jsx)(n.p,{children:"LLMs have inherent limitations when applied to robotics planning. They can hallucinate information not present in their training data, struggle with precise spatial reasoning, and may generate plans that seem reasonable but are physically impossible. Robust cognitive planning systems must include validation mechanisms to catch these issues before execution. Additionally, LLMs may generate different responses to identical prompts due to their probabilistic nature, requiring careful consideration of consistency in safety-critical applications."}),"\n",(0,a.jsx)(n.h2,{id:"practical-example-llm-based-task-planning-for-cleaning",children:"Practical Example: LLM-Based Task Planning for Cleaning"}),"\n",(0,a.jsx)(n.p,{children:'Consider a humanoid robot receiving the command "Clean the living room." A cognitive planning system would process this command through several stages:'}),"\n",(0,a.jsx)(n.p,{children:"First, the LLM interprets the command and identifies the key components: the goal (cleaning), the location (living room), and the implied constraints (don't damage furniture, return items to proper places). The system might use a prompt template like: \"Decompose the task 'Clean the living room' into specific robotic actions. Consider what cleaning involves, what objects are typically found in a living room, and what actions are needed to make the space clean.\""}),"\n",(0,a.jsx)(n.p,{children:"The LLM generates a hierarchical plan:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Navigate to living room"}),"\n",(0,a.jsx)(n.li,{children:"Survey environment to identify objects and obstacles"}),"\n",(0,a.jsx)(n.li,{children:"Categorize objects (trash, misplaced items, furniture to be preserved)"}),"\n",(0,a.jsxs)(n.li,{children:["For each category:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Trash: Pick up and dispose"}),"\n",(0,a.jsx)(n.li,{children:"Misplaced items: Identify correct location and return"}),"\n",(0,a.jsx)(n.li,{children:"Furniture: Leave in place"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:"Return to home position"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:'The cognitive planning system then maps these high-level steps to specific ROS2 actions. "Navigate to living room" becomes a Navigation2 goal. "Survey environment" triggers perception nodes. "Pick up and dispose" decomposes into manipulation action sequences.'}),"\n",(0,a.jsx)(n.p,{children:"Throughout this process, the system maintains awareness of constraints: the robot cannot lift objects heavier than its payload capacity, it must avoid collisions, and it should preserve valuable items. If the robot encounters an unexpected obstacle during navigation, the system can replan or request assistance."}),"\n",(0,a.jsx)(n.p,{children:"This example demonstrates how cognitive planning transforms abstract human goals into concrete robot behaviors while maintaining flexibility to handle real-world complexities."}),"\n",(0,a.jsx)(n.h2,{id:"-practice-exercise",children:"\ud83e\udd1d Practice Exercise"}),"\n",(0,a.jsx)(n.p,{children:'Analyze the command "Set up for a meeting" and break it down into ROS2 action sequences. Consider what objects might be needed (projector, chairs, whiteboard markers), where they might be located, and in what order the setup should occur. What clarifying questions might the robot need to ask?'}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"Cognitive planning with LLMs represents a paradigm shift in robotics, enabling robots to understand and execute complex tasks from natural language commands. The key components include natural language understanding, task decomposition, world modeling, and action mapping. Effective systems employ techniques like Chain-of-Thought prompting and hierarchical task decomposition to generate reliable plans."}),"\n",(0,a.jsx)(n.p,{children:"The approach enables unprecedented flexibility and natural human-robot interaction, allowing robots to handle novel tasks without explicit programming. However, successful implementation requires careful attention to grounding in physical reality, error handling, and validation of LLM outputs."}),"\n",(0,a.jsx)(n.p,{children:"As LLM capabilities continue to advance, cognitive planning will become increasingly sophisticated, enabling robots to handle more complex and nuanced tasks. The integration of cognitive planning with other robotic capabilities like perception and action execution creates truly autonomous systems capable of meaningful collaboration with humans."}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(n.p,{children:"In the next lesson, we'll explore how computer vision and language models work together in Vision-Language integration systems. We'll examine how robots identify and manipulate objects mentioned in natural language commands, building on the cognitive planning foundation we've established here."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>r});var t=i(6540);const a={},o=t.createContext(a);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);