<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Lesson 2: Depth Sensing Technologies | Physical AI &amp; Humanoid Robotics Textbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://anusbutt.github.io/hackathon-phase-01/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://anusbutt.github.io/hackathon-phase-01/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Lesson 2: Depth Sensing Technologies | Physical AI &amp; Humanoid Robotics Textbook"><meta data-rh="true" name="description" content="What Is Depth Sensing in Robotics?"><meta data-rh="true" property="og:description" content="What Is Depth Sensing in Robotics?"><link data-rh="true" rel="icon" href="/hackathon-phase-01/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing"><link data-rh="true" rel="alternate" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing" hreflang="en"><link data-rh="true" rel="alternate" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Physical AI & Humanoid Robotics","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"},{"@type":"ListItem","position":2,"name":"Module 2: Sensors and Perception for Humanoid Robots","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"},{"@type":"ListItem","position":3,"name":"Lesson 2: Depth Sensing Technologies","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing"}]}</script><link rel="alternate" type="application/rss+xml" href="/hackathon-phase-01/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics Textbook RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/hackathon-phase-01/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Textbook Atom Feed"><link rel="stylesheet" href="/hackathon-phase-01/assets/css/styles.5903934b.css">
<script src="/hackathon-phase-01/assets/js/runtime~main.94161a1c.js" defer="defer"></script>
<script src="/hackathon-phase-01/assets/js/main.ab534ac3.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/hackathon-phase-01/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/hackathon-phase-01/"><div class="navbar__logo"><img src="/hackathon-phase-01/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/hackathon-phase-01/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics Textbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/">Textbook</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/anusbutt/hackathon-phase-01" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"><span title="Physical AI &amp; Humanoid Robotics" class="categoryLinkLabel_W154">Physical AI &amp; Humanoid Robotics</span></a><button aria-label="Collapse sidebar category &#x27;Physical AI &amp; Humanoid Robotics&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/ros2-nervous-system/"><span title="Module 1: The Robotic Nervous System (ROS 2)" class="categoryLinkLabel_W154">Module 1: The Robotic Nervous System (ROS 2)</span></a><button aria-label="Expand sidebar category &#x27;Module 1: The Robotic Nervous System (ROS 2)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"><span title="Module 2: Sensors and Perception for Humanoid Robots" class="categoryLinkLabel_W154">Module 2: Sensors and Perception for Humanoid Robots</span></a><button aria-label="Collapse sidebar category &#x27;Module 2: Sensors and Perception for Humanoid Robots&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems"><span title="Lesson 1: Camera Systems and Computer Vision" class="linkLabel_WmDU">Lesson 1: Camera Systems and Computer Vision</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems.summary"><span title="Summary: Lesson 1 - Camera Systems and Computer Vision" class="linkLabel_WmDU">Summary: Lesson 1 - Camera Systems and Computer Vision</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing"><span title="Lesson 2: Depth Sensing Technologies" class="linkLabel_WmDU">Lesson 2: Depth Sensing Technologies</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing.summary"><span title="Summary: Lesson 2 - Depth Sensing Technologies" class="linkLabel_WmDU">Summary: Lesson 2 - Depth Sensing Technologies</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception"><span title="IMU &amp; Proprioception" class="linkLabel_WmDU">IMU &amp; Proprioception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception.summary"><span title="Summary: Lesson 3 - IMU and Proprioception" class="linkLabel_WmDU">Summary: Lesson 3 - IMU and Proprioception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion"><span title="Sensor Fusion" class="linkLabel_WmDU">Sensor Fusion</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion.summary"><span title="Lesson 4 Summary: Sensor Fusion Techniques for Humanoid Robots" class="linkLabel_WmDU">Lesson 4 Summary: Sensor Fusion Techniques for Humanoid Robots</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.architecture"><span title="Multi-Sensor Fusion Capstone: Architecture Documentation" class="linkLabel_WmDU">Multi-Sensor Fusion Capstone: Architecture Documentation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.casestudies"><span title="Case Studies: Real-World Multi-Sensor Integration" class="linkLabel_WmDU">Case Studies: Real-World Multi-Sensor Integration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content"><span title="Module 2 Capstone Project: Integrated Sensor System Design" class="linkLabel_WmDU">Module 2 Capstone Project: Integrated Sensor System Design</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project"><span title="Capstone Project: Design a Multi-Sensor Humanoid Perception System" class="linkLabel_WmDU">Capstone Project: Design a Multi-Sensor Humanoid Perception System</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.summary"><span title="Capstone Summary" class="linkLabel_WmDU">Capstone Summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/quiz"><span title="Module 2 Quiz: Sensors and Perception for Humanoid Robots" class="linkLabel_WmDU">Module 2 Quiz: Sensors and Perception for Humanoid Robots</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/isaac-ai-brain/"><span title="Module 3: The AI-Robot Brain (NVIDIA Isaacâ„¢)" class="categoryLinkLabel_W154">Module 3: The AI-Robot Brain (NVIDIA Isaacâ„¢)</span></a><button aria-label="Expand sidebar category &#x27;Module 3: The AI-Robot Brain (NVIDIA Isaacâ„¢)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/"><span title="Module 4: Vision-Language-Action (VLA)" class="categoryLinkLabel_W154">Module 4: Vision-Language-Action (VLA)</span></a><button aria-label="Expand sidebar category &#x27;Module 4: Vision-Language-Action (VLA)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/hackathon-phase-01/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"><span>Physical AI &amp; Humanoid Robotics</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"><span>Module 2: Sensors and Perception for Humanoid Robots</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Lesson 2: Depth Sensing Technologies</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Lesson 2: Depth Sensing Technologies for Humanoid Robots</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-depth-sensing-in-robotics">What Is Depth Sensing in Robotics?<a href="#what-is-depth-sensing-in-robotics" class="hash-link" aria-label="Direct link to What Is Depth Sensing in Robotics?" title="Direct link to What Is Depth Sensing in Robotics?" translate="no">â€‹</a></h2>
<p>Depth sensing is the technology that measures how far away objects are from a robot, converting distance information into digital data that enables safe navigation, precise manipulation, and intelligent interaction with the physical world. While this might sound simple, depth sensing represents one of the most critical capabilities separating a robot that merely sees from one that truly understands its spatial environment.</p>
<p>Think of the difference this way: a camera tells your humanoid robot <em>what</em> an object isâ€”perhaps recognizing a coffee cup on a table through computer vision. But depth sensing answers the equally important question of <em>where</em> that cup exists in three-dimensional space. Is it 50 centimeters away or 2 meters? Is it within reach, or does the robot need to walk closer? Without accurate depth information, even the most sophisticated vision system leaves a robot guessing about distances, leading to navigation collisions and failed grasping attempts.</p>
<p>Depth sensing enables three foundational capabilities for humanoid robots. First, it allows <strong>safe navigation</strong>â€”the robot can detect obstacles, stairs, and uneven terrain, planning collision-free paths through complex environments. Second, it enables <strong>accurate manipulation</strong>â€”when reaching for objects, the robot knows exactly how far to extend its arm and where to position its gripper. Third, depth sensing supports <strong>3D scene understanding</strong>â€”by building volumetric maps of surroundings, robots can localize themselves, track moving objects, and predict safe trajectories for dynamic tasks like catching or handing objects to humans.</p>
<p>To understand depth sensing systems, you&#x27;ll encounter several key terms throughout this lesson. <strong>Range</strong> refers to the maximum distance a sensor can measureâ€”some sensors reach only a few meters (ideal for manipulation tasks), while others measure distances beyond 100 meters (critical for outdoor navigation). <strong>Depth resolution</strong> describes the accuracy of each measurement, typically expressed as a tolerance like Â±3cm or Â±5cm. <strong>Point clouds</strong> are collections of 3D coordinates (x, y, z points) that represent the sensed environment as thousands or millions of individual measurements. <strong>Field of view (FOV)</strong> defines the angular extent of the sensing areaâ€”a 360-degree FOV means the sensor captures a complete horizontal circle around the robot. Finally, <strong>scan rate</strong> indicates how often the sensor updates its measurements, measured in Hertz (Hz)â€”higher rates enable faster reaction to dynamic obstacles.</p>
<blockquote>
<p><strong>ðŸ’¬ Ask your AI assistant</strong>: &quot;Explain the difference between a 2D LiDAR scanning a 360Â° horizontal plane and a 3D LiDAR scanning the full environment. Use an analogy of looking at a world from a specific height (2D) versus viewing it from all angles (3D). What information is lost with 2D, and why might a humanoid robot still use 2D LiDAR for certain tasks?&quot;</p>
<p>This prompt helps you develop intuition about the trade-offs between 2D and 3D sensing before diving into technical details.</p>
</blockquote>
<p>With these fundamentals established, you&#x27;re ready to explore why depth sensing matters so profoundly for physical AI systems that must operate safely and intelligently in human environments.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="why-depth-sensing-matters-for-physical-ai">Why Depth Sensing Matters for Physical AI<a href="#why-depth-sensing-matters-for-physical-ai" class="hash-link" aria-label="Direct link to Why Depth Sensing Matters for Physical AI" title="Direct link to Why Depth Sensing Matters for Physical AI" translate="no">â€‹</a></h2>
<p>Depth sensing serves as the bridge between understanding what an object is and being able to interact with it effectively. Vision systems powered by deep learning can identify thousands of object categories with impressive accuracy, but without depth information, a humanoid robot cannot answer the most basic question a physical agent must solve: &quot;How do I get from here to there without colliding with anything?&quot;</p>
<p>Consider the core capabilities that depth sensing unlocks for humanoid robots. <strong>Safe navigation</strong> depends entirely on knowing obstacle distancesâ€”a robot navigating a home must detect furniture, walls, and floor transitions to plan collision-free paths. Even a sophisticated vision system that recognizes &quot;chair&quot; or &quot;table&quot; cannot safely navigate without knowing whether that chair is 10 centimeters or 3 meters away. Depth sensors provide this crucial spatial information in real-time, updating measurements 10 to 40 times per second to account for moving obstacles and dynamic environments.</p>
<p><strong>Manipulation and grasping</strong> represent another domain where depth sensing proves indispensable. When a humanoid robot reaches for a water bottle on a counter, it must extend its arm to precisely the right distance and angle. Camera-based object detection might locate the bottle in the image, but depth sensing determines the exact 3D position: 45 centimeters forward, 15 centimeters to the right, and 80 centimeters above the floor. This precision enables smooth, confident grasping rather than tentative visual servoing where the robot repeatedly adjusts based on image feedback.</p>
<p>Beyond individual tasks, depth sensing enables <strong>3D scene understanding</strong>â€”the ability to construct and maintain a coherent spatial model of the environment. This capability underpins Simultaneous Localization and Mapping (SLAM), where robots build metric maps of unknown spaces while tracking their own position within those maps. Without depth information, SLAM algorithms would rely solely on visual features, which fail in textureless environments like white hallways or poorly lit spaces. Depth measurements provide geometric constraints that make localization robust even when visual features are sparse.</p>
<p><strong>Human-robot safety</strong> emerges as perhaps the most critical application of depth sensing in humanoid robotics. Unlike industrial robots that operate in safety cages, humanoid robots work directly alongside humans in shared spaces. Depth sensors continuously monitor the robot&#x27;s surroundings, detecting when humans approach and triggering protective behaviorsâ€”slowing movement, adjusting paths, or stopping entirely if someone enters a danger zone. This real-time proximity detection transforms robots from potential hazards into safe collaborative partners.</p>
<p>Real-world humanoid robots demonstrate these principles in practice. <strong>Boston Dynamics&#x27; Atlas</strong> integrates multiple LiDAR sensors with stereo cameras to enable autonomous navigation through complex terrain, from rocky outdoor environments to cluttered industrial spaces. The depth sensors provide the 3D awareness Atlas needs to plan foot placements on uneven ground and avoid obstacles during dynamic running and jumping. <strong>Tesla Optimus</strong>, designed for warehouse and manufacturing tasks, relies on depth sensing for object manipulationâ€”grasping packages, placing items on shelves, and navigating dynamic environments where humans and forklifts move unpredictably.</p>
<p>Earlier research platforms illustrated these same principles. The <strong>PR-2 robot</strong> from Willow Garage pioneered the integration of RGB-D cameras (combining color images with depth) for household manipulation tasks. Its depth cameras enabled robust grasping of everyday objects, even in cluttered refrigerators and cabinets where lighting conditions challenged pure vision systems. <strong>Boston Dynamics&#x27; Spot quadruped</strong> showcases industrial-grade depth sensing with 3D LiDAR for mapping, localization, and obstacle detection in outdoor construction sites and inspection scenarios where GPS fails and visual odometry degrades.</p>
<p>What becomes impossible without depth sensing? Any task requiring accurate knowledge of object distances failsâ€”navigation in obstacle-rich environments becomes guesswork, grasping without visual servoing proves unreliable, and collision-free path planning degrades to reactive bumping behaviors. Depth sensing transforms robots from cautious, tentative agents into confident systems that understand their spatial environment with precision comparable to human depth perception.</p>
<blockquote>
<p><strong>ðŸŽ“ Expert Insight</strong>: Many roboticists assume that &quot;more sophisticated = better,&quot; leading them to choose 3D LiDAR for tasks that are better solved with 2D LiDAR or depth cameras. In reality, 3D LiDAR generates massive point clouds (millions of points per second) that require significant computational resources to process. A humanoid robot with limited onboard compute may spend all its processing power filtering and downsampling the point cloud instead of making navigation decisions.</p>
<p>For humanoid robot navigation in structured environments (homes, offices), a <strong>2D LiDAR at waist level often suffices</strong> for safe path planning. Add a depth camera (like RealSense) for manipulation tasks. Reserve 3D LiDAR for outdoor SLAM or high-speed dynamic environments (sports robots, disaster response robots) where full 3D awareness is non-negotiable.</p>
<p><strong>Best practice</strong>: Match your depth sensor to your task and compute budget. A 2D LiDAR + depth camera combination often outperforms a single 3D LiDAR on humanoid robots with tight processing constraints.</p>
</blockquote>
<p>The fundamental insight is this: while cameras tell you <em>what</em> an object is, depth sensors tell you <em>where</em> it exists in 3D space. Together, these modalities create the perceptual foundation for physical intelligence.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-principles">Key Principles<a href="#key-principles" class="hash-link" aria-label="Direct link to Key Principles" title="Direct link to Key Principles" translate="no">â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="31-depth-sensing-technologies-and-trade-offs">3.1 Depth Sensing Technologies and Trade-offs<a href="#31-depth-sensing-technologies-and-trade-offs" class="hash-link" aria-label="Direct link to 3.1 Depth Sensing Technologies and Trade-offs" title="Direct link to 3.1 Depth Sensing Technologies and Trade-offs" translate="no">â€‹</a></h3>
<p>Depth sensing technologies fall into four main categories, each with distinct operating principles, capabilities, and limitations. Understanding these trade-offs is essential for designing effective humanoid robot perception systems.</p>
<p><strong>LiDAR (Light Detection and Ranging)</strong> operates by emitting laser pulses and measuring the time-of-flight (ToF) of reflected light to calculate distance. The technology divides into two primary variants based on scanning patterns.</p>
<p><strong>2D LiDAR</strong> performs planar scanning with a single laser at a fixed vertical angle, typically mounted 10-30 centimeters above ground level. A motor rotates the laser through a 360-degree horizontal sweep, measuring distance at each angle increment. The result is a 2D slice of the environment in a single horizontal planeâ€”imagine cutting through a room at waist height and measuring the distance to every surface in that slice. Each complete rotation takes 25-100 milliseconds, providing scan rates of 10-40 Hz. The advantages are substantial: simple data interpretation, robust outdoor performance unaffected by sunlight, and proven reliability in industrial applications. The primary limitation is the inability to detect obstacles above or below the scanning planeâ€”a 2D LiDAR might miss a hanging branch at head height or a hole in the floor. Common models include the SICK LMS111 and Hokuyo UTM-30LX, with typical ranges of 10-30 meters and accuracy around Â±3-5cm. In ROS2, these sensors publish <code>sensor_msgs/LaserScan</code> messages containing arrays of range measurements paired with angular positions.</p>
<p><strong>3D LiDAR</strong> extends this principle to volumetric scanning using an array of 16, 32, or 64 laser diodes arranged vertically. All lasers rotate simultaneously, each capturing a horizontal sweep at a different vertical angle. This creates a dense 3D point cloud with multiple vertical layersâ€”typically covering a 15-40 degree vertical field of view. High-end models generate 300,000 to 2 million points per second at 10-20 Hz frame rates. The advantages include complete 3D environmental awareness, ability to detect obstacles at any height, and rich data for detailed scene reconstruction and SLAM. The disadvantages are equally significant: dramatically higher cost (often 10-50 times more expensive than 2D LiDAR), increased power consumption, and intensive computational requirements for point cloud processing. Examples include Velodyne VLP-16, Livox Mid-70, and SICK Multiscan100, with ranges extending 50-100 meters depending on surface reflectivity. These sensors publish <code>sensor_msgs/PointCloud2</code> messages containing unstructured collections of 3D points.</p>
<p><strong>Structured Light (Active Stereo)</strong> technology projects an infrared (IR) pattern onto the scene and captures the distortion of that pattern through one or more cameras. By analyzing how the known pattern deforms on different surfaces, the system infers depth at each pixel. This approach excels at providing accurate depth measurements even for featureless surfaces that challenge passive stereo visionâ€”a white wall or smooth table creates clear depth measurements despite lacking visual texture. The technology operates at fast frame rates (30-60 Hz) with excellent accuracy at close range (Â±1-2cm within 0.5-4 meters). However, structured light sensors fail completely outdoors because sunlight overpowers the IR pattern. They also struggle with transparent surfaces like glass and highly reflective materials like polished metal. The Microsoft Kinect sensor popularized this approach for robotics research, and modern examples include Intel RealSense D455, Asus Xtion, and Orbbec Astra cameras. These sensors typically publish depth as <code>sensor_msgs/Image</code> with 16-bit or 32-bit float encoding (each pixel value represents depth in millimeters), often accompanied by synchronized RGB images.</p>
<p><strong>Time-of-Flight (ToF) Cameras</strong> measure depth by emitting IR pulses and directly measuring the time for reflected light to return to each pixel. Unlike structured light which requires pattern projection and triangulation, ToF cameras directly calculate depth at every pixel simultaneously. This makes them faster than structured light and more robust to outdoor lightingâ€”while still affected by bright sunlight, ToF sensors perform better outdoors than structured light. The trade-off is lower spatial resolution (typically 320Ã—240 or 640Ã—480 pixels versus megapixel RGB cameras) and medium range limitations (0.5-10 meters effective range). Accuracy falls in the Â±2-5cm range, less precise than structured light at close distances but more versatile across different environments. Examples include the Microsoft Kinect v3 (Azure Kinect) and certain Intel RealSense models operating in ToF mode. Like structured light sensors, ToF cameras publish depth images or point clouds through standard ROS2 messages.</p>
<p>The comparison table below summarizes key characteristics:</p>
<table><thead><tr><th>Technology</th><th>Range</th><th>FOV</th><th>Accuracy</th><th>Outdoor</th><th>Cost</th><th>Compute</th><th>Best For</th></tr></thead><tbody><tr><td>2D LiDAR</td><td>10-30m</td><td>360Â° horiz</td><td>Â±3-5cm</td><td>Excellent</td><td>$$</td><td>Low</td><td>Ground navigation</td></tr><tr><td>3D LiDAR</td><td>50-100m</td><td>Multi-plane</td><td>Â±3-5cm</td><td>Excellent</td><td>$$$$</td><td>High</td><td>Full 3D SLAM</td></tr><tr><td>Structured Light</td><td>0.5-4m</td><td>~70-80Â°</td><td>Â±1-2cm</td><td>Poor (indoor only)</td><td>$</td><td>Medium</td><td>Tabletop manipulation</td></tr><tr><td>ToF Camera</td><td>0.5-10m</td><td>~70-90Â°</td><td>Â±2-5cm</td><td>Fair</td><td>$$</td><td>Low-Medium</td><td>Balanced indoor/outdoor</td></tr></tbody></table>
<p>Choosing the right depth sensor requires matching technology to task requirements, environmental constraints, and computational budget. A humanoid robot operating indoors might use structured light for manipulation and 2D LiDAR for navigation, while an outdoor disaster response robot would require 3D LiDAR despite the higher cost and processing demands.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="32-point-cloud-data-representation">3.2 Point Cloud Data Representation<a href="#32-point-cloud-data-representation" class="hash-link" aria-label="Direct link to 3.2 Point Cloud Data Representation" title="Direct link to 3.2 Point Cloud Data Representation" translate="no">â€‹</a></h3>
<p>A point cloud is an unordered collection of 3D points, each representing a surface measurement in Cartesian space with (x, y, z) coordinates. Unlike images where pixels form a regular grid, point clouds are unstructuredâ€”there is no inherent spatial ordering, and neighboring points in the data array may represent distant surfaces in the physical world.</p>
<p>Most depth sensors output measurements in <strong>Cartesian coordinates</strong> (x, y, z) where x and y define a horizontal plane and z represents vertical height. This representation is intuitive for robot planning algorithms and matches the coordinate systems used in ROS2&#x27;s TF2 transform library. However, many rotating LiDAR sensors naturally measure in <strong>cylindrical coordinates</strong> (range, angle, height) or spherical coordinates (range, azimuth, elevation). These measurements must be converted to Cartesian form through trigonometric transformations before most processing algorithms can use them.</p>
<p>Beyond basic position, point clouds often carry additional attributes. <strong>Intensity</strong> values represent the reflectivity of the surfaceâ€”darker materials absorb more light and return lower intensity values, while retroreflective surfaces produce high intensity. This information helps distinguish between materials (asphalt versus concrete, vegetation versus rock) and can improve segmentation algorithms. <strong>RGB color</strong> data comes from depth cameras that fuse color images with depth measurements, creating colored point clouds where each 3D point also carries red, green, and blue values. This enables powerful fusion of appearance and geometry for object recognition. <strong>Normal vectors</strong> indicate surface orientationâ€”perpendicular to the local surface at each point. While not directly measured, normals are computed from point neighborhoods and enable algorithms to distinguish floors from walls or identify graspable surfaces. <strong>Timestamps</strong> record when each point was captured, critical for fast-moving robots where the sensor pose changes during a single scan.</p>
<p>Point clouds present several computational challenges. Their <strong>unstructured nature</strong> means you cannot simply index into a point cloud like an imageâ€”there is no concept of &quot;the point at row 100, column 50.&quot; Instead, algorithms must search through potentially millions of points to find neighbors or regions of interest. Point clouds vary from <strong>sparse</strong> (2D LiDAR with hundreds of points) to <strong>dense</strong> (structured light cameras with hundreds of thousands of points), requiring different processing strategies. <strong>Noise and outliers</strong> arise from sensor limitations, reflections, shadows, and measurement errors, creating spurious points that don&#x27;t represent real surfaces. A bird flying through the LiDAR scan might create a cluster of points floating in mid-air. Finally, the sheer <strong>scale</strong> of 3D LiDAR dataâ€”up to 2 million points per secondâ€”demands efficient algorithms and often GPU acceleration for real-time processing.</p>
<p>Common point cloud operations address these challenges. <strong>Filtering</strong> removes unwanted points: simple range filters discard measurements beyond a distance threshold, statistical outlier removal identifies isolated points that don&#x27;t fit local point density patterns, and voxel grid filtering downsamples clouds by averaging points within small 3D grid cells. <strong>Segmentation</strong> groups points belonging to the same object or surfaceâ€”clustering algorithms like DBSCAN find spatially connected regions, plane fitting extracts floor and wall surfaces using RANSAC, and more sophisticated methods use machine learning to segment objects by category. <strong>Registration</strong> aligns two point clouds from different viewpoints or time steps, typically using the Iterative Closest Point (ICP) algorithm that iteratively matches corresponding points and minimizes alignment error. <strong>Downsampling</strong> reduces point count while preserving geometric structure, essential for processing 3D LiDAR data in real-time on compute-constrained robots.</p>
<p>Understanding point cloud representation and manipulation forms the foundation for working with 3D depth sensors in humanoid robotics. Whether you&#x27;re building navigation systems, object recognition pipelines, or manipulation controllers, proficiency with point cloud processing directly determines your robot&#x27;s perceptual capabilities.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="33-lidar-principles-2d-vs-3d-scanning">3.3 LiDAR Principles: 2D vs. 3D Scanning<a href="#33-lidar-principles-2d-vs-3d-scanning" class="hash-link" aria-label="Direct link to 3.3 LiDAR Principles: 2D vs. 3D Scanning" title="Direct link to 3.3 LiDAR Principles: 2D vs. 3D Scanning" translate="no">â€‹</a></h3>
<p>The fundamental difference between 2D and 3D LiDAR lies in their scanning patterns and the dimensionality of the resulting measurements.</p>
<p><strong>2D LiDAR</strong> operates with a single laser emitter positioned at a fixed vertical angleâ€”commonly mounted 10-30 centimeters above ground on mobile robots. A motor spins this laser through a complete 360-degree horizontal rotation, pulsing the laser thousands of times per revolution. At each angular position, the sensor measures the time-of-flight of the reflected laser pulse and converts this to distance. The result is a 2D slice of the environment at a single heightâ€”if you imagine looking down at a floor plan, a 2D LiDAR gives you the distances to all walls, furniture, and obstacles at that specific elevation.</p>
<p>This scanning pattern offers several advantages for humanoid robot navigation. The data interpretation is straightforward: each scan produces an ordered array of distances paired with angles, trivially converted to (x, y) obstacle positions. The computational requirements are modestâ€”hundreds to thousands of points per scan, easily processed in real-time on embedded computers. The update rates are fast, with complete 360-degree scans delivered 10-40 times per second, enabling responsive obstacle avoidance. For ground-based navigation tasks where obstacles primarily matter at a consistent height (furniture legs, walls, other robots), 2D LiDAR provides exactly the information needed without excess data.</p>
<p>The limitation, of course, is the inability to detect objects above or below the scanning plane. A 2D LiDAR mounted at waist height might miss a low-hanging branch, an overhead beam, or a hole in the floor. This creates scenarios where the robot believes a path is clear based on 2D measurements, only to collide with obstacles at different heights. For humanoid robots with head-mounted cameras and other sensors, this limitation is often acceptableâ€”the 2D LiDAR handles horizontal navigation while other sensors provide vertical awareness.</p>
<p><strong>3D LiDAR</strong> addresses these limitations by using an array of 16, 32, 64, or more laser emitters arranged vertically. Each laser operates at a fixed vertical angle relative to the sensor, creating a fan of laser beams that together cover a vertical field of viewâ€”typically 15-40 degrees depending on the model. As the entire array rotates horizontally, each laser traces out a horizontal ring at its specific elevation. The result is a dense 3D point cloud with multiple vertical layers, capturing the full volumetric structure of the environment.</p>
<p>A 16-channel 3D LiDAR, for example, might have lasers spaced at 2-degree vertical increments, covering a 30-degree vertical FOV. As this array completes one horizontal rotation, it captures 16 horizontal rings of points at different heights, collectively forming a 3D snapshot of the surroundings. High-end 64-channel units provide even denser vertical sampling, enabling detailed reconstruction of small objects and terrain features.</p>
<p>This volumetric scanning unlocks capabilities impossible with 2D sensing. The robot can detect obstacles at any height, distinguish between navigable space and overhead obstacles, and build detailed 3D maps for SLAM. The point clouds enable sophisticated scene understandingâ€”extracting ground planes, identifying curbs and stairs, segmenting individual objects, and tracking their 3D motion over time. For outdoor navigation on uneven terrain or in complex 3D environments (multi-story buildings, construction sites), 3D LiDAR becomes essential.</p>
<p>The challenges are equally significant. Each rotation produces 300,000 to 2 million points, requiring substantial computational resources for filtering, segmentation, and map building. The sensors themselves are more expensive, often 10-50 times the cost of comparable 2D units. Power consumption increases proportionally with the number of laser channels. The point clouds require careful temporal handlingâ€”since each point is captured at a slightly different time as the sensor rotates, fast robot motion can distort the cloud if not properly corrected using motion compensation algorithms.</p>
<p>Many humanoid robots adopt hybrid approaches: a 2D LiDAR at waist or ankle height for robust ground-level navigation, combined with 3D depth cameras (structured light or ToF) for manipulation and close-range 3D awareness. This combination provides the coverage of 3D sensing where needed while avoiding the computational burden of processing 3D LiDAR data continuously. The 2D LiDAR handles navigation costmaps and localization, while depth cameras activate during manipulation tasks that require detailed 3D geometry.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="34-ros2-messages-for-depth-sensing">3.4 ROS2 Messages for Depth Sensing<a href="#34-ros2-messages-for-depth-sensing" class="hash-link" aria-label="Direct link to 3.4 ROS2 Messages for Depth Sensing" title="Direct link to 3.4 ROS2 Messages for Depth Sensing" translate="no">â€‹</a></h3>
<p>ROS2 standardizes depth sensor data through several message types in the <code>sensor_msgs</code> package, enabling consistent interfaces between sensors, processing algorithms, and applications.</p>
<p><strong>sensor_msgs/LaserScan</strong> represents 2D LiDAR data as an ordered array of range measurements at known angular positions. The key fields include:</p>
<ul>
<li class=""><code>header</code>: Contains timestamp (<code>stamp</code>) and coordinate frame (<code>frame_id</code>, typically &quot;base_link&quot; or &quot;laser_link&quot;)</li>
<li class=""><code>angle_min</code>, <code>angle_max</code>: Start and end angles of the scan in radians (commonly -Ï€ to +Ï€ for 360Â° scans)</li>
<li class=""><code>angle_increment</code>: Angular resolution in radians between consecutive measurements (e.g., 0.01 radians = 0.57 degrees)</li>
<li class=""><code>time_increment</code>: Time between individual range measurements (important for motion compensation)</li>
<li class=""><code>scan_time</code>: Total duration for one complete rotation</li>
<li class=""><code>range_min</code>, <code>range_max</code>: Valid measurement range limits in meters (e.g., 0.1m to 30m)</li>
<li class=""><code>ranges</code>: Float array of distance measurements, one per angular increment</li>
<li class=""><code>intensities</code>: Optional float array of reflectivity values, same length as ranges</li>
</ul>
<p>To use LaserScan data, you iterate through the <code>ranges</code> array, computing the angle for each measurement as <code>angle = angle_min + i * angle_increment</code>. Each (angle, range) pair converts to Cartesian coordinates via <code>x = range * cos(angle)</code> and <code>y = range * sin(angle)</code>, giving obstacle positions in the sensor&#x27;s frame. Invalid measurements (out of range or no return) are typically encoded as infinity or NaN values and must be filtered.</p>
<p><strong>sensor_msgs/PointCloud2</strong> represents 3D point cloud data in a flexible binary format that accommodates varying point attributes. The message structure includes:</p>
<ul>
<li class=""><code>header</code>: Timestamp and coordinate frame reference (e.g., &quot;camera_link&quot; for depth cameras)</li>
<li class=""><code>height</code>, <code>width</code>: Grid dimensionsâ€”organized clouds from depth cameras have height &gt; 1 (like an image); unorganized clouds from rotating LiDAR have height = 1</li>
<li class=""><code>fields</code>: Array of PointField descriptors defining the data layoutâ€”typically includes &quot;x&quot;, &quot;y&quot;, &quot;z&quot; for position, plus optional &quot;intensity&quot;, &quot;rgb&quot;, or custom fields</li>
<li class=""><code>is_bigendian</code>: Byte order indicator for cross-platform compatibility</li>
<li class=""><code>point_step</code>: Number of bytes per point (depends on fields present)</li>
<li class=""><code>row_step</code>: Number of bytes per row (equals <code>point_step * width</code>)</li>
<li class=""><code>data</code>: Raw point data as a byte array, requiring field descriptors to parse</li>
<li class=""><code>is_dense</code>: Boolean indicating whether the cloud contains invalid points (NaN/Inf values)</li>
</ul>
<p>The PointCloud2 format&#x27;s flexibility comes at the cost of parsing complexity. You cannot simply access &quot;the x coordinate of point 47&quot; without first examining the <code>fields</code> array to determine byte offsets and data types. Most ROS2 users rely on helper libraries like <code>pcl_ros</code> (Point Cloud Library integration) or <code>open3d_ros2</code> to convert PointCloud2 messages to structured formats like numpy arrays or native point cloud objects.</p>
<p><strong>Depth Images</strong> from structured light and ToF cameras often publish as <code>sensor_msgs/Image</code> with encoding &quot;16UC1&quot; (16-bit unsigned integer) or &quot;32FC1&quot; (32-bit float). Each pixel value represents depth in millimeters (for integer encodings) or meters (for float encodings). This format is more compact than PointCloud2 for organized depth data and integrates naturally with image processing pipelines. To convert a depth image to a 3D point cloud, you need the camera&#x27;s intrinsic parameters from a <code>sensor_msgs/CameraInfo</code> message, which provides focal lengths and principal point coordinates for the pixel-to-3D transformation.</p>
<p><strong>Message synchronization</strong> becomes critical when fusing depth with other sensors. A humanoid robot might combine depth images with RGB images from the same camera, requiring both messages to be captured at nearly the same timestamp. ROS2&#x27;s <code>message_filters</code> package provides <code>ApproximateTimeSynchronizer</code> to match messages by timestamp with configurable tolerance. This is essential for robots that move during sensor acquisitionâ€”even a 50-millisecond delay between depth and color images can cause misalignment if the robot&#x27;s head is turning or the body is walking.</p>
<p>Understanding these message formats and their trade-offs allows you to design robust depth perception pipelines that integrate seamlessly with ROS2&#x27;s navigation, manipulation, and SLAM packages.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="35-depth-sensor-integration-with-navigation-and-slam">3.5 Depth Sensor Integration with Navigation and SLAM<a href="#35-depth-sensor-integration-with-navigation-and-slam" class="hash-link" aria-label="Direct link to 3.5 Depth Sensor Integration with Navigation and SLAM" title="Direct link to 3.5 Depth Sensor Integration with Navigation and SLAM" translate="no">â€‹</a></h3>
<p>Depth sensors provide the geometric foundation for two critical capabilities in autonomous robotics: Simultaneous Localization and Mapping (SLAM) and navigation planning.</p>
<p><strong>SLAM algorithms</strong> use depth measurements to build maps of unknown environments while simultaneously tracking the robot&#x27;s position within those maps. For 2D LiDAR, algorithms like GMapping and Cartographer consume <code>sensor_msgs/LaserScan</code> messages to construct occupancy gridsâ€”2D maps where each cell is marked as free space, occupied (obstacle), or unknown. As the robot moves, the SLAM algorithm matches new scans to the existing map, refining both the map geometry and the robot&#x27;s estimated position. Loop closure detection recognizes when the robot returns to a previously visited location, correcting accumulated drift and improving global map consistency.</p>
<p>3D SLAM extends these principles to volumetric mapping using <code>sensor_msgs/PointCloud2</code> data. Algorithms like LOAM (LiDAR Odometry and Mapping) and rtabmap extract geometric features from point cloudsâ€”edges from sharp corners, planar surfaces from walls and floorsâ€”and match these features across time to estimate motion. The result is a dense 3D map that captures the complete geometric structure of the environment, enabling localization even in 3D spaces like multi-story buildings where 2D maps are insufficient.</p>
<p><strong>Navigation stack integration</strong> in ROS2 centers on costmapsâ€”probabilistic grids that represent obstacle likelihood and traversability. The <code>nav2_costmap_2d</code> package subscribes to depth sensor topics (LaserScan or PointCloud2) and projects measurements into 2D occupancy grids. Recent depth measurements inflate a <strong>local costmap</strong> around the robot, marking obstacles detected in the last few seconds with high cost values. This local costmap updates at high frequency (typically 5-10 Hz), enabling real-time obstacle avoidance as the robot moves.</p>
<p>A <strong>global costmap</strong> combines SLAM-generated maps with depth sensor data to represent the entire known environment. Path planners like Nav2&#x27;s Planner Server search this global costmap for collision-free paths from the robot&#x27;s current position to goal positions. Local planners then compute velocity commands that follow the global path while avoiding newly detected obstacles in the local costmap.</p>
<p><strong>Frame transformations</strong> through ROS2&#x27;s TF2 library enable depth sensor data to be correctly positioned relative to the robot&#x27;s base and the world map. A depth camera mounted on a humanoid robot&#x27;s head has a fixed transformation relative to the head link, which in turn has a dynamic transformation relative to the torso (if the head can pan/tilt), which transforms to the robot&#x27;s base. TF2 maintains this tree of coordinate frames and allows seamless conversion of point clouds from &quot;camera_link&quot; frame to &quot;base_link&quot; or &quot;map&quot; frame based on the robot&#x27;s current joint angles and global pose.</p>
<p>The typical <strong>perception pipeline</strong> for humanoid robot navigation follows this flow:</p>
<ol>
<li class="">Sensor drivers publish raw LaserScan or PointCloud2 messages at the sensor&#x27;s native frame rate</li>
<li class="">Preprocessing nodes apply filters (range limits, statistical outlier removal, voxel downsampling) to clean the data</li>
<li class="">Segmentation algorithms separate ground plane from obstacles, clustering remaining points into objects</li>
<li class="">SLAM nodes fuse filtered depth data with odometry (from wheel encoders, IMU, or visual odometry) to update the map and robot pose estimate</li>
<li class="">Costmap nodes project recent depth measurements into 2D grids aligned with the map frame</li>
<li class="">Nav2&#x27;s planner generates global paths through the costmap from current position to goals</li>
<li class="">Local planners compute velocity commands that follow the path while avoiding obstacles</li>
<li class="">Motion controllers convert velocity commands into joint trajectories for the robot&#x27;s actuators</li>
</ol>
<p><strong>Practical considerations for humanoid robots</strong> introduce additional complexity. Humanoid gait is inherently dynamicâ€”the robot&#x27;s center of mass shifts during walking, and foot impacts create vibrations that affect sensor measurements. SLAM algorithms must remain robust to these motion-induced distortions, often using IMU data to predict and compensate for body motion during depth sensor acquisition.</p>
<p>Sensor mounting locations matter significantly. Head-mounted depth sensors move when the robot looks around, requiring continuous updates to the TF tree and introducing motion blur if the head moves quickly during a scan. Torso-mounted sensors provide more stable measurements but may have limited field of view blocked by the robot&#x27;s arms. Many designs use multiple depth sensorsâ€”a 2D LiDAR at waist height for navigation, stereo cameras or depth cameras in the head for manipulationâ€”requiring temporal and spatial fusion to maintain a coherent world model.</p>
<p><strong>Latency sensitivity</strong> affects navigation responsiveness. If depth processing takes 200 milliseconds and the robot walks at 1 meter per second, the costmap represents where obstacles were 20 centimeters ago. For dynamic obstacles (moving humans, other robots), this delay can cause collisions. High-frequency local costmap updates (10-20 Hz) and efficient point cloud processing (GPU acceleration, optimized algorithms) are essential for safe operation.</p>
<p>Multi-sensor fusion combines depth sensing with other modalities for robustness. A 2D LiDAR provides reliable long-range obstacle detection but misses vertical obstacles. Depth cameras fill this gap at close range. Stereo cameras provide dense 3D reconstruction but fail in poor lighting. Fusing these complementary sensors through weighted costmap layers creates perception systems more robust than any single sensor.</p>
<blockquote>
<p><strong>ðŸ¤ Practice Exercise</strong>: You&#x27;re designing perception for a humanoid robot that must:</p>
<ol>
<li class="">Navigate autonomously through a home with furniture, stairs, and obstacles</li>
<li class="">Reach and grasp household objects (cups, books, bottles)</li>
<li class="">Maintain balance while walking on slightly uneven floors</li>
<li class="">Safely avoid humans and other moving objects</li>
</ol>
<p>For each capability, identify:</p>
<ul>
<li class="">Which depth sensor(s) would you use? (2D LiDAR, 3D LiDAR, structured light camera, ToF camera, or combination)</li>
<li class="">What ROS2 messages would you subscribe to? (<code>sensor_msgs/LaserScan</code>, <code>sensor_msgs/PointCloud2</code>, depth image)</li>
<li class="">How would you integrate the sensor data with navigation (SLAM, costmaps, path planning)?</li>
<li class="">What are the processing requirements (point cloud filtering, segmentation) for your choice?</li>
</ul>
<p><strong>Advanced variation</strong>: Consider cost and power constraints. Your robot must run on a battery for 8 hours. Which sensor combination would you choose to balance capability and efficiency?</p>
<p><strong>Optional</strong>: Ask Claude to review your proposed sensor configuration and suggest improvements based on real humanoid robot designs.</p>
</blockquote>
<p>Understanding depth sensor integration with SLAM and navigation transforms isolated measurements into actionable spatial awareness, enabling humanoid robots to move confidently and safely through complex human environments.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="real-world-case-studies-depth-sensing-in-production-robots">Real-World Case Studies: Depth Sensing in Production Robots<a href="#real-world-case-studies-depth-sensing-in-production-robots" class="hash-link" aria-label="Direct link to Real-World Case Studies: Depth Sensing in Production Robots" title="Direct link to Real-World Case Studies: Depth Sensing in Production Robots" translate="no">â€‹</a></h2>
<p>The principles above translate into concrete engineering choices made by leading robotics manufacturers. The following case studies demonstrate how different robots select and deploy depth sensors based on their specific operational requirements and constraints.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="-case-study-boston-dynamics-spot---3d-lidar-for-outdoor-slam-and-inspection">ðŸ“Š Case Study: Boston Dynamics Spot - 3D LiDAR for Outdoor SLAM and Inspection<a href="#-case-study-boston-dynamics-spot---3d-lidar-for-outdoor-slam-and-inspection" class="hash-link" aria-label="Direct link to ðŸ“Š Case Study: Boston Dynamics Spot - 3D LiDAR for Outdoor SLAM and Inspection" title="Direct link to ðŸ“Š Case Study: Boston Dynamics Spot - 3D LiDAR for Outdoor SLAM and Inspection" translate="no">â€‹</a></h3>
<p><strong>Robot</strong>: Boston Dynamics Spot (quadrupedal robot)
<strong>Depth Sensor</strong>: Velodyne VLP-16 3D LiDAR (16-channel, 100m range)
<strong>Application</strong>: Autonomous inspection and mapping of industrial sites, construction areas, and disaster response scenarios</p>
<p>Boston Dynamics Spot operates in unstructured outdoor environments where GPS fails, visual features are sparse, and terrain varies dramaticallyâ€”muddy construction sites, rocky disaster zones, and weathered industrial facilities. The choice of 3D LiDAR reflects these environmental demands. The Velodyne VLP-16 generates 300,000 points per second across 16 vertical layers, providing dense geometric data for SLAM even in featureless environments like gravel yards or concrete parking lots where visual odometry would fail.</p>
<p>The sensor&#x27;s mounting on Spot&#x27;s body serves dual purposes: simultaneous localization and mapping during exploration missions, and continuous obstacle detection for dynamic path planning. The 16-channel vertical distribution enables detection of terrain features critical for quadruped locomotionâ€”curbs, rocks, and surface irregularities that 2D LiDAR would miss entirely. Boston Dynamics&#x27; custom SLAM algorithm fuses the 3D LiDAR with proprioceptive sensors (leg encoder feedback) to maintain accurate localization even during dynamic gaits where the body platform accelerates and the sensor viewpoint changes rapidly.</p>
<p>The computational challenge is substantial: 300,000 points per second requires significant onboard processing for filtering, ground plane extraction, and loop closure detection. Spot addresses this through GPU acceleration on its onboard compute module, enabling real-time SLAM at 5 Hz update rates despite the massive point density. This represents a deliberate trade-off: higher computational cost and power consumption in exchange for robust outdoor SLAM where visual features are unreliable.</p>
<p><strong>Key Takeaway</strong>: For unstructured outdoor environments where visual features fail, 3D LiDAR&#x27;s computational cost is justified by the geometric richness needed for reliable SLAM and safe autonomous navigation.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="-case-study-agility-robotics-digit---depth-sensing-for-bipedal-stair-and-terrain-detection">ðŸ“Š Case Study: Agility Robotics Digit - Depth Sensing for Bipedal Stair and Terrain Detection<a href="#-case-study-agility-robotics-digit---depth-sensing-for-bipedal-stair-and-terrain-detection" class="hash-link" aria-label="Direct link to ðŸ“Š Case Study: Agility Robotics Digit - Depth Sensing for Bipedal Stair and Terrain Detection" title="Direct link to ðŸ“Š Case Study: Agility Robotics Digit - Depth Sensing for Bipedal Stair and Terrain Detection" translate="no">â€‹</a></h3>
<p><strong>Robot</strong>: Agility Robotics Digit (bipedal humanoid)
<strong>Depth Sensor</strong>: Intel RealSense D435i (active stereo, 1280Ã—720 RGB-D, 0.3-10m range optimal 0.3-3m)
<strong>Application</strong>: Autonomous navigation of stairs, curbs, and mixed terrain during package delivery tasks</p>
<p>Digit was designed for last-mile package deliveryâ€”a task requiring humanoid bipedal locomotion through residential neighborhoods with sidewalks, stairs, and natural terrain. Unlike wheeled or quadrupedal robots, bipedal walkers have a narrow support base, making balance critically sensitive to terrain geometry. Uneven steps or unexpected drops can destabilize the robot, creating falls or tipping hazards.</p>
<p>Agility&#x27;s engineering team selected the Intel RealSense D435i active stereo camera for its accuracy in close-range depth perception and robust indoor/outdoor performance. Mounted in Digit&#x27;s head, the camera provides high-resolution depth maps (1280Ã—720 pixels) that reveal terrain texture and elevation changes within 0.5-6 metersâ€”the critical range for bipedal footfall planning. The active stereo approach (IR pattern projection + stereo matching) performs better outdoors than traditional structured light, making it suitable for the afternoon delivery window typical of package delivery operations. The synchronized RGB images enable the robot&#x27;s vision system to identify stairs visually while depth data measures exact step geometryâ€”critical for bipeds that must place feet on narrow stair treads.</p>
<p>The processing pipeline extracts stair edges and terrain planes from the depth image using RANSAC-based surface fitting. These geometric features feed directly into the motion planning algorithm, which computes safe foot placements and adjusts step height and stride length to match detected terrain. Unlike legged quadrupeds that can step over obstacles, bipedal robots must navigate surface continuity carefullyâ€”a hidden step could trigger balance recovery or a fall.</p>
<p>The RealSense choice reflects a focused trade-off: lower range (10m versus 50m for LiDAR) accepted in exchange for high spatial resolution at close range where bipedal gait planning operates. The 90Â° field of view provides sufficient angular coverage for head-mounted sensing. Power consumption remains modestâ€”critical for a battery-powered humanoid that must complete delivery routes.</p>
<p><strong>Key Takeaway</strong>: For humanoid platforms with narrow stability margins, high-resolution close-range depth sensing enables precise terrain analysis for safe bipedal locomotion on complex surfaces beyond flat ground.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="-case-study-pr-2-robot-willow-garage---kinect-rgb-d-for-manipulation-in-cluttered-household-scenes">ðŸ“Š Case Study: PR-2 Robot (Willow Garage) - Kinect RGB-D for Manipulation in Cluttered Household Scenes<a href="#-case-study-pr-2-robot-willow-garage---kinect-rgb-d-for-manipulation-in-cluttered-household-scenes" class="hash-link" aria-label="Direct link to ðŸ“Š Case Study: PR-2 Robot (Willow Garage) - Kinect RGB-D for Manipulation in Cluttered Household Scenes" title="Direct link to ðŸ“Š Case Study: PR-2 Robot (Willow Garage) - Kinect RGB-D for Manipulation in Cluttered Household Scenes" translate="no">â€‹</a></h3>
<p><strong>Robot</strong>: PR-2 (Personal Robot 2, mobile manipulator)
<strong>Depth Sensor</strong>: Microsoft Kinect v1 (structured light RGB-D, 640Ã—480, 0.4-4m range)
<strong>Application</strong>: Object grasping, shelf organization, and household manipulation in unstructured home environments</p>
<p>The PR-2, developed by Willow Garage, became the research platform for mobile manipulation during the early 2010s. Its missionâ€”enabling service robots to manipulate household objects in unstructured environmentsâ€”required depth sensing that could identify both object locations and grasp-relevant geometry (handles, edges, surface normals) without relying on perfect ambient lighting or textured surfaces.</p>
<p>The Kinect v1 sensor, mounted in the PR-2&#x27;s head and shoulder, provided exactly this capability. The structured light pattern excels in the 0.5-4 meter range typical for manipulationâ€”close enough to enable detailed geometry analysis but far enough for arm reach planning. The RGB component allowed the robot&#x27;s vision system to recognize object categories (mugs, bottles, boxes) while the synchronized depth stream revealed exact 3D positions and surface geometry. This combination enabled the PR-2 to locate a water bottle on a cluttered kitchen shelf, estimate its 3D position and orientation, plan a collision-free arm trajectory, and execute a grasp without toppling other objects.</p>
<p>A critical advantage of structured light for household tasks: performance independent of surface texture. A white plate (featureless to visual tracking) still produces clear depth measurements through structured light pattern analysis. This robustness enabled grasping of a wider variety of household objects than purely visual approaches. The 640Ã—480 resolution was sufficient to identify grasp points (rim of a cup, handle of a mug) at typical manipulation distances.</p>
<p>The Kinect&#x27;s main limitationâ€”failure in bright sunlightâ€”proved acceptable for indoor manipulation tasks. However, over time, the focus on RGB-D sensing from fixed depth cameras gave way to hybrid approaches combining multiple sensors. Modern mobile manipulators often integrate lightweight depth cameras with 2D LiDAR for navigation and stereo cameras for finer visual detail during manipulation.</p>
<p><strong>Key Takeaway</strong>: For manipulation-focused systems operating indoors with varied object surfaces, RGB-D cameras provide the tight coupling of appearance and geometry needed for robust grasping without texture dependence.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="practical-example-subscribing-to-laserscan-data">Practical Example: Subscribing to LaserScan Data<a href="#practical-example-subscribing-to-laserscan-data" class="hash-link" aria-label="Direct link to Practical Example: Subscribing to LaserScan Data" title="Direct link to Practical Example: Subscribing to LaserScan Data" translate="no">â€‹</a></h2>
<p>This example demonstrates a ROS2 node that subscribes to 2D LiDAR data and performs basic obstacle detectionâ€”a fundamental pattern for humanoid robot navigation systems.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> rclpy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> rclpy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">node </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Node</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> sensor_msgs</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">msg </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> LaserScan</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> numpy </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> np</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> typing </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Optional</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">DepthObstacleDetector</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Node</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;Subscribes to LaserScan and detects nearby obstacles.&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;depth_obstacle_detector&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Subscribe to 2D LiDAR scan topic</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">scan_subscription </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">create_subscription</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            LaserScan</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string" style="color:#e3116c">&#x27;/scan&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">scan_callback</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token number" style="color:#36acaa">10</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># QoS queue depth</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Configurable danger zone threshold (meters)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">danger_threshold</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">float</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_logger</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">info</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;Obstacle detector initialized&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">scan_callback</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> msg</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> LaserScan</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;Process LaserScan message and detect close obstacles.&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Convert ranges to numpy array for efficient element-wise operations</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># This creates a 1D array of distance measurements from the LiDAR sensor</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ranges</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ndarray </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">array</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ranges</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Generate corresponding angles for each range measurement using linear spacing</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># linspace divides the angular range evenly across all measurements for consistent resolution</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        num_readings</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">int</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token builtin">len</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">ranges</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        angles</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ndarray </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">linspace</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">angle_min</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">angle_max</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            num_readings</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Filter invalid measurements (inf/nan values) using boolean indexing</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># isfinite() returns True only for valid numeric values, filtering sensor errors and out-of-range returns</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        valid_mask</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ndarray </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">isfinite</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">ranges</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        valid_ranges</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ndarray </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> ranges</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">valid_mask</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Keep only valid distance measurements</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        valid_angles</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ndarray </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> angles</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">valid_mask</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">   </span><span class="token comment" style="color:#999988;font-style:italic"># Keep corresponding valid angles</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Find obstacles within danger zone using vectorized comparison</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Boolean mask identifies all points closer than the threshold distance</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        close_mask</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ndarray </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> valid_ranges </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">danger_threshold</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        close_obstacles</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ndarray </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> valid_ranges</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">close_mask</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">      </span><span class="token comment" style="color:#999988;font-style:italic"># Extract dangerous distances</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        obstacle_angles</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ndarray </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> valid_angles</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">close_mask</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">      </span><span class="token comment" style="color:#999988;font-style:italic"># Extract dangerous angles</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> </span><span class="token builtin">len</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">close_obstacles</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token comment" style="color:#999988;font-style:italic"># Convert angles from radians to degrees for human-readable logging</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            angles_deg</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ndarray </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">degrees</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">obstacle_angles</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token comment" style="color:#999988;font-style:italic"># Find minimum distance using argmin to locate closest obstacle index</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            min_distance</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">float</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> np</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">min</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">close_obstacles</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_logger</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">warn</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token string-interpolation string" style="color:#e3116c">f&#x27;DANGER: </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation builtin">len</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">(</span><span class="token string-interpolation interpolation">close_obstacles</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">)</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c"> obstacles within </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">self</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">danger_threshold</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">m! &#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token string-interpolation string" style="color:#e3116c">f&#x27;Closest: </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">min_distance</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">:</span><span class="token string-interpolation interpolation format-spec">.2f</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">m at </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">angles_deg</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">[</span><span class="token string-interpolation interpolation">np</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">argmin</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">(</span><span class="token string-interpolation interpolation">close_obstacles</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">)</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">]</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">:</span><span class="token string-interpolation interpolation format-spec">.1f</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">Â°&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">main</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Optional</span><span class="token punctuation" style="color:#393A34">[</span><span class="token builtin">list</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    rclpy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">init</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">args</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    node </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> DepthObstacleDetector</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    rclpy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">spin</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">node</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    node</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">destroy_node</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    rclpy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">shutdown</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> __name__ </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;__main__&#x27;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    main</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>This node demonstrates the fundamental pattern for integrating depth sensors into ROS2-based humanoid robots. The <code>scan_callback</code> method executes each time a new <code>LaserScan</code> message arrives on the <code>/scan</code> topicâ€”typically 10-40 times per second from a 2D LiDAR sensor. The <code>sensor_msgs/LaserScan</code> message contains an array of distance measurements (<code>ranges</code>) at known angular positions, along with metadata defining the angular extent and resolution of the scan.</p>
<p>The code extracts these range measurements into a NumPy array for efficient numerical processing, then generates the corresponding angle for each measurement using <code>np.linspace</code> with the message&#x27;s <code>angle_min</code>, <code>angle_max</code>, and array length. This pairing of distances with angles enables conversion to Cartesian coordinates if needed, though this example works directly in polar coordinates for obstacle detection.</p>
<p>A critical step is filtering invalid measurements. LiDAR sensors encode out-of-range readings as infinity values and missed detections as NaN (not-a-number). The code uses <code>np.isfinite()</code> to create a boolean mask selecting only valid measurements before performing obstacle detection. Skipping this step would cause errors when comparing infinity values against thresholds.</p>
<p>The obstacle detection logic identifies measurements falling below a danger thresholdâ€”here set to 1 meter, a typical safety margin for humanoid navigation. By applying a boolean mask to the valid ranges, the code efficiently extracts all nearby obstacles and their angular positions. If obstacles are detected, the system logs a warning including the count, closest distance, and angular position in degrees (converted from radians for readability).</p>
<p>In production humanoid robot systems, this callback would not just log warnings but would actively trigger navigation behaviors. The detected obstacles might be published to a costmap layer, fed into local path planners for avoidance maneuvers, or used to modulate the robot&#x27;s walking speed when approaching narrow passages. The pattern remains the same: subscribe to depth messages, process the data to extract actionable information, and integrate with higher-level planning and control systems.</p>
<p>For 3D depth sensors publishing <code>sensor_msgs/PointCloud2</code>, the subscription pattern is identicalâ€”only the callback processing changes. Instead of iterating through a 1D range array, you would use libraries like <code>pcl_ros</code> or <code>open3d_ros2</code> to parse the binary point cloud data into structured 3D coordinates. The core principle persists: ROS2 messages deliver sensor data to your node, callbacks process that data in real-time, and the extracted information drives robot behaviors.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="practical-example-subscribing-to-pointcloud2-data">Practical Example: Subscribing to PointCloud2 Data<a href="#practical-example-subscribing-to-pointcloud2-data" class="hash-link" aria-label="Direct link to Practical Example: Subscribing to PointCloud2 Data" title="Direct link to Practical Example: Subscribing to PointCloud2 Data" translate="no">â€‹</a></h2>
<p>For 3D depth sensing, this example demonstrates a ROS2 node that subscribes to point cloud data from a depth camera or 3D LiDAR sensor and extracts metadata and coordinate information from the binary point cloud formatâ€”a fundamental pattern for depth camera-based manipulation and close-range 3D perception in humanoid robots.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> rclpy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> rclpy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">node </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Node</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> sensor_msgs</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">msg </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> PointCloud2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> struct</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> typing </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Optional</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> Tuple</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">PointCloudSubscriber</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Node</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;Subscribes to PointCloud2 and extracts 3D point data and statistics.&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;pointcloud_subscriber&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Subscribe to depth camera point cloud topic</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">subscription </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">create_subscription</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            PointCloud2</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string" style="color:#e3116c">&#x27;/camera/depth/points&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">pointcloud_callback</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token number" style="color:#36acaa">10</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># QoS queue depth</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_logger</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">info</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;PointCloud subscriber initialized on /camera/depth/points&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">pointcloud_callback</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> msg</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> PointCloud2</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;Process point cloud and extract 3D coordinate and statistical data.&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Calculate total number of points from height and width dimensions</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># For organized clouds (from depth cameras), dimensions match image grid</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># For unorganized clouds (from rotating LiDAR), typically height=1, width=num_points</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        total_points</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">int</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">height </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">width</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Extract binary point size to properly parse coordinates</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># point_step indicates bytes per point (typically 12-16 bytes for x,y,z + optional fields)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        point_step</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">int</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">point_step</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Ensure we have data to process and avoid index errors</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> </span><span class="token builtin">len</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">data</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">or</span><span class="token plain"> total_points </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_logger</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">warn</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;Received empty point cloud&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Extract first point&#x27;s x,y,z coordinates from binary data</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Most depth cameras encode coordinates as 3 consecutive 32-bit floats</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># struct.unpack_from unpacks binary data at specific offset without copying</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">try</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            x</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">float</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            y</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">float</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            z</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">float</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> z </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> struct</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">unpack_from</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;fff&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">data</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">except</span><span class="token plain"> struct</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">error </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> e</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_logger</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">error</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string-interpolation string" style="color:#e3116c">f&#x27;Failed to parse point cloud data: </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">e</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Log comprehensive point cloud metadata and sample point</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_logger</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">info</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string-interpolation string" style="color:#e3116c">f&#x27;PointCloud received: </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">total_points</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c"> points, &#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string-interpolation string" style="color:#e3116c">f&#x27;dims=</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">msg</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">height</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">x</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">msg</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">width</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">, &#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string-interpolation string" style="color:#e3116c">f&#x27;first_point=(</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">x</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">:</span><span class="token string-interpolation interpolation format-spec">.3f</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">, </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">y</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">:</span><span class="token string-interpolation interpolation format-spec">.3f</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">, </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">z</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">:</span><span class="token string-interpolation interpolation format-spec">.3f</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">)m, &#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string-interpolation string" style="color:#e3116c">f&#x27;frame=</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">msg</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">header</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">frame_id</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">main</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Optional</span><span class="token punctuation" style="color:#393A34">[</span><span class="token builtin">list</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    rclpy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">init</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">args</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">args</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    node </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> PointCloudSubscriber</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    rclpy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">spin</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">node</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    node</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">destroy_node</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    rclpy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">shutdown</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> __name__ </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;__main__&#x27;</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    main</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>This example demonstrates ROS2 integration with 3D depth sensors that publish <code>sensor_msgs/PointCloud2</code> messagesâ€”the standard format for structured light cameras (Intel RealSense), ToF cameras, and 3D LiDAR sensors. Unlike the ordered array structure of <code>LaserScan</code>, point clouds store 3D coordinates in a flexible binary format that requires careful parsing.</p>
<p>The <code>PointCloud2</code> message contains height and width fields that describe the point cloud structure. For organized clouds from depth cameras, these dimensions match the resolution of the underlying image (e.g., 480 height Ã— 640 width pixels). For unorganized clouds from rotating LiDAR sensors, typically height equals 1 and width equals the total point count. Multiplying height Ã— width gives the total number of points in the cloud.</p>
<p>The critical detail is the <code>point_step</code> field, which specifies how many bytes each point occupies in the binary data array. A typical 3D point with (x, y, z) coordinates uses 12 bytes (3 floats Ã— 4 bytes each), but cameras may include additional fields like intensity, RGB color, or normal vectors, increasing point_step to 16, 20, or more bytes. The <code>struct.unpack_from()</code> function safely extracts binary data by interpreting bytes at a specific offset without copying the entire data arrayâ€”efficient for large point clouds.</p>
<p>The callback gracefully handles errors that might occur during parsing. Empty point clouds (which can occur during initialization or sensor disconnections) are detected and logged rather than causing crashes. The try-except block catches <code>struct.error</code> exceptions that might occur if the binary format doesn&#x27;t match the expected float layout, providing diagnostic information for debugging sensor configuration issues.</p>
<p>In production humanoid robot systems using depth cameras for manipulation, this callback might extract not just the first point but process the entire cloud to find the closest point, identify clustered objects, or locate grasp-able surfaces. For head-mounted depth cameras on manipulation tasks, this pattern forms the foundation for vision-based reaching and graspingâ€”the camera publishes organized point clouds at 30 Hz, each callback processes the cloud to find target objects, and detected 3D positions are published for the arm controller to reach toward.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary" translate="no">â€‹</a></h2>
<p>Depth sensing transforms humanoid robots from cautious observers into confident spatial agents capable of safe navigation and precise manipulation. Five key insights characterize effective depth sensing systems:</p>
<p><strong>Depth sensing complements vision</strong>: While cameras capture what objects look like through 2D images, depth sensors measure where objects exist in 3D space. This complementary relationship underlies modern perception systemsâ€”vision identifies what to interact with, depth determines how to reach it safely. Together, they enable robots to perceive, plan, and interact with their environment at human-like levels of spatial awareness.</p>
<p><strong>Technology trade-offs dominate sensor selection</strong>: 2D LiDAR provides robust, computationally efficient navigation suitable for structured environments, delivering 360-degree obstacle detection at waist height with minimal processing overhead. 3D LiDAR offers rich volumetric maps essential for outdoor SLAM and complex 3D environments, at the cost of massive computational requirements and higher price points. Structured light cameras excel at close-range manipulation tasks with millimeter-level accuracy but fail completely in outdoor sunlight. ToF cameras provide a balanced middle ground with faster processing than structured light and better outdoor performance, though with lower resolution. Your sensor choice must align with task requirements, environmental constraints, and computational budgetâ€”there is no universal best sensor, only the right match for your specific application.</p>
<p><strong>Point clouds are the universal 3D representation</strong>: Whether from 2D LiDAR producing structured scans or 3D sensors generating millions of unstructured points, depth data ultimately represents surfaces as collections of (x, y, z) coordinates. Understanding point cloud operationsâ€”filtering noise, segmenting objects, downsampling for efficiency, and registering multiple viewsâ€”forms the foundation of 3D perception. The challenges of working with unstructured data (no inherent ordering, variable density, noise and outliers) demand careful algorithm design and often GPU acceleration for real-time processing on resource-constrained humanoid robots.</p>
<p><strong>ROS2 integration enables systematic perception pipelines</strong>: Depth data flows through standardized message typesâ€”<code>sensor_msgs/LaserScan</code> for 2D scans, <code>sensor_msgs/PointCloud2</code> for 3D clouds, and depth images for camera-based sensors. These messages integrate seamlessly with SLAM algorithms that build maps and localize robots, costmap systems that mark obstacles for path planning, and TF2 transforms that position sensor data in consistent coordinate frames. Mastering these ROS2 patterns allows you to leverage existing navigation stacks, SLAM implementations, and visualization tools rather than building perception systems from scratch.</p>
<p><strong>Humanoid robots require multi-sensor fusion</strong>: A single depth sensor rarely provides sufficient coverage and reliability for humanoid robot tasks. Most production systems combine 2D LiDAR for efficient ground-level navigation, 3D depth cameras mounted in the head for manipulation and close-range 3D awareness, and sometimes 3D LiDAR for outdoor SLAM. This integration complexity demands careful consideration of sensor mounting locations (head versus torso), temporal synchronization of measurements from sensors running at different rates, and spatial fusion of overlapping coverage areas. The added system complexity pays dividends in robustnessâ€”when one sensor fails or encounters challenging conditions, complementary sensors maintain perceptual awareness.</p>
<p>Understanding depth sensing technology, point cloud processing, and ROS2 integration equips you to design perception systems that match humanoid robot capabilities to task requirements. Whether optimizing for cost, computational efficiency, or perceptual richness, these principles guide informed sensor selection and effective system integration.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="next-steps">Next Steps<a href="#next-steps" class="hash-link" aria-label="Direct link to Next Steps" title="Direct link to Next Steps" translate="no">â€‹</a></h2>
<p>Now that you understand how humanoid robots perceive distance and build 3D spatial awareness through depth sensing, the next critical component of robot perception addresses a fundamentally different question: how does the robot know its own body configuration and motion state? While depth sensors reveal the external world, humanoid robots also need continuous awareness of their own orientation, acceleration, and limb positions to move reliably and maintain balance.</p>
<p>In Lesson 3, you&#x27;ll explore <strong>IMU sensors and proprioception</strong>â€”the technologies that allow humanoid robots to sense their own orientation relative to gravity, measure linear and angular acceleration, and track joint positions throughout their kinematic chains. An Inertial Measurement Unit (IMU) combining accelerometers and gyroscopes provides the vestibular sense that complements depth sensing, enabling robots to detect when they&#x27;re tilting, falling, or experiencing external forces. Joint encoders and torque sensors provide proprioceptive feedback about limb positions and forces, essential for coordinated movement and compliant interaction.</p>
<p>Combined with depth sensing, proprioception enables humanoid robots to maintain balance during dynamic walking, perform coordinated whole-body movements like reaching while maintaining stability, and execute compliant behaviors that safely absorb impacts or adapt to external forces. Understanding both external perception (depth sensing) and internal sensing (proprioception) prepares you to design integrated perception systems for physically capable humanoid robots.</p>
<p>Continue to <a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/03-imu-proprioception">Lesson 3: IMU and Proprioception</a></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-tags-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/ros-2">ros2</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/sensors">sensors</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/depth-sensing">depth-sensing</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/lidar">lidar</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/point-cloud">point-cloud</a></li></ul></div></div><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/02-depth-sensing.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems.summary"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Summary: Lesson 1 - Camera Systems and Computer Vision</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing.summary"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Summary: Lesson 2 - Depth Sensing Technologies</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-depth-sensing-in-robotics" class="table-of-contents__link toc-highlight">What Is Depth Sensing in Robotics?</a></li><li><a href="#why-depth-sensing-matters-for-physical-ai" class="table-of-contents__link toc-highlight">Why Depth Sensing Matters for Physical AI</a></li><li><a href="#key-principles" class="table-of-contents__link toc-highlight">Key Principles</a><ul><li><a href="#31-depth-sensing-technologies-and-trade-offs" class="table-of-contents__link toc-highlight">3.1 Depth Sensing Technologies and Trade-offs</a></li><li><a href="#32-point-cloud-data-representation" class="table-of-contents__link toc-highlight">3.2 Point Cloud Data Representation</a></li><li><a href="#33-lidar-principles-2d-vs-3d-scanning" class="table-of-contents__link toc-highlight">3.3 LiDAR Principles: 2D vs. 3D Scanning</a></li><li><a href="#34-ros2-messages-for-depth-sensing" class="table-of-contents__link toc-highlight">3.4 ROS2 Messages for Depth Sensing</a></li><li><a href="#35-depth-sensor-integration-with-navigation-and-slam" class="table-of-contents__link toc-highlight">3.5 Depth Sensor Integration with Navigation and SLAM</a></li></ul></li><li><a href="#real-world-case-studies-depth-sensing-in-production-robots" class="table-of-contents__link toc-highlight">Real-World Case Studies: Depth Sensing in Production Robots</a><ul><li><a href="#-case-study-boston-dynamics-spot---3d-lidar-for-outdoor-slam-and-inspection" class="table-of-contents__link toc-highlight">ðŸ“Š Case Study: Boston Dynamics Spot - 3D LiDAR for Outdoor SLAM and Inspection</a></li><li><a href="#-case-study-agility-robotics-digit---depth-sensing-for-bipedal-stair-and-terrain-detection" class="table-of-contents__link toc-highlight">ðŸ“Š Case Study: Agility Robotics Digit - Depth Sensing for Bipedal Stair and Terrain Detection</a></li><li><a href="#-case-study-pr-2-robot-willow-garage---kinect-rgb-d-for-manipulation-in-cluttered-household-scenes" class="table-of-contents__link toc-highlight">ðŸ“Š Case Study: PR-2 Robot (Willow Garage) - Kinect RGB-D for Manipulation in Cluttered Household Scenes</a></li></ul></li><li><a href="#practical-example-subscribing-to-laserscan-data" class="table-of-contents__link toc-highlight">Practical Example: Subscribing to LaserScan Data</a></li><li><a href="#practical-example-subscribing-to-pointcloud2-data" class="table-of-contents__link toc-highlight">Practical Example: Subscribing to PointCloud2 Data</a></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#next-steps" class="table-of-contents__link toc-highlight">Next Steps</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Learn</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/">Textbook</a></li><li class="footer__item"><a class="footer__link-item" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/ros2-nervous-system/">Module 1: ROS2</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/anusbutt/hackathon-phase-01" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://linkedin.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">X (Twitter)<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://instagram.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://panaversity.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Panaversity<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2026 Physical AI & Humanoid Robotics Textbook. Built for Panaversity Hackathon.</div></div></div></footer><button class="floatingChatButton__Uyc" aria-label="Open AI Assistant" title="Ask questions about Physical AI &amp; Humanoid Robotics"><span class="chatIcon_GGtK">ðŸ’¬</span><span class="chatLabel_InO3">AI Assistant</span></button></div>
</body>
</html>