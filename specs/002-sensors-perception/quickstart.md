# Quickstart Guide: Module 2 - Sensors and Perception

**Feature**: 002-sensors-perception
**Purpose**: Help developers and content creators understand, extend, and maintain Module 2
**Last Updated**: 2025-12-07

## Overview

This guide provides practical instructions for working with Module 2 (Sensors and Perception for Humanoid Robots). Whether you're adding new lessons, fixing content, or understanding the module structure, this is your starting point.

---

## Prerequisites

Before working on Module 2, ensure you have:

### Knowledge Requirements

- âœ… **Module 1 Complete**: Understanding of ROS2 nodes, topics, publishers, subscribers, and message types
- âœ… **Python 3.11+**: Object-oriented programming, type hints, imports, basic numpy for array operations
- âœ… **ROS2 Humble Basics**: Familiarity with `rclpy` library, `ros2` CLI commands (`ros2 topic list`, `ros2 topic echo`)
- âœ… **Markdown/MDX**: Basic markdown syntax, YAML frontmatter
- âœ… **Git Workflow**: Feature branches, commits, pull requests

### Tools Required

- **Node.js 18+**: For Docusaurus development server and build
- **Docusaurus v3**: Installed in `book-source/` directory (`npm install`)
- **ROS2 Humble** (optional): For testing code examples and RViz visualization (can use simulation)
- **Python 3.11+**: For running/validating code examples
- **Git**: Version control for feature branches and commits

### Optional Tools

- **RViz**: ROS2 visualization tool (for testing sensor visualization exercises)
- **Gazebo**: Robot simulation (for generating synthetic sensor data)
- **VS Code**: Recommended editor with extensions (Python, Markdown All in One, YAML)

---

## Module 2 Structure

### File Organization

```
book-source/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/
â”œâ”€â”€ README.md                                (Module overview, navigation links)
â”œâ”€â”€ 01-camera-systems.md                     (Lesson 1: Cameras & Vision)
â”œâ”€â”€ 01-camera-systems.summary.md             (Lesson 1 summary extraction)
â”œâ”€â”€ 02-depth-sensing.md                      (Lesson 2: LiDAR & Depth Cameras)
â”œâ”€â”€ 02-depth-sensing.summary.md
â”œâ”€â”€ 03-imu-proprioception.md                 (Lesson 3: IMU & Balance)
â”œâ”€â”€ 03-imu-proprioception.summary.md
â”œâ”€â”€ 04-sensor-fusion.md                      (Lesson 4: Multi-Sensor Integration)
â”œâ”€â”€ 04-sensor-fusion.summary.md
â”œâ”€â”€ 05-capstone-project.md                   (Multi-Sensor System Design)
â”œâ”€â”€ 05-capstone-project.summary.md
â””â”€â”€ 06-quiz.md                               (Assessment - 15-20 questions)

book-source/static/img/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/
â”œâ”€â”€ camera-types-comparison.svg              (Monocular vs Stereo vs RGB-D)
â”œâ”€â”€ lidar-scanning-pattern.svg               (2D/3D LiDAR visualization)
â”œâ”€â”€ imu-sensor-axes.svg                      (3-axis accelerometer/gyro)
â””â”€â”€ sensor-fusion-architecture.svg           (Multi-sensor integration diagram)

specs/002-sensors-perception/
â”œâ”€â”€ spec.md                                  (Feature specification - user stories, requirements)
â”œâ”€â”€ plan.md                                  (Implementation plan - architecture, decisions)
â”œâ”€â”€ research.md                              (Phase 0 research findings)
â”œâ”€â”€ data-model.md                            (Entity definitions, schemas)
â”œâ”€â”€ quickstart.md                            (This file)
â”œâ”€â”€ contracts/
â”‚   â”œâ”€â”€ lesson-structure.md                  (7-section content template)
â”‚   â”œâ”€â”€ frontmatter-schema.yaml              (13-field metadata schema)
â”‚   â””â”€â”€ agent-pipeline.md                    (9-agent pipeline contracts)
â””â”€â”€ tasks.md                                 (Generated by /sp.tasks command - task breakdown)
```

### Content Components

**4 Main Lessons**:
1. **Camera Systems**: Monocular, stereo, RGB-D cameras + sensor_msgs/Image
2. **Depth Sensing**: LiDAR, structured light, ToF + sensor_msgs/PointCloud2, LaserScan
3. **IMU & Proprioception**: Accelerometer, gyroscope, magnetometer + sensor_msgs/Imu
4. **Sensor Fusion**: Kalman filtering, complementary filtering, VIO, multi-sensor strategies

**Capstone Project**: Multi-sensor perception system design (integrates all 4 lessons)

**Quiz**: 15-20 questions covering conceptual, code-reading, and scenario-based questions

**Each Lesson Includes**:
- 13-field frontmatter metadata (YAML)
- 7-section content structure (What Is, Why Matters, Key Principles, Example, Summary, Next Steps)
- 1-2 AI colearning prompts (ðŸ’¬)
- 1-2 expert insights (ðŸŽ“)
- 1-2 practice exercises (ðŸ¤)
- 1-2 code examples (Python + ROS2 rclpy, 10-30 lines)
- Paired `.summary.md` file

---

## Adding a New Lesson

Follow the **9-step agent pipeline** (see `contracts/agent-pipeline.md` for full details):

### Quick Workflow

1. **Create Outline** (Agent 1)
   - Define topic, learning goals, prerequisites
   - Generate 7-section outline with word count estimates
   - Output: Structured outline markdown

2. **Write Content** (Agent 2)
   - Use outline as template
   - Write full lesson content (2000-3000 words)
   - Embed callouts (AI prompts, expert insights, practice exercises)
   - Output: Full lesson markdown

3. **Add Case Study** (Agent 3)
   - Select humanoid robot example (Atlas, Optimus, Digit)
   - Describe real-world sensor application
   - Include sensors used, fusion strategy, outcome
   - Output: 200-400 word case study section

4. **Create Code Example** (Agent 4)
   - Write Python ROS2 node (10-30 lines)
   - Use correct message types (sensor_msgs/Image, PointCloud2, Imu)
   - Add type hints, docstrings, inline comments
   - Write 200-300 word explanation
   - Output: Code block + explanation

5. **Technical Review** (Agent 5)
   - Validate ROS2 message field names
   - Check sensor physics accuracy
   - Verify rclpy API usage
   - Validate humanoid robot examples
   - Output: Validation report with corrections

6. **Structure & Style** (Agent 6)
   - Enforce 7-section structure
   - Validate H2/H3 heading hierarchy
   - Check callout format (emoji + blockquote)
   - Ensure Summary is bulleted, Next Steps has link
   - Output: Reformatted markdown

7. **Generate Frontmatter** (Agent 7)
   - Extract learning objectives from content
   - Create skills metadata (3 skills, 6 fields each)
   - Generate cognitive load assessment
   - Add differentiation (extension + remedial)
   - Output: 13-field YAML frontmatter

8. **Docusaurus Integration** (Agent 8)
   - Combine frontmatter + content
   - Write lesson file to `book-source/docs/.../NN-lesson-name.md`
   - Create paired summary file (NN-lesson-name.summary.md)
   - Validate image paths
   - Run Docusaurus build test
   - Output: Final lesson file + summary file

9. **User Review** (Agent 9 = Human)
   - Preview Docusaurus page
   - Validate against spec learning objectives
   - Check technical accuracy
   - Approve or request revisions
   - Output: Approval â†’ commit to feature branch

### Commit After Each Step

```bash
# Example commit messages (docs(module-02): verb + what)
git add specs/002-sensors-perception/
git commit -m "docs(module-02): add outline for depth sensing lesson"

git add book-source/docs/.../02-depth-sensing.md
git commit -m "docs(module-02): add depth sensing lesson content"

git add book-source/docs/.../02-depth-sensing.md
git commit -m "docs(module-02): add technical review corrections to depth sensing"

git add book-source/docs/.../02-depth-sensing.md
git commit -m "docs(module-02): finalize depth sensing lesson with frontmatter"
```

---

## Testing Content

### 1. Validate Frontmatter Schema

```bash
# Run validation script (checks all 13 fields, enum values, tag vocabulary)
.specify/scripts/bash/validate-markdown.sh book-source/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/01-camera-systems.md
```

**Expected Output**:
```
âœ“ YAML syntax valid
âœ“ All 13 required fields present
âœ“ sidebar_position unique within module
âœ“ Tags match controlled vocabulary
âœ“ Bloom levels valid (understand, apply, analyze)
âœ“ Cognitive load new_concepts in range 3-8
```

### 2. Run Docusaurus Build

```bash
cd book-source
npm run build
```

**Expected Output**:
```
[SUCCESS] Generated static files in "build"
Build time: 45.2s
```

**Common Errors**:
- **Broken links**: Change `./lesson.md` to `./lesson` (Docusaurus ID format)
- **Image not found**: Check path is `/img/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/diagram.svg`
- **Invalid frontmatter**: YAML syntax error (check indentation, quotes)

### 3. Preview Locally

```bash
cd book-source
npm start
```

Visit `http://localhost:3000/ros2-physical-ai-book/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems`

**Check**:
- [ ] Lesson renders correctly
- [ ] Navigation sidebar shows all lessons
- [ ] Images display (not broken)
- [ ] Internal links work (Next Steps, references)
- [ ] Code blocks have syntax highlighting
- [ ] Callouts render with emoji and blockquotes

### 4. Test RViz Visualization (Optional)

If you have ROS2 installed:

```bash
# Start RViz
ros2 run rviz2 rviz2

# In RViz:
# 1. Set Fixed Frame to "base_link" or "world"
# 2. Add Image display type â†’ select topic /camera/image_raw
# 3. Add PointCloud2 display â†’ select topic /lidar/points
# 4. Add TF display â†’ visualize IMU coordinate frame

# Verify visualization matches lesson descriptions
```

---

## Common Tasks

### Update ROS2 Message Examples

**Scenario**: ROS2 Humble changes sensor_msgs/Imu message structure

1. Check official ROS2 message definition:
   ```bash
   ros2 interface show sensor_msgs/msg/Imu
   ```

2. Update code examples in affected lessons (Lesson 3, 4)

3. Update data-model.md catalog entry:
   ```markdown
   **sensor_msgs/Imu**:
   - Fields: ["header", "orientation", "angular_velocity", ...] # Update this list
   ```

4. Run Technical Reviewer Agent (step 5) to validate changes

5. Rebuild Docusaurus and test

### Add New Sensor Type

**Scenario**: Add lesson on ultrasonic sensors

1. Update spec.md with new user story:
   ```markdown
   ### User Story 5 - Understanding Ultrasonic Sensors (Priority: P5)
   A student wants to understand how ultrasonic sensors provide proximity detection...
   ```

2. Run `/sp.plan` to update plan.md with new lesson outline

3. Follow 9-step pipeline to create `05-ultrasonic-sensors.md`

4. Update capstone project to include ultrasonic sensor option

5. Add quiz questions covering ultrasonic concepts

6. Update module README with link to new lesson

### Extend Capstone Project

**Scenario**: Add more challenging capstone scenarios

1. Edit `05-capstone-project.md`

2. Add new scenario to scenarios list:
   ```markdown
   ### Scenario 3: Outdoor Navigation on Uneven Terrain
   Design a sensor system for a humanoid robot hiking on rocky trails...
   ```

3. Update success criteria:
   ```markdown
   - [ ] Sensor selection justified for outdoor, uneven terrain
   - [ ] Fusion strategy handles GPS+IMU+stereo camera
   ```

4. Run Structure & Style Agent (step 6) to validate format

5. Rebuild and test

---

## Troubleshooting

### Broken Links in Docusaurus Build

**Error**:
```
Docusaurus found broken links!
- Broken link on source page path = /docs/.../ quiz:
   -> linking to ./01-camera-systems.md
```

**Fix**: Remove `.md` extension (use Docusaurus ID format)
```markdown
# Before (broken):
[Lesson 1](./01-camera-systems.md)

# After (fixed):
[Lesson 1](./01-camera-systems)
```

### Frontmatter Validation Error

**Error**:
```
Invalid bloom_level: 'comprehension' not in [remember, understand, apply, analyze, evaluate, create]
```

**Fix**: Use correct Bloom taxonomy level
```yaml
# Before (wrong):
bloom_level: "comprehension"

# After (correct):
bloom_level: "understand"
```

### Image Not Displaying

**Error**: Broken image icon in Docusaurus page

**Fix**: Check image path format
```markdown
# Correct format (absolute path from static/):
![Camera Types](/img/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/camera-types-comparison.svg)

# Wrong (relative path won't work):
![Camera Types](./camera-types-comparison.svg)
```

### Code Example Not Highlighting

**Error**: Code appears as plain text without syntax colors

**Fix**: Specify language in fence
```markdown
# Before (no highlighting):
â€‹```
import rclpy
â€‹```

# After (Python highlighting):
â€‹```python
import rclpy
â€‹```
```

### Quiz Questions Not Referencing Lessons

**Error**: Quiz question doesn't indicate which lesson it covers

**Fix**: Add lesson reference in quiz.md
```markdown
## Questions 1-4: Camera Systems

**Lesson Reference**: [Lesson 1: Camera Systems](./camera-systems)

1. What is the primary advantage of stereo cameras over monocular cameras?
   - A) Higher resolution
   - B) Depth perception through triangulation âœ“
   ...
```

---

## Best Practices

### Content Writing

1. **Use Active Voice**: "ROS2 publishes sensor data" not "Sensor data is published by ROS2"
2. **Define Before Using**: Introduce acronyms (IMU, LiDAR, FOV) before using them
3. **Concrete Examples**: Reference real robots (Atlas, Optimus, Digit) not hypothetical systems
4. **Code Comments**: Explain non-obvious ROS2 patterns (QoS, callback signatures)
5. **Progressive Complexity**: Lesson 1 (beginner) â†’ Lesson 4 (intermediate)

### Technical Accuracy

1. **Verify ROS2 Messages**: Cross-check field names with `ros2 interface show sensor_msgs/msg/Image`
2. **Test Code Snippets**: Ensure code is syntactically valid Python (even if conceptual)
3. **Cite Sources**: Reference official ROS2 docs, robot technical papers, IEEE publications
4. **Avoid Speculation**: Don't claim capabilities without documentation (e.g., "Atlas probably uses...")

### Consistency

1. **Use Module 1 Patterns**: Follow established callout format, heading styles, frontmatter structure
2. **Controlled Vocabulary**: Tags from approved list (ros2, sensors, camera, lidar, imu, fusion)
3. **Naming Conventions**: File names lowercase with hyphens (01-camera-systems.md not 01_Camera_Systems.md)
4. **Link Format**: Docusaurus IDs (./lesson-name) not file paths (./lesson-name.md)

---

## Resources

### ROS2 Documentation

- **sensor_msgs Package**: https://docs.ros.org/en/humble/p/sensor_msgs/
- **rclpy API**: https://docs.ros.org/en/humble/p/rclpy/
- **RViz User Guide**: https://github.com/ros2/rviz/blob/humble/docs/user_guide.md

### Humanoid Robotics References

- **Boston Dynamics Atlas**: Technical publications, YouTube videos
- **Agility Robotics Digit**: https://agilityrobotics.com/digit
- **Tesla Optimus**: Tesla AI Day presentations (2021, 2022)

### Sensor Fusion Literature

- **Visual-Inertial Odometry**: VINS-Mono, ORB-SLAM3 papers
- **Kalman Filtering**: "Probabilistic Robotics" (Thrun, Burgard, Fox) - Chapter 3
- **robot_localization**: https://github.com/cra-ros-pkg/robot_localization

### Educational Design

- **Bloom's Taxonomy**: https://cft.vanderbilt.edu/guides-sub-pages/blooms-taxonomy/
- **DigComp 2.2**: European Digital Competence Framework
- **Cognitive Load Theory**: Sweller et al. (for new_concepts calibration)

---

## Getting Help

### For Content Questions

1. **Check spec.md**: Does your change align with user stories and requirements?
2. **Review Module 1**: Has this pattern been used before? (consistency)
3. **Ask Technical Reviewer Agent**: Validate sensor physics, ROS2 message accuracy

### For Build Issues

1. **Check Docusaurus Logs**: `npm run build` shows specific errors (broken links, invalid MDX)
2. **Validate YAML**: Use online YAML validator or `yq` command
3. **Test Incrementally**: Add content section-by-section, rebuild after each

### For Design Decisions

1. **Consult Constitution**: `.specify/memory/constitution.md` for principles
2. **Review plan.md**: Architecture decisions and rationale
3. **Check contracts/**: Lesson structure, frontmatter schema, agent pipeline specs

---

## Summary

This quickstart guide enables you to:

- âœ… **Understand Module 2 structure**: 4 lessons + capstone + quiz covering sensor perception
- âœ… **Add new content**: Follow 9-step agent pipeline for consistent quality
- âœ… **Test changes**: Validate frontmatter, build Docusaurus, preview locally
- âœ… **Troubleshoot issues**: Fix broken links, frontmatter errors, image paths
- âœ… **Maintain quality**: Use best practices for content, technical accuracy, consistency

**Next Steps**:
- For new features: Run `/sp.specify` â†’ `/sp.plan` â†’ `/sp.tasks` â†’ `/sp.implement`
- For content fixes: Edit lesson file â†’ validate â†’ rebuild â†’ commit
- For major changes: Review spec.md and plan.md first to ensure alignment

---

**Last Updated**: 2025-12-07 | **Feature**: 002-sensors-perception | **Version**: 1.0
