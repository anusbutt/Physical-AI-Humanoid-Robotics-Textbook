"use strict";(globalThis.webpackChunkbook_source=globalThis.webpackChunkbook_source||[]).push([[673],{7558:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>r});const o=JSON.parse('{"id":"Physical-AI-Humanoid-Robotics/vision-language-action/README","title":"Module 4: Vision-Language-Action (VLA)","description":"Overview","source":"@site/docs/13-Physical-AI-Humanoid-Robotics/04-vision-language-action/README.md","sourceDirName":"13-Physical-AI-Humanoid-Robotics/04-vision-language-action","slug":"/Physical-AI-Humanoid-Robotics/vision-language-action/","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/04-vision-language-action/README.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 3 Quiz: The AI-Robot Brain (NVIDIA Isaac\u2122)","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/isaac-ai-brain/quiz"},"next":{"title":"Lesson 1 - Voice-to-Action Systems","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/voice-to-action"}}');var s=i(4848),a=i(8453);const t={},c="Module 4: Vision-Language-Action (VLA)",l={},r=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Lessons",id:"lessons",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Technology Stack",id:"technology-stack",level:2},{value:"Estimated Time",id:"estimated-time",level:2},{value:"Navigation",id:"navigation",level:2}];function d(n){const e={a:"a",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,s.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(e.p,{children:"Welcome to Module 4 of the Physical AI & Humanoid Robotics Learning Platform! This module explores the convergence of Large Language Models (LLMs) and robotics through Vision-Language-Action (VLA) systems. You'll learn how to build systems that process voice commands, decompose natural language into robotic actions, integrate vision-language processing, and execute complete robotic tasks."}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(e.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Understand voice-to-action systems using OpenAI Whisper for speech recognition"}),"\n",(0,s.jsx)(e.li,{children:"Apply cognitive planning with LLMs to translate natural language commands into action sequences"}),"\n",(0,s.jsx)(e.li,{children:"Integrate vision-language systems for multimodal perception and object grounding"}),"\n",(0,s.jsx)(e.li,{children:"Execute complete VLA pipelines from voice command to robot action"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(e.p,{children:"This module assumes you have completed:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Module 1: The Robotic Nervous System (ROS 2)"}),"\n",(0,s.jsx)(e.li,{children:"Module 2: Sensors and Perception for Humanoid Robots"}),"\n",(0,s.jsx)(e.li,{children:"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"lessons",children:"Lessons"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/voice-to-action",children:"Voice-to-Action Systems"})," - Introduction to speech recognition and Whisper API integration"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/cognitive-planning",children:"Cognitive Planning with LLMs"})," - Using LLMs for natural language understanding and task decomposition"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/vision-language-integration",children:"Vision-Language Integration"})," - Multimodal perception and language-grounded vision"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/action-execution-control",children:"Action Execution and Control"})," - Complete VLA pipeline with ROS2 action servers"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/capstone-project",children:"Capstone Project: The Autonomous Humanoid"})," - Complete VLA task implementation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/quiz",children:"Module Quiz"})," - Assessment of VLA concepts and integration"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Voice-to-Action"}),": Processing natural language commands through speech recognition"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cognitive Planning"}),": Decomposing high-level goals into executable action sequences"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision-Language Integration"}),": Multimodal perception combining visual and linguistic information"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action Execution"}),": Safe and reliable execution of planned actions on robotic platforms"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"technology-stack",children:"Technology Stack"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"OpenAI Whisper API for speech recognition"}),"\n",(0,s.jsx)(e.li,{children:"Large Language Models (GPT, Claude) for cognitive planning"}),"\n",(0,s.jsx)(e.li,{children:"Vision-Language models (CLIP, BLIP) for multimodal processing"}),"\n",(0,s.jsx)(e.li,{children:"ROS2 Humble for action execution"}),"\n",(0,s.jsx)(e.li,{children:"Isaac Sim for simulation environments"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"estimated-time",children:"Estimated Time"}),"\n",(0,s.jsx)(e.p,{children:"Completing all lessons, capstone project, and quiz should take approximately 6-8 hours depending on your prior experience with LLMs and multimodal systems."}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"navigation",children:"Navigation"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/isaac-ai-brain/",children:"\u2190 Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/voice-to-action",children:"Lesson 1: Voice-to-Action Systems \u2192"})}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>c});var o=i(6540);const s={},a=o.createContext(s);function t(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:t(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);