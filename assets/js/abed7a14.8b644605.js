"use strict";(globalThis.webpackChunkbook_source=globalThis.webpackChunkbook_source||[]).push([[9310],{3799:o=>{o.exports=JSON.parse('{"tag":{"label":"perception","permalink":"/hackathon-phase-01/docs/tags/perception","allTagsPath":"/hackathon-phase-01/docs/tags","count":6,"items":[{"id":"Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project","title":"Capstone Project: Design a Multi-Sensor Humanoid Perception System","description":"Integrative project combining camera systems, depth sensing, IMU integration, and sensor fusion to design a complete perception system for a humanoid warehouse robot.","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project"},{"id":"Physical-AI-Humanoid-Robotics/isaac-ai-brain/isaac-ros-perception","title":"Isaac ROS Hardware-Accelerated Perception","description":"Isaac ROS brings NVIDIA\'s GPU acceleration to the Robot Operating System (ROS2), enabling real-time perception capabilities that are essential for humanoid robot autonomy. Through specialized GPU-accelerated libraries called GEMs (GPU-accelerated modules), Isaac ROS dramatically improves the performance of computationally intensive perception tasks like Visual SLAM, object detection, and pose estimation. For humanoid robots that require rapid processing of sensor data to maintain balance and navigate safely, Isaac ROS provides the performance necessary for real-time operation.","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/isaac-ai-brain/isaac-ros-perception"},{"id":"Physical-AI-Humanoid-Robotics/vision-language-action/vision-language-integration","title":"Lesson 3 - Vision-Language Integration","description":"What Is Vision-Language Integration?","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/vision-language-integration"},{"id":"Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception","title":"Lesson 3: IMU and Proprioception for Humanoid Robots","description":"Learn how inertial measurement units and proprioceptive sensors enable humanoid robots to sense their own motion, orientation, and body position for dynamic balance control and spatial self-awareness.","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception"},{"id":"Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion","title":"Lesson 4: Sensor Fusion Techniques for Humanoid Robots","description":"Learn how sensor fusion combines camera, depth, and IMU data to create robust perception systems that overcome individual sensor limitations through complementary filtering, Kalman filtering, and Visual-Inertial Odometry.","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion"},{"id":"Physical-AI-Humanoid-Robotics/vision-language-action/vision-language-integration.summary","title":"Summary - Lesson 3 - Vision-Language Integration","description":"Module: Module 4 - Vision-Language-Action (VLA)","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/vision-language-integration.summary"}],"unlisted":false}}')}}]);