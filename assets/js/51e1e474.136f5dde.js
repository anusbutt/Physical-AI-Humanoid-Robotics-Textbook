"use strict";(globalThis.webpackChunkbook_source=globalThis.webpackChunkbook_source||[]).push([[4185],{5189:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.architecture","title":"Multi-Sensor Fusion Capstone: Architecture Documentation","description":"System Architecture Overview","source":"@site/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/05-capstone-project.architecture.md","sourceDirName":"13-Physical-AI-Humanoid-Robotics/02-sensors-perception","slug":"/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.architecture","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/05-capstone-project.architecture.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4 Summary: Sensor Fusion Techniques for Humanoid Robots","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion.summary"},"next":{"title":"Case Studies: Real-World Multi-Sensor Integration","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.casestudies"}}');var r=s(4848),t=s(8453);const l={},o="Multi-Sensor Fusion Capstone: Architecture Documentation",c={},a=[{value:"System Architecture Overview",id:"system-architecture-overview",level:2},{value:"Data Flow Pipeline",id:"data-flow-pipeline",level:2},{value:"1. Sensor Acquisition (Parallel Streams)",id:"1-sensor-acquisition-parallel-streams",level:3},{value:"2. Time Synchronization",id:"2-time-synchronization",level:3},{value:"3. Sensor Processing (Modular Design)",id:"3-sensor-processing-modular-design",level:3},{value:"4. Fusion Algorithm (Customizable Core)",id:"4-fusion-algorithm-customizable-core",level:3},{value:"5. Output Publication",id:"5-output-publication",level:3},{value:"ROS2 Concepts Demonstrated",id:"ros2-concepts-demonstrated",level:2},{value:"Core ROS2 Features",id:"core-ros2-features",level:3},{value:"Advanced Patterns",id:"advanced-patterns",level:3},{value:"Connections to Course Lessons",id:"connections-to-course-lessons",level:2},{value:"Lesson 1: Camera Systems",id:"lesson-1-camera-systems",level:3},{value:"Lesson 2: Depth Sensing",id:"lesson-2-depth-sensing",level:3},{value:"Lesson 3: IMU &amp; Proprioception",id:"lesson-3-imu--proprioception",level:3},{value:"Lesson 4: Sensor Fusion",id:"lesson-4-sensor-fusion",level:3},{value:"Student Adaptation Guide",id:"student-adaptation-guide",level:2},{value:"Scenario 1: Humanoid Balance Control",id:"scenario-1-humanoid-balance-control",level:3},{value:"Scenario 2: Obstacle Avoidance Navigation",id:"scenario-2-obstacle-avoidance-navigation",level:3},{value:"Scenario 3: Visual-Inertial Odometry (VIO)",id:"scenario-3-visual-inertial-odometry-vio",level:3},{value:"Key Architectural Decisions",id:"key-architectural-decisions",level:2},{value:"Decision 1: Message Filters for Synchronization",id:"decision-1-message-filters-for-synchronization",level:3},{value:"Decision 2: Dictionary-Based Interfaces",id:"decision-2-dictionary-based-interfaces",level:3},{value:"Decision 3: Separate Processing Methods",id:"decision-3-separate-processing-methods",level:3},{value:"Decision 4: Configuration via YAML",id:"decision-4-configuration-via-yaml",level:3},{value:"Testing &amp; Validation Checklist",id:"testing--validation-checklist",level:2},{value:"Integration Tests",id:"integration-tests",level:3},{value:"Functional Tests",id:"functional-tests",level:3},{value:"Performance Tests",id:"performance-tests",level:3},{value:"Educational Tests",id:"educational-tests",level:3},{value:"Future Extensions",id:"future-extensions",level:2},{value:"Resources for Students",id:"resources-for-students",level:2},{value:"ROS2 Documentation",id:"ros2-documentation",level:3},{value:"Sensor Fusion Algorithms",id:"sensor-fusion-algorithms",level:3},{value:"Example Capstone Projects",id:"example-capstone-projects",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"multi-sensor-fusion-capstone-architecture-documentation",children:"Multi-Sensor Fusion Capstone: Architecture Documentation"})}),"\n",(0,r.jsx)(n.h2,{id:"system-architecture-overview",children:"System Architecture Overview"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                    MULTI-SENSOR CAPSTONE ARCHITECTURE                    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n\r\n                         SENSOR HARDWARE LAYER\r\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n    \u2502  Camera Driver  \u2502  \u2502 Depth Sensor    \u2502  \u2502   IMU Driver    \u2502\r\n    \u2502  (USB/CSI)      \u2502  \u2502 (RealSense/     \u2502  \u2502  (BNO055/MPU)   \u2502\r\n    \u2502                 \u2502  \u2502  Kinect/LiDAR)  \u2502  \u2502                 \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n             \u2502                    \u2502                    \u2502\r\n             \u2502 /camera/image_raw  \u2502 /camera/depth/     \u2502 /imu/data\r\n             \u2502 (sensor_msgs/      \u2502  points            \u2502 (sensor_msgs/\r\n             \u2502  Image)            \u2502 (sensor_msgs/      \u2502  Imu)\r\n             \u2502                    \u2502  PointCloud2)      \u2502\r\n             \u25bc                    \u25bc                    \u25bc\r\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n    \u2502         MESSAGE_FILTERS: TIME SYNCHRONIZATION              \u2502\r\n    \u2502  ApproximateTimeSynchronizer(queue=10, slop=0.1s)          \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                             \u2502\r\n                             \u2502 Synchronized Callback\r\n                             \u2502\r\n                             \u25bc\r\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n    \u2502        MULTI-SENSOR CAPSTONE NODE (Fusion Core)            \u2502\r\n    \u2502                                                             \u2502\r\n    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\r\n    \u2502  \u2502 Camera        \u2502  \u2502 Depth         \u2502  \u2502 IMU           \u2502  \u2502\r\n    \u2502  \u2502 Processor     \u2502  \u2502 Processor     \u2502  \u2502 Processor     \u2502  \u2502\r\n    \u2502  \u2502 (Lesson 1)    \u2502  \u2502 (Lesson 2)    \u2502  \u2502 (Lesson 3)    \u2502  \u2502\r\n    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\r\n    \u2502          \u2502                  \u2502                  \u2502           \u2502\r\n    \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\r\n    \u2502                     \u2502                                      \u2502\r\n    \u2502           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502\r\n    \u2502           \u2502  Fusion Algorithm  \u2502                           \u2502\r\n    \u2502           \u2502    (Lesson 4)      \u2502                           \u2502\r\n    \u2502           \u2502 \u2022 Complementary    \u2502                           \u2502\r\n    \u2502           \u2502 \u2022 Kalman Filter    \u2502                           \u2502\r\n    \u2502           \u2502 \u2022 Weighted Average \u2502                           \u2502\r\n    \u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                          \u2502\r\n                          \u2502\r\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n            \u2502                            \u2502\r\n            \u25bc                            \u25bc\r\n    /fused_pose                   /fused_odom\r\n    (geometry_msgs/               (nav_msgs/\r\n     PoseStamped)                  Odometry)\r\n            \u2502                            \u2502\r\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                          \u2502\r\n                          \u25bc\r\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n    \u2502              DOWNSTREAM APPLICATIONS                        \u2502\r\n    \u2502  \u2022 Robot Control (balance, navigation)                     \u2502\r\n    \u2502  \u2022 Safety Monitoring (fall detection, collision avoidance) \u2502\r\n    \u2502  \u2022 Visualization (RViz)                                    \u2502\r\n    \u2502  \u2022 Data Logging & Analysis                                 \u2502\r\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"data-flow-pipeline",children:"Data Flow Pipeline"}),"\n",(0,r.jsx)(n.h3,{id:"1-sensor-acquisition-parallel-streams",children:"1. Sensor Acquisition (Parallel Streams)"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Sensor"}),(0,r.jsx)(n.th,{children:"Message Type"}),(0,r.jsx)(n.th,{children:"Frequency"}),(0,r.jsx)(n.th,{children:"QoS Profile"}),(0,r.jsx)(n.th,{children:"Purpose"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Camera"}),(0,r.jsx)(n.td,{children:"sensor_msgs/Image"}),(0,r.jsx)(n.td,{children:"30 Hz"}),(0,r.jsx)(n.td,{children:"BEST_EFFORT"}),(0,r.jsx)(n.td,{children:"Visual features, object detection"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Depth"}),(0,r.jsx)(n.td,{children:"sensor_msgs/PointCloud2"}),(0,r.jsx)(n.td,{children:"30 Hz"}),(0,r.jsx)(n.td,{children:"BEST_EFFORT"}),(0,r.jsx)(n.td,{children:"Spatial awareness, obstacle distance"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"IMU"}),(0,r.jsx)(n.td,{children:"sensor_msgs/Imu"}),(0,r.jsx)(n.td,{children:"100 Hz"}),(0,r.jsx)(n.td,{children:"RELIABLE"}),(0,r.jsx)(n.td,{children:"Orientation, acceleration, angular velocity"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"2-time-synchronization",children:"2. Time Synchronization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"message_filters.ApproximateTimeSynchronizer\r\n\u251c\u2500\u2500 Queue Size: 10 messages per sensor\r\n\u251c\u2500\u2500 Slop Tolerance: 0.1 seconds (configurable)\r\n\u2514\u2500\u2500 Callback: Triggered when all 3 sensors aligned within tolerance\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Decision"}),": ApproximateTimeSynchronizer chosen over ExactTimeSynchronizer"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rationale"}),": Real sensors have clock drift and variable latency"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tradeoff"}),": Small temporal misalignment (\u2264100ms) vs missing data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Alternative"}),": ExactTimeSynchronizer for simulation with perfect clocks"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-sensor-processing-modular-design",children:"3. Sensor Processing (Modular Design)"}),"\n",(0,r.jsx)(n.p,{children:"Each sensor has dedicated processing method:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"_process_camera()  \u2192 visual_features (dict)\r\n_process_depth()   \u2192 spatial_info (dict)\r\n_process_imu()     \u2192 motion_state (dict)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Extension Pattern"}),": Students implement scenario-specific logic in these methods"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Camera: Object detection, visual SLAM features"}),"\n",(0,r.jsx)(n.li,{children:"Depth: Obstacle detection, ground segmentation"}),"\n",(0,r.jsx)(n.li,{children:"IMU: Balance monitoring, motion classification"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"4-fusion-algorithm-customizable-core",children:"4. Fusion Algorithm (Customizable Core)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"_fuse_sensors(visual, spatial, motion) \u2192 fused_state (dict)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Fusion Strategies"})," (students choose based on scenario):"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Strategy"}),(0,r.jsx)(n.th,{children:"Use Case"}),(0,r.jsx)(n.th,{children:"Complexity"}),(0,r.jsx)(n.th,{children:"Accuracy"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Weighted Average"}),(0,r.jsx)(n.td,{children:"Simple fusion, confidence-based"}),(0,r.jsx)(n.td,{children:"Low"}),(0,r.jsx)(n.td,{children:"Moderate"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Complementary Filter"}),(0,r.jsx)(n.td,{children:"Orientation fusion (IMU + visual)"}),(0,r.jsx)(n.td,{children:"Medium"}),(0,r.jsx)(n.td,{children:"Good"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Extended Kalman Filter"}),(0,r.jsx)(n.td,{children:"Full state estimation (pose + velocity)"}),(0,r.jsx)(n.td,{children:"High"}),(0,r.jsx)(n.td,{children:"Excellent"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Visual-Inertial Odometry"}),(0,r.jsx)(n.td,{children:"Navigation, localization"}),(0,r.jsx)(n.td,{children:"Very High"}),(0,r.jsx)(n.td,{children:"Excellent"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"5-output-publication",children:"5. Output Publication"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PoseStamped"}),": Position + orientation (for control)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Odometry"}),": Full state (pose + velocity, optional)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"ros2-concepts-demonstrated",children:"ROS2 Concepts Demonstrated"}),"\n",(0,r.jsx)(n.h3,{id:"core-ros2-features",children:"Core ROS2 Features"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Node Architecture"}),": Object-oriented node design with ",(0,r.jsx)(n.code,{children:"rclpy.node.Node"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Publishers & Subscribers"}),": Multi-topic communication"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Message Filters"}),": Time synchronization for multi-sensor fusion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"QoS Profiles"}),": Reliability vs performance tradeoffs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Parameters"}),": Runtime configurability without code changes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Launch System"}),": Orchestrating multiple nodes with Python launch files"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"YAML Configuration"}),": Centralized parameter management"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"advanced-patterns",children:"Advanced Patterns"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Separation of Concerns"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Sensor drivers (hardware interface)"}),"\n",(0,r.jsx)(n.li,{children:"Processing logic (computation)"}),"\n",(0,r.jsx)(n.li,{children:"Fusion algorithm (integration)"}),"\n",(0,r.jsx)(n.li,{children:"Output publishing (communication)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Modularity"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Each sensor processor is independent"}),"\n",(0,r.jsx)(n.li,{children:"Fusion algorithm is swappable"}),"\n",(0,r.jsx)(n.li,{children:"Configuration externalized to YAML"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Extensibility"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Clear TODO markers for student customization"}),"\n",(0,r.jsx)(n.li,{children:"Dictionary-based interfaces (easy to add fields)"}),"\n",(0,r.jsx)(n.li,{children:"Scenario-specific parameters in config file"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"connections-to-course-lessons",children:"Connections to Course Lessons"}),"\n",(0,r.jsx)(n.h3,{id:"lesson-1-camera-systems",children:"Lesson 1: Camera Systems"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Template Integration"}),": ",(0,r.jsx)(n.code,{children:"_process_camera()"})," method"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Concepts Applied"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Image message handling (sensor_msgs/Image)"}),"\n",(0,r.jsx)(n.li,{children:"Visual feature extraction placeholders"}),"\n",(0,r.jsx)(n.li,{children:"Object detection integration points"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Student Extensions"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"YOLO object detection"}),"\n",(0,r.jsx)(n.li,{children:"Visual SLAM features (ORB, SIFT)"}),"\n",(0,r.jsx)(n.li,{children:"Lane detection for navigation"}),"\n",(0,r.jsx)(n.li,{children:"Gesture recognition"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"lesson-2-depth-sensing",children:"Lesson 2: Depth Sensing"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Template Integration"}),": ",(0,r.jsx)(n.code,{children:"_process_depth()"})," method"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Concepts Applied"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"PointCloud2 message processing"}),"\n",(0,r.jsx)(n.li,{children:"Obstacle distance computation"}),"\n",(0,r.jsx)(n.li,{children:"Spatial awareness for navigation"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Student Extensions"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Ground plane segmentation"}),"\n",(0,r.jsx)(n.li,{children:"3D object localization"}),"\n",(0,r.jsx)(n.li,{children:"Occupancy grid generation"}),"\n",(0,r.jsx)(n.li,{children:"Collision avoidance logic"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"lesson-3-imu--proprioception",children:"Lesson 3: IMU & Proprioception"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Template Integration"}),": ",(0,r.jsx)(n.code,{children:"_process_imu()"})," method"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Concepts Applied"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Quaternion orientation handling"}),"\n",(0,r.jsx)(n.li,{children:"Angular velocity and linear acceleration"}),"\n",(0,r.jsx)(n.li,{children:"Motion state estimation"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Student Extensions"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Balance monitoring (humanoid robots)"}),"\n",(0,r.jsx)(n.li,{children:"Fall detection algorithms"}),"\n",(0,r.jsx)(n.li,{children:"Motion classification (walk/stand/run)"}),"\n",(0,r.jsx)(n.li,{children:"Tilt angle computation for stability"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"lesson-4-sensor-fusion",children:"Lesson 4: Sensor Fusion"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Template Integration"}),": ",(0,r.jsx)(n.code,{children:"_fuse_sensors()"})," method"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Concepts Applied"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Multi-modal data integration"}),"\n",(0,r.jsx)(n.li,{children:"Confidence-weighted fusion"}),"\n",(0,r.jsx)(n.li,{children:"Synchronized processing pipeline"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Student Extensions"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Complementary filter (IMU + visual orientation)"}),"\n",(0,r.jsx)(n.li,{children:"Extended Kalman Filter (state estimation)"}),"\n",(0,r.jsx)(n.li,{children:"Visual-Inertial Odometry (VIO)"}),"\n",(0,r.jsx)(n.li,{children:"Bayesian fusion with uncertainty"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"student-adaptation-guide",children:"Student Adaptation Guide"}),"\n",(0,r.jsx)(n.h3,{id:"scenario-1-humanoid-balance-control",children:"Scenario 1: Humanoid Balance Control"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Modifications"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU Processing"}),": Implement tilt angle computation from quaternion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Processing"}),": Detect ground plane for stance stability"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fusion"}),": Complementary filter combining IMU orientation with visual correction"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Publish joint control commands to maintain balance"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Config Changes"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"balance_control:\r\n  max_tilt_angle: 15.0\r\n  imu_orientation_weight: 0.7\r\n  enable_fall_detection: true\n"})}),"\n",(0,r.jsx)(n.h3,{id:"scenario-2-obstacle-avoidance-navigation",children:"Scenario 2: Obstacle Avoidance Navigation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Modifications"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera Processing"}),": Detect obstacles using object detection (YOLO)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Processing"}),": Compute minimum obstacle distance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fusion"}),": Choose nearest obstacle from visual + depth sources"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Publish velocity commands (Twist) for safe navigation"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Config Changes"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'obstacle_avoidance:\r\n  safe_distance: 0.5\r\n  fusion_strategy: "depth_priority"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"scenario-3-visual-inertial-odometry-vio",children:"Scenario 3: Visual-Inertial Odometry (VIO)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Modifications"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera Processing"}),": Extract visual features (ORB) and track between frames"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Processing"}),": Provide scale correction for monocular visual odometry"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU Processing"}),": Predict pose changes using angular velocity"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fusion"}),": EKF combining visual motion estimates with IMU predictions"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Config Changes"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"visual_inertial_odometry:\r\n  max_features: 200\r\n  enable_depth_scale: true\r\n  measurement_noise_visual: 0.1\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-architectural-decisions",children:"Key Architectural Decisions"}),"\n",(0,r.jsx)(n.h3,{id:"decision-1-message-filters-for-synchronization",children:"Decision 1: Message Filters for Synchronization"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Alternatives Considered"}),": Manual timestamp matching, callback-based sync"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rationale"}),": message_filters provides tested, reliable time alignment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tradeoff"}),": Adds latency (queue buffering) but guarantees alignment"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"decision-2-dictionary-based-interfaces",children:"Decision 2: Dictionary-Based Interfaces"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Alternatives Considered"}),": Typed dataclasses, ROS messages for internal data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rationale"}),": Flexibility for students to add scenario-specific fields"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tradeoff"}),": Less type safety, but easier to extend"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"decision-3-separate-processing-methods",children:"Decision 3: Separate Processing Methods"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Alternatives Considered"}),": Single unified callback with inline processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rationale"}),": Modularity allows students to focus on one sensor at a time"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tradeoff"}),": More function calls, but clearer code organization"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"decision-4-configuration-via-yaml",children:"Decision 4: Configuration via YAML"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Alternatives Considered"}),": Hard-coded constants, command-line arguments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rationale"}),": Industry best practice, supports multiple scenarios without recompilation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tradeoff"}),": Additional file to manage, but promotes good practices"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"testing--validation-checklist",children:"Testing & Validation Checklist"}),"\n",(0,r.jsx)(n.h3,{id:"integration-tests",children:"Integration Tests"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Camera topic receives messages at expected rate"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Depth topic receives messages at expected rate"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","IMU topic receives messages at expected rate"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Synchronized callback triggers only when all 3 sensors available"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Fusion output published within 100ms of sensor data arrival"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"functional-tests",children:"Functional Tests"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Camera processing extracts expected features"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Depth processing computes obstacle distances"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","IMU processing converts quaternions correctly"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Fusion algorithm produces reasonable outputs"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Output messages have correct timestamp and frame_id"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"performance-tests",children:"Performance Tests"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Fusion node maintains target rate (30 Hz)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Memory usage stable over 1-hour run"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","No message queue overflows or dropped data"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","CPU usage within acceptable limits (<50% single core)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"educational-tests",children:"Educational Tests"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Students can modify camera processing without breaking fusion"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Students can swap fusion algorithms via config changes"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Code comments clearly explain ROS2 concepts"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Launch file successfully starts all nodes on first try"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"future-extensions",children:"Future Extensions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Advanced Synchronization"}),": Adaptive slop based on sensor latency"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Rate Fusion"}),": Handle sensors with different frequencies"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fault Tolerance"}),": Graceful degradation if one sensor fails"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance Profiling"}),": Built-in timing diagnostics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Replay Capability"}),": Record/playback sensor data for debugging"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Web Dashboard"}),": Real-time visualization of fusion metrics"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"resources-for-students",children:"Resources for Students"}),"\n",(0,r.jsx)(n.h3,{id:"ros2-documentation",children:"ROS2 Documentation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Tutorials/Intermediate/Tf2/Writing-A-Tf2-Broadcaster-Py.html",children:"message_filters Tutorial"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Tutorials/Intermediate/Launch/Launch-Main.html",children:"Launch Files Guide"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Quality-of-Service-Settings.html",children:"QoS Profiles"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"sensor-fusion-algorithms",children:"Sensor Fusion Algorithms"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Complementary Filter: ",(0,r.jsx)(n.a,{href:"https://www.pieter-jan.com/node/11",children:"Blog Post"})]}),"\n",(0,r.jsxs)(n.li,{children:["Extended Kalman Filter: ",(0,r.jsx)(n.a,{href:"https://github.com/cra-ros-pkg/robot_localization",children:"ROS2 robot_localization"})]}),"\n",(0,r.jsxs)(n.li,{children:["Visual-Inertial Odometry: ",(0,r.jsx)(n.a,{href:"https://github.com/HKUST-Aerial-Robotics/VINS-Fusion",children:"VINS-Fusion"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-capstone-projects",children:"Example Capstone Projects"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Two-wheeled balancing robot (Segway-style)"}),"\n",(0,r.jsx)(n.li,{children:"Autonomous wheelchair navigation"}),"\n",(0,r.jsx)(n.li,{children:"Drone visual-inertial navigation"}),"\n",(0,r.jsx)(n.li,{children:"Humanoid walking stabilization"}),"\n",(0,r.jsx)(n.li,{children:"Mobile manipulator obstacle avoidance"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>o});var i=s(6540);const r={},t=i.createContext(r);function l(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);