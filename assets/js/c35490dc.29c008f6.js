"use strict";(globalThis.webpackChunkbook_source=globalThis.webpackChunkbook_source||[]).push([[5334],{8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>r});var o=i(6540);const s={},t=o.createContext(s);function a(n){const e=o.useContext(t);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),o.createElement(t.Provider,{value:e},n.children)}},9612:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"Physical-AI-Humanoid-Robotics/vision-language-action/quiz","title":"Module 4 Quiz - Vision-Language-Action (VLA)","description":"Instructions","source":"@site/docs/13-Physical-AI-Humanoid-Robotics/04-vision-language-action/06-quiz.md","sourceDirName":"13-Physical-AI-Humanoid-Robotics/04-vision-language-action","slug":"/Physical-AI-Humanoid-Robotics/vision-language-action/quiz","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/quiz","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/04-vision-language-action/06-quiz.md","tags":[{"inline":true,"label":"quiz","permalink":"/hackathon-phase-01/docs/tags/quiz"},{"inline":true,"label":"vla","permalink":"/hackathon-phase-01/docs/tags/vla"},{"inline":true,"label":"assessment","permalink":"/hackathon-phase-01/docs/tags/assessment"},{"inline":true,"label":"robotics","permalink":"/hackathon-phase-01/docs/tags/robotics"}],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Module 4 Quiz - Vision-Language-Action (VLA)","sidebar_position":6,"skills":["VLA Concepts","Speech Recognition","LLM Planning","Vision-Language Integration","Action Execution","ROS2 Integration"],"learning_objectives":["Demonstrate understanding of Vision-Language-Action system components and integration","Apply knowledge of OpenAI Whisper integration for robotics applications","Analyze cognitive planning with LLMs for task decomposition","Evaluate vision-language integration techniques for robotics","Assess action execution and safety validation in VLA systems"],"cognitive_load":5,"differentiation":"Self-Assessment","tags":["quiz","vla","assessment","robotics"],"created":"2025-12-23","last_modified":"2025-12-23","ros2_version":"humble"},"sidebar":"tutorialSidebar","previous":{"title":"Summary - Capstone Project - The Autonomous Humanoid","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/capstone-project.summary"}}');var s=i(4848),t=i(8453);const a={title:"Module 4 Quiz - Vision-Language-Action (VLA)",sidebar_position:6,skills:["VLA Concepts","Speech Recognition","LLM Planning","Vision-Language Integration","Action Execution","ROS2 Integration"],learning_objectives:["Demonstrate understanding of Vision-Language-Action system components and integration","Apply knowledge of OpenAI Whisper integration for robotics applications","Analyze cognitive planning with LLMs for task decomposition","Evaluate vision-language integration techniques for robotics","Assess action execution and safety validation in VLA systems"],cognitive_load:5,differentiation:"Self-Assessment",tags:["quiz","vla","assessment","robotics"],created:"2025-12-23",last_modified:"2025-12-23",ros2_version:"humble"},r="Module 4 Quiz: Vision-Language-Action (VLA)",l={},c=[{value:"Instructions",id:"instructions",level:2},{value:"Questions 1-4: Voice-to-Action Systems (Lesson 1)",id:"questions-1-4-voice-to-action-systems-lesson-1",level:2},{value:"Questions 5-8: Cognitive Planning with LLMs (Lesson 2)",id:"questions-5-8-cognitive-planning-with-llms-lesson-2",level:2},{value:"Questions 9-12: Vision-Language Integration (Lesson 3)",id:"questions-9-12-vision-language-integration-lesson-3",level:2},{value:"Questions 13-15: Action Execution and Control (Lesson 4)",id:"questions-13-15-action-execution-and-control-lesson-4",level:2},{value:"Questions 16-18: Integration and Capstone Concepts",id:"questions-16-18-integration-and-capstone-concepts",level:2},{value:"Answer Key",id:"answer-key",level:2},{value:"Questions 1-4: Voice-to-Action Systems",id:"questions-1-4-voice-to-action-systems",level:3},{value:"Questions 5-8: Cognitive Planning with LLMs",id:"questions-5-8-cognitive-planning-with-llms",level:3},{value:"Questions 9-12: Vision-Language Integration",id:"questions-9-12-vision-language-integration",level:3},{value:"Questions 13-15: Action Execution and Control",id:"questions-13-15-action-execution-and-control",level:3},{value:"Questions 16-18: Integration and Capstone Concepts",id:"questions-16-18-integration-and-capstone-concepts-1",level:3},{value:"Grading Rubric",id:"grading-rubric",level:2}];function h(n){const e={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"module-4-quiz-vision-language-action-vla",children:"Module 4 Quiz: Vision-Language-Action (VLA)"})}),"\n",(0,s.jsx)(e.h2,{id:"instructions",children:"Instructions"}),"\n",(0,s.jsx)(e.p,{children:"This quiz assesses your understanding of Vision-Language-Action (VLA) systems. The quiz contains 18 questions covering all aspects of VLA: voice-to-action systems, cognitive planning with LLMs, vision-language integration, and action execution. To pass, you must score at least 80% (14 out of 18 questions correct)."}),"\n",(0,s.jsx)(e.h2,{id:"questions-1-4-voice-to-action-systems-lesson-1",children:"Questions 1-4: Voice-to-Action Systems (Lesson 1)"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Multiple Choice"}),": What is the primary purpose of audio preprocessing in a VLA system?"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"A) To increase the volume of the audio signal"}),"\n",(0,s.jsx)(e.li,{children:"B) To optimize audio quality for speech recognition accuracy"}),"\n",(0,s.jsx)(e.li,{children:"C) To compress the audio for faster transmission"}),"\n",(0,s.jsx)(e.li,{children:"D) To convert audio to text format"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Multiple Choice"}),": Which of the following is NOT a key component of a voice-to-action system?"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"A) Speech recognition engine"}),"\n",(0,s.jsx)(e.li,{children:"B) Natural language understanding module"}),"\n",(0,s.jsx)(e.li,{children:"C) Computer vision processor"}),"\n",(0,s.jsx)(e.li,{children:"D) Action execution system"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Short Answer"}),": Explain the role of confidence scoring in voice recognition systems and why it's important for robotic applications."]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Short Answer"}),": Describe the main challenges of implementing speech recognition in robotic environments compared to consumer applications."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"questions-5-8-cognitive-planning-with-llms-lesson-2",children:"Questions 5-8: Cognitive Planning with LLMs (Lesson 2)"}),"\n",(0,s.jsxs)(e.ol,{start:"5",children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Multiple Choice"}),': What does "Chain-of-Thought" prompting refer to in the context of LLM-based cognitive planning?']}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"A) A technique to make LLMs think step-by-step before providing answers"}),"\n",(0,s.jsx)(e.li,{children:"B) A method to chain multiple LLMs together"}),"\n",(0,s.jsx)(e.li,{children:"C) A way to reduce the computational cost of LLMs"}),"\n",(0,s.jsx)(e.li,{children:"D) A technique to improve the speed of LLM processing"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Multiple Choice"}),": Which of the following is a key principle of task decomposition in cognitive planning?"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"A) Breaking complex tasks into hierarchical subtasks"}),"\n",(0,s.jsx)(e.li,{children:"B) Executing all tasks in parallel for efficiency"}),"\n",(0,s.jsx)(e.li,{children:"C) Minimizing the number of required sensors"}),"\n",(0,s.jsx)(e.li,{children:"D) Reducing the robot's mobility requirements"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Short Answer"}),': Describe how an LLM might decompose the command "Clean the living room" into executable robotic actions.']}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Scenario-Based"}),': A user says "Set up for a meeting" to a humanoid robot. What clarifying questions might the robot need to ask, and why are they important for successful task execution?']}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"questions-9-12-vision-language-integration-lesson-3",children:"Questions 9-12: Vision-Language Integration (Lesson 3)"}),"\n",(0,s.jsxs)(e.ol,{start:"9",children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Multiple Choice"}),": What is object grounding in vision-language integration?"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"A) Connecting linguistic references to specific visual entities"}),"\n",(0,s.jsx)(e.li,{children:"B) Grounding the robot to prevent electrical hazards"}),"\n",(0,s.jsx)(e.li,{children:"C) Connecting the robot to the internet"}),"\n",(0,s.jsx)(e.li,{children:"D) Calibrating the robot's sensors"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Multiple Choice"}),": Which vision-language model is known for learning visual concepts from natural language descriptions?"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"A) GPT-3"}),"\n",(0,s.jsx)(e.li,{children:"B) CLIP"}),"\n",(0,s.jsx)(e.li,{children:"C) Whisper"}),"\n",(0,s.jsx)(e.li,{children:"D) DALL-E"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Short Answer"}),': Explain how a vision-language system would handle the command "the red cup on the left" when multiple red cups are visible in the scene.']}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Scenario-Based"}),': Describe the challenges a vision-language system faces when trying to identify "the big book" in a scene with multiple books of similar size. How might the system resolve this ambiguity?']}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"questions-13-15-action-execution-and-control-lesson-4",children:"Questions 13-15: Action Execution and Control (Lesson 4)"}),"\n",(0,s.jsxs)(e.ol,{start:"13",children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Multiple Choice"}),": What is the primary role of ROS2 action servers in VLA systems?"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"A) To store robot configuration data"}),"\n",(0,s.jsx)(e.li,{children:"B) To provide standardized interfaces for long-running tasks with feedback"}),"\n",(0,s.jsx)(e.li,{children:"C) To process voice commands"}),"\n",(0,s.jsx)(e.li,{children:"D) To perform computer vision tasks"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Multiple Choice"}),": Which of the following is NOT a key component of safety validation in VLA systems?"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"A) Collision detection"}),"\n",(0,s.jsx)(e.li,{children:"B) Capability verification"}),"\n",(0,s.jsx)(e.li,{children:"C) Audio preprocessing"}),"\n",(0,s.jsx)(e.li,{children:"D) Context validation"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Short Answer"}),": Explain the importance of feedback control loops in VLA action execution and provide an example of how they might adapt to changing conditions."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"questions-16-18-integration-and-capstone-concepts",children:"Questions 16-18: Integration and Capstone Concepts"}),"\n",(0,s.jsxs)(e.ol,{start:"16",children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Short Answer"}),": Describe the complete VLA pipeline from voice command to robot action execution, highlighting the key integration points between components."]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Multiple Choice"}),": Which of the following represents the correct sequence of processing in a VLA system?"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"A) Action execution \u2192 Cognitive planning \u2192 Voice recognition \u2192 Vision-language integration"}),"\n",(0,s.jsx)(e.li,{children:"B) Voice recognition \u2192 Cognitive planning \u2192 Vision-language integration \u2192 Action execution"}),"\n",(0,s.jsx)(e.li,{children:"C) Vision-language integration \u2192 Voice recognition \u2192 Cognitive planning \u2192 Action execution"}),"\n",(0,s.jsx)(e.li,{children:"D) Cognitive planning \u2192 Voice recognition \u2192 Vision-language integration \u2192 Action execution"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Scenario-Based"}),': A humanoid robot receives the command "Please bring me the blue water bottle from the kitchen." Describe how each component of the VLA system would contribute to fulfilling this request.']}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"answer-key",children:"Answer Key"}),"\n",(0,s.jsx)(e.h3,{id:"questions-1-4-voice-to-action-systems",children:"Questions 1-4: Voice-to-Action Systems"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"B) To optimize audio quality for speech recognition accuracy"}),"\n",(0,s.jsx)(e.li,{children:"C) Computer vision processor"}),"\n",(0,s.jsx)(e.li,{children:"Confidence scoring indicates the reliability of speech recognition results. In robotics, commands with low confidence should trigger clarification requests rather than execution to prevent robots from acting on misrecognized commands, which could be dangerous or counterproductive."}),"\n",(0,s.jsx)(e.li,{children:"Key challenges include robot-internal noise from motors and fans, acoustic reflections in indoor environments creating reverberation, and real-time processing requirements that differ from batch processing applications."}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"questions-5-8-cognitive-planning-with-llms",children:"Questions 5-8: Cognitive Planning with LLMs"}),"\n",(0,s.jsxs)(e.ol,{start:"5",children:["\n",(0,s.jsx)(e.li,{children:"A) A technique to make LLMs think step-by-step before providing answers"}),"\n",(0,s.jsx)(e.li,{children:"A) Breaking complex tasks into hierarchical subtasks"}),"\n",(0,s.jsx)(e.li,{children:'The LLM would decompose "Clean the living room" into subtasks like: navigate to living room, identify dirty items, categorize items (trash vs. misplaced), dispose of trash, return misplaced items to proper locations, verify cleanliness.'}),"\n",(0,s.jsx)(e.li,{children:'Clarifying questions might include: "Where is the meeting?", "What items are needed for the meeting?", "Are there specific arrangements required?", "Who will attend the meeting?" These questions are important because "setting up for a meeting" is ambiguous without context.'}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"questions-9-12-vision-language-integration",children:"Questions 9-12: Vision-Language Integration"}),"\n",(0,s.jsxs)(e.ol,{start:"9",children:["\n",(0,s.jsx)(e.li,{children:"A) Connecting linguistic references to specific visual entities"}),"\n",(0,s.jsx)(e.li,{children:"B) CLIP"}),"\n",(0,s.jsx)(e.li,{children:"The system would use spatial reasoning to identify which red cup is positioned to the left relative to the speaker's perspective or a reference point. It might consider additional context like distance from other objects to disambiguate."}),"\n",(0,s.jsx)(e.li,{children:'The system might use additional context like "the big book near the laptop" or "the big book on the table" to resolve ambiguity. It could also ask for clarification like "Do you mean the book on the left or the one on the right?"'}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"questions-13-15-action-execution-and-control",children:"Questions 13-15: Action Execution and Control"}),"\n",(0,s.jsxs)(e.ol,{start:"13",children:["\n",(0,s.jsx)(e.li,{children:"B) To provide standardized interfaces for long-running tasks with feedback"}),"\n",(0,s.jsx)(e.li,{children:"C) Audio preprocessing"}),"\n",(0,s.jsx)(e.li,{children:"Feedback control loops allow the system to monitor execution progress and adapt to changing conditions. For example, if a planned navigation path becomes blocked by a moving obstacle, the feedback loop would detect this and trigger replanning to find an alternative route."}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"questions-16-18-integration-and-capstone-concepts-1",children:"Questions 16-18: Integration and Capstone Concepts"}),"\n",(0,s.jsxs)(e.ol,{start:"16",children:["\n",(0,s.jsx)(e.li,{children:"The VLA pipeline starts with voice recognition converting speech to text, followed by cognitive planning using LLMs to decompose commands into action sequences, vision-language integration to ground language in visual entities, and action execution through ROS2 action servers with safety validation throughout."}),"\n",(0,s.jsx)(e.li,{children:"B) Voice recognition \u2192 Cognitive planning \u2192 Vision-language integration \u2192 Action execution"}),"\n",(0,s.jsx)(e.li,{children:'Voice recognition processes "Please bring me the blue water bottle from the kitchen"; cognitive planning decomposes this into navigation and manipulation tasks; vision-language integration identifies the blue water bottle in the kitchen scene; action execution orchestrates navigation to the kitchen, approach to the bottle, grasp, and return to the user.'}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"grading-rubric",children:"Grading Rubric"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Multiple Choice Questions (1, 2, 5, 6, 9, 10, 13, 14, 17): 1 point each"}),"\n",(0,s.jsx)(e.li,{children:"Short Answer Questions (3, 4, 7, 11, 15): 2 points each (1 point for key concept, 1 point for explanation)"}),"\n",(0,s.jsx)(e.li,{children:"Scenario-Based Questions (8, 12, 18): 3 points each (1 point for approach, 1 point for technical accuracy, 1 point for completeness)"}),"\n",(0,s.jsx)(e.li,{children:"Integration Questions (16): 2 points"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Total Points"}),": 18\r\n",(0,s.jsx)(e.strong,{children:"Passing Score"}),": 14/18 (80%)"]})]})}function d(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(h,{...n})}):h(n)}}}]);