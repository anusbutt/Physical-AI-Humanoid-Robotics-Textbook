"use strict";(globalThis.webpackChunkbook_source=globalThis.webpackChunkbook_source||[]).push([[3267],{6781:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems.summary","title":"Summary: Lesson 1 - Camera Systems and Computer Vision","description":"Module: Module 2 - Sensors and Perception for Humanoid Robots","source":"@site/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/01-camera-systems.summary.md","sourceDirName":"13-Physical-AI-Humanoid-Robotics/02-sensors-perception","slug":"/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems.summary","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems.summary","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/01-camera-systems.summary.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1: Camera Systems and Computer Vision","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems"},"next":{"title":"Lesson 2: Depth Sensing Technologies","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing"}}');var r=n(4848),t=n(8453);const a={},l="Summary: Lesson 1 - Camera Systems and Computer Vision",o={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Key Concepts Covered",id:"key-concepts-covered",level:2},{value:"Camera Types (Section 3.1)",id:"camera-types-section-31",level:3},{value:"Camera Parameters (Section 3.2)",id:"camera-parameters-section-32",level:3},{value:"Camera Placement (Section 3.3)",id:"camera-placement-section-33",level:3},{value:"ROS2 Integration (Sections 3.4-3.5)",id:"ros2-integration-sections-34-35",level:3},{value:"Real-World Examples",id:"real-world-examples",level:2},{value:"Boston Dynamics Atlas",id:"boston-dynamics-atlas",level:3},{value:"Tesla Optimus",id:"tesla-optimus",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Practice Exercises",id:"practice-exercises",level:2},{value:"Common Pitfalls (Expert Insights)",id:"common-pitfalls-expert-insights",level:2},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Metadata",id:"metadata",level:2},{value:"Validation Status",id:"validation-status",level:2}];function d(e){const s={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.header,{children:(0,r.jsx)(s.h1,{id:"summary-lesson-1---camera-systems-and-computer-vision",children:"Summary: Lesson 1 - Camera Systems and Computer Vision"})}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Module"}),": Module 2 - Sensors and Perception for Humanoid Robots\r\n",(0,r.jsx)(s.strong,{children:"Lesson"}),": 01-camera-systems.md\r\n",(0,r.jsx)(s.strong,{children:"Target Audience"}),": CS students with Python + Module 1 (ROS2) knowledge\r\n",(0,r.jsx)(s.strong,{children:"Estimated Time"}),": 30-45 minutes\r\n",(0,r.jsx)(s.strong,{children:"Difficulty"}),": Beginner"]}),"\n",(0,r.jsx)(s.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsx)(s.p,{children:"By the end of this lesson, students will be able to:"}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Understand"})," the five foundational camera parameters (pixels, resolution, frame rate, field of view, image encoding)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Differentiate"})," between monocular, stereo, and RGB-D cameras based on trade-offs"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Apply"})," ROS2 sensor_msgs/Image and CameraInfo message structures to process camera data"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Analyze"})," camera placement strategies (head, wrist, chest) and their impact on humanoid capabilities"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Evaluate"})," trade-offs in multi-camera system design for competing requirements"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"key-concepts-covered",children:"Key Concepts Covered"}),"\n",(0,r.jsx)(s.h3,{id:"camera-types-section-31",children:"Camera Types (Section 3.1)"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Monocular"}),": Simple, low-cost, no depth (2D only)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Stereo"}),": Passive depth via triangulation, 0.5-10m range"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"RGB-D"}),": Active depth (IR/ToF), 0.3-10m range, limited outdoor use"]}),"\n"]}),"\n",(0,r.jsx)(s.h3,{id:"camera-parameters-section-32",children:"Camera Parameters (Section 3.2)"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Resolution"}),": 640\xd7480 (VGA) to 4K, trade-off with computation"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Field of View"}),": 30-180\xb0, inverse relationship with focal length"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Frame Rate"}),": 10-60+ FPS, trade-off with bandwidth"]}),"\n"]}),"\n",(0,r.jsx)(s.h3,{id:"camera-placement-section-33",children:"Camera Placement (Section 3.3)"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Head-mounted"}),": Navigation, situational awareness, pan/tilt capability"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Wrist-mounted"}),": Manipulation, visual servoing, eye-in-hand"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Chest-mounted"}),": Stable SLAM reference, compromise viewpoint"]}),"\n"]}),"\n",(0,r.jsx)(s.h3,{id:"ros2-integration-sections-34-35",children:"ROS2 Integration (Sections 3.4-3.5)"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"sensor_msgs/Image"}),": header, dimensions, encoding, step, raw data"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"sensor_msgs/CameraInfo"}),": K matrix (intrinsics), distortion coefficients"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Publisher-Subscriber Pattern"}),": Camera driver \u2192 multiple vision nodes"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"QoS"}),": Best effort reliability, moderate queue depth for real-time"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"real-world-examples",children:"Real-World Examples"}),"\n",(0,r.jsx)(s.h3,{id:"boston-dynamics-atlas",children:"Boston Dynamics Atlas"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Stereo cameras in head (752\xd7480 @ 30fps, 10cm baseline)"}),"\n",(0,r.jsx)(s.li,{children:"RGB cameras in wrists (1280\xd7720 @ 10-15fps)"}),"\n",(0,r.jsx)(s.li,{children:"Task specialization: depth for locomotion, color for manipulation"}),"\n"]}),"\n",(0,r.jsx)(s.h3,{id:"tesla-optimus",children:"Tesla Optimus"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Eight monocular cameras (1280\xd7960 @ 36fps)"}),"\n",(0,r.jsx)(s.li,{children:"Vision-only approach (no lidar)"}),"\n",(0,r.jsx)(s.li,{children:"Neural network depth estimation from monocular cues"}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"CameraSubscriber"}),": Subscribe to /camera/image_raw and log metadata"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"CameraInfoSubscriber"}),": Extract K matrix and calculate FOV from focal length"]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"Both examples demonstrate:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Type hints on all function signatures"}),"\n",(0,r.jsx)(s.li,{children:"ROS2 node inheritance pattern"}),"\n",(0,r.jsx)(s.li,{children:"Callback-based message processing"}),"\n",(0,r.jsx)(s.li,{children:"Proper use of sensor_msgs types"}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"practice-exercises",children:"Practice Exercises"}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Multi-Camera System Design"}),": Design vision system for delivery robot (navigation + object recognition + human interaction)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Live Topic Inspection"}),": Use ros2 CLI tools to inspect camera topics and extract parameters"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"AI Colearning Prompts"}),":","\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"FOV/focal length relationship analogy"}),"\n",(0,r.jsx)(s.li,{children:"Stereo synchronization requirements"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"common-pitfalls-expert-insights",children:"Common Pitfalls (Expert Insights)"}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Resolution Sweet Spot"}),": 640\xd7480 @ 15-30fps often better than 4K for real-time humanoid applications"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"RGB vs BGR"}),": OpenCV uses BGR by default; rgb8 messages need conversion to avoid color inversion"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,r.jsx)(s.p,{children:"Students demonstrate mastery when they can:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Explain camera type differences and justify selection for specific humanoid tasks"}),"\n",(0,r.jsx)(s.li,{children:"Subscribe to ROS2 camera topics and process image/calibration data"}),"\n",(0,r.jsx)(s.li,{children:"Design multi-camera configurations with trade-off justification"}),"\n",(0,r.jsx)(s.li,{children:"Calculate FOV from intrinsic matrix parameters"}),"\n",(0,r.jsx)(s.li,{children:"Identify appropriate camera placements for manipulation vs navigation"}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Module 1: ROS2 Basics (nodes, topics, publishers, subscribers, message types)"}),"\n",(0,r.jsx)(s.li,{children:"Python 3.11+ with type hints"}),"\n",(0,r.jsx)(s.li,{children:"Basic linear algebra (vectors, matrices for K matrix understanding)"}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Lesson 2"}),": Depth Sensing Technologies (LiDAR, structured light, ToF)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Connection"}),": Cameras provide 2D semantic info; depth sensors add 3D spatial awareness"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"metadata",children:"Metadata"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Generated by"}),": Agent Pipeline (9-agent system)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Created"}),": 2025-12-08"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Tags"}),": ros2, sensors, camera, computer-vision, humanoid-robotics"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Cognitive Load"}),": Moderate (6 new concepts, builds on Module 1)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Word Count"}),": ~3,200 words (including code + callouts)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Sections"}),": 7 (What Is, Why Matters, Key Principles, Callouts, Code Examples, Summary, Next Steps)"]}),"\n"]}),"\n",(0,r.jsx)(s.h2,{id:"validation-status",children:"Validation Status"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"\u2705 Technical Review: PASS WITH MINOR REVISIONS (all fixed)"}),"\n",(0,r.jsx)(s.li,{children:"\u2705 Structure & Style: PASS (all 7 sections, proper callouts)"}),"\n",(0,r.jsx)(s.li,{children:"\u2705 Frontmatter: COMPLETE (13 fields generated)"}),"\n",(0,r.jsx)(s.li,{children:"\u2705 Code Quality: PASS (type hints, docstrings, ROS2 patterns)"}),"\n",(0,r.jsx)(s.li,{children:"\u2705 Case Studies: 2 detailed examples (Atlas, Optimus)"}),"\n",(0,r.jsx)(s.li,{children:"\u2705 Callouts: 2 AI Colearning, 2 Expert Insights, 2 Practice Exercises"}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>a,x:()=>l});var i=n(6540);const r={},t=i.createContext(r);function a(e){const s=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(t.Provider,{value:s},e.children)}}}]);