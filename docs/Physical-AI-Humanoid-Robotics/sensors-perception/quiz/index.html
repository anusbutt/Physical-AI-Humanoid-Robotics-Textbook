<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Physical-AI-Humanoid-Robotics/sensors-perception/quiz" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Module 2 Quiz: Sensors and Perception for Humanoid Robots | Physical AI &amp; Humanoid Robotics Textbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://anusbutt.github.io/hackathon-phase-01/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://anusbutt.github.io/hackathon-phase-01/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/quiz"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Module 2 Quiz: Sensors and Perception for Humanoid Robots | Physical AI &amp; Humanoid Robotics Textbook"><meta data-rh="true" name="description" content="Assessment quiz covering camera systems, depth sensing, IMU sensors, and sensor fusion for humanoid robots. 15 questions testing comprehension across all four lessons. Passing score: 12/18 points (67%)."><meta data-rh="true" property="og:description" content="Assessment quiz covering camera systems, depth sensing, IMU sensors, and sensor fusion for humanoid robots. 15 questions testing comprehension across all four lessons. Passing score: 12/18 points (67%)."><link data-rh="true" rel="icon" href="/hackathon-phase-01/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/quiz"><link data-rh="true" rel="alternate" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/quiz" hreflang="en"><link data-rh="true" rel="alternate" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/quiz" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Physical AI & Humanoid Robotics","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"},{"@type":"ListItem","position":2,"name":"Module 2: Sensors and Perception for Humanoid Robots","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"},{"@type":"ListItem","position":3,"name":"Module 2 Quiz: Sensors and Perception for Humanoid Robots","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/quiz"}]}</script><link rel="alternate" type="application/rss+xml" href="/hackathon-phase-01/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics Textbook RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/hackathon-phase-01/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Textbook Atom Feed"><link rel="stylesheet" href="/hackathon-phase-01/assets/css/styles.5903934b.css">
<script src="/hackathon-phase-01/assets/js/runtime~main.94161a1c.js" defer="defer"></script>
<script src="/hackathon-phase-01/assets/js/main.ab534ac3.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/hackathon-phase-01/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/hackathon-phase-01/"><div class="navbar__logo"><img src="/hackathon-phase-01/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/hackathon-phase-01/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics Textbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/">Textbook</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/anusbutt/hackathon-phase-01" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"><span title="Physical AI &amp; Humanoid Robotics" class="categoryLinkLabel_W154">Physical AI &amp; Humanoid Robotics</span></a><button aria-label="Collapse sidebar category &#x27;Physical AI &amp; Humanoid Robotics&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/ros2-nervous-system/"><span title="Module 1: The Robotic Nervous System (ROS 2)" class="categoryLinkLabel_W154">Module 1: The Robotic Nervous System (ROS 2)</span></a><button aria-label="Expand sidebar category &#x27;Module 1: The Robotic Nervous System (ROS 2)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"><span title="Module 2: Sensors and Perception for Humanoid Robots" class="categoryLinkLabel_W154">Module 2: Sensors and Perception for Humanoid Robots</span></a><button aria-label="Collapse sidebar category &#x27;Module 2: Sensors and Perception for Humanoid Robots&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems"><span title="Lesson 1: Camera Systems and Computer Vision" class="linkLabel_WmDU">Lesson 1: Camera Systems and Computer Vision</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems.summary"><span title="Summary: Lesson 1 - Camera Systems and Computer Vision" class="linkLabel_WmDU">Summary: Lesson 1 - Camera Systems and Computer Vision</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing"><span title="Lesson 2: Depth Sensing Technologies" class="linkLabel_WmDU">Lesson 2: Depth Sensing Technologies</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing.summary"><span title="Summary: Lesson 2 - Depth Sensing Technologies" class="linkLabel_WmDU">Summary: Lesson 2 - Depth Sensing Technologies</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception"><span title="IMU &amp; Proprioception" class="linkLabel_WmDU">IMU &amp; Proprioception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception.summary"><span title="Summary: Lesson 3 - IMU and Proprioception" class="linkLabel_WmDU">Summary: Lesson 3 - IMU and Proprioception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion"><span title="Sensor Fusion" class="linkLabel_WmDU">Sensor Fusion</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion.summary"><span title="Lesson 4 Summary: Sensor Fusion Techniques for Humanoid Robots" class="linkLabel_WmDU">Lesson 4 Summary: Sensor Fusion Techniques for Humanoid Robots</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.architecture"><span title="Multi-Sensor Fusion Capstone: Architecture Documentation" class="linkLabel_WmDU">Multi-Sensor Fusion Capstone: Architecture Documentation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.casestudies"><span title="Case Studies: Real-World Multi-Sensor Integration" class="linkLabel_WmDU">Case Studies: Real-World Multi-Sensor Integration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content"><span title="Module 2 Capstone Project: Integrated Sensor System Design" class="linkLabel_WmDU">Module 2 Capstone Project: Integrated Sensor System Design</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project"><span title="Capstone Project: Design a Multi-Sensor Humanoid Perception System" class="linkLabel_WmDU">Capstone Project: Design a Multi-Sensor Humanoid Perception System</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.summary"><span title="Capstone Summary" class="linkLabel_WmDU">Capstone Summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/quiz"><span title="Module 2 Quiz: Sensors and Perception for Humanoid Robots" class="linkLabel_WmDU">Module 2 Quiz: Sensors and Perception for Humanoid Robots</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/isaac-ai-brain/"><span title="Module 3: The AI-Robot Brain (NVIDIA Isaacâ„¢)" class="categoryLinkLabel_W154">Module 3: The AI-Robot Brain (NVIDIA Isaacâ„¢)</span></a><button aria-label="Expand sidebar category &#x27;Module 3: The AI-Robot Brain (NVIDIA Isaacâ„¢)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/"><span title="Module 4: Vision-Language-Action (VLA)" class="categoryLinkLabel_W154">Module 4: Vision-Language-Action (VLA)</span></a><button aria-label="Expand sidebar category &#x27;Module 4: Vision-Language-Action (VLA)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/hackathon-phase-01/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"><span>Physical AI &amp; Humanoid Robotics</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"><span>Module 2: Sensors and Perception for Humanoid Robots</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Module 2 Quiz: Sensors and Perception for Humanoid Robots</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Module 2 Quiz: Sensors and Perception for Humanoid Robots</h1></header>
<p>Test your understanding of camera systems, depth sensing, IMU sensors, and sensor fusion techniques for humanoid robots.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="instructions">Instructions<a href="#instructions" class="hash-link" aria-label="Direct link to Instructions" title="Direct link to Instructions" translate="no">â€‹</a></h2>
<ul>
<li class=""><strong>Total Questions</strong>: 15</li>
<li class=""><strong>Passing Score</strong>: 12/15 correct (80%)</li>
<li class=""><strong>Time Limit</strong>: 45 minutes (recommended)</li>
<li class=""><strong>Question Types</strong>: Multiple choice, short answer, code reading</li>
<li class=""><strong>Resources</strong>: You may refer to lesson notes during the quiz</li>
<li class=""><strong>Scoring</strong>:<!-- -->
<ul>
<li class="">Multiple choice: 1 point each</li>
<li class="">Short answer: 1-2 points each (partial credit available)</li>
<li class="">Code reading: 2 points each (with rubric)</li>
</ul>
</li>
</ul>
<p><strong>Need to review?</strong> Questions reference specific lessons. If you&#x27;re unsure, revisit:</p>
<ul>
<li class="">Questions 1-4: <a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems">Lesson 1 - Camera Systems</a></li>
<li class="">Questions 5-8: <a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing">Lesson 2 - Depth Sensing</a></li>
<li class="">Questions 9-11: <a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception">Lesson 3 - IMU &amp; Proprioception</a></li>
<li class="">Questions 12-13: <a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion">Lesson 4 - Sensor Fusion</a></li>
<li class="">Questions 14-15: Integration across multiple lessons</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="questions">Questions<a href="#questions" class="hash-link" aria-label="Direct link to Questions" title="Direct link to Questions" translate="no">â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="lesson-1-camera-systems-questions-1-4">Lesson 1: Camera Systems (Questions 1-4)<a href="#lesson-1-camera-systems-questions-1-4" class="hash-link" aria-label="Direct link to Lesson 1: Camera Systems (Questions 1-4)" title="Direct link to Lesson 1: Camera Systems (Questions 1-4)" translate="no">â€‹</a></h3>
<p><strong>Question 1 (Multiple Choice)</strong> - 1 point</p>
<p>What is the primary difference between monocular, stereo, and RGB-D cameras in terms of depth perception?</p>
<p>A) Monocular cameras can see in 3D, stereo cameras are 2D, RGB-D cameras are color only
B) Monocular cameras provide no depth, stereo cameras estimate depth through triangulation, RGB-D cameras provide direct depth measurements
C) Monocular cameras are for indoor use, stereo cameras for outdoor, RGB-D cameras for underwater
D) Monocular cameras are expensive, stereo cameras are cheap, RGB-D cameras are free</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct Answer: B</strong></p><p><strong>Explanation</strong>: Monocular cameras provide only 2D images with no depth information. Stereo cameras use two lenses to capture slightly different views, allowing depth estimation through triangulation. RGB-D cameras (like Kinect) provide both color (RGB) and direct depth measurements (D) from structured light or time-of-flight sensors.</p><p><em>Lesson Reference: 01-camera-systems.md (Camera Types for Humanoid Robots)</em></p></div></div></details>
<hr>
<p><strong>Question 2 (Multiple Choice)</strong> - 1 point</p>
<p>Which ROS2 message type is used for standard camera image data?</p>
<p>A) sensor_msgs/Image
B) sensor_msgs/Camera
C) sensor_msgs/Video
D) sensor_msgs/CameraImage</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct Answer: A</strong></p><p><strong>Explanation</strong>: The sensor_msgs/Image message type is the standard ROS2 message for camera image data. It contains the raw pixel data along with metadata like encoding, height, width, and step size.</p><p><em>Lesson Reference: 01-camera-systems.md (ROS2 Integration - sensor_msgs/Image)</em></p></div></div></details>
<hr>
<p><strong>Question 3 (Short Answer)</strong> - 2 points</p>
<p>Explain the trade-offs between placing cameras on a humanoid robot&#x27;s head, chest, and wrists. For each location, describe one advantage and one disadvantage for humanoid robot tasks.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer &amp; Rubric</summary><div><div class="collapsibleContent_i85q"><p><strong>Sample Answer</strong>:</p><ul>
<li class=""><strong>Head</strong>: Advantage - matches human eye perspective for natural interaction and navigation. Disadvantage - limited view of hands during manipulation tasks.</li>
<li class=""><strong>Chest</strong>: Advantage - good view of ground and obstacles for navigation. Disadvantage - limited view of upper environment and human faces during interaction.</li>
<li class=""><strong>Wrists</strong>: Advantage - provides visual feedback during manipulation tasks. Disadvantage - limited field of view and constantly moving with arm motion.</li>
</ul><p><strong>Grading Rubric (2 points total)</strong>:</p><ul>
<li class=""><strong>2 points</strong>: Correctly identifies advantages and disadvantages for all three locations with specific robot tasks</li>
<li class=""><strong>1 point</strong>: Correctly identifies advantages/disadvantages for 1-2 locations OR has partial understanding</li>
<li class=""><strong>0 points</strong>: Incorrect or irrelevant responses</li>
</ul><p><em>Lesson Reference: 01-camera-systems.md (Camera Placement Strategies)</em></p></div></div></details>
<hr>
<p><strong>Question 4 (Short Answer)</strong> - 1 point</p>
<p>What does the &quot;field of view&quot; (FOV) parameter indicate for a camera, and why is it important for humanoid robot navigation?</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer &amp; Rubric</summary><div><div class="collapsibleContent_i85q"><p><strong>Sample Answer</strong>: Field of view indicates the angular extent of the scene that a camera can capture. For humanoid robots, FOV is important because it determines how much of the environment the robot can see at once, affecting obstacle detection, navigation safety, and situational awareness.</p><p><strong>Grading Rubric (1 point)</strong>:</p><ul>
<li class=""><strong>1 point</strong>: Correctly explains FOV as angular extent AND relates to robot navigation/environment awareness</li>
<li class=""><strong>0 points</strong>: Incorrect or incomplete explanation</li>
</ul><p><em>Lesson Reference: 01-camera-systems.md (Camera Parameters - Field of View)</em></p></div></div></details>
<hr>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="lesson-2-depth-sensing-questions-5-8">Lesson 2: Depth Sensing (Questions 5-8)<a href="#lesson-2-depth-sensing-questions-5-8" class="hash-link" aria-label="Direct link to Lesson 2: Depth Sensing (Questions 5-8)" title="Direct link to Lesson 2: Depth Sensing (Questions 5-8)" translate="no">â€‹</a></h3>
<p><strong>Question 5 (Multiple Choice)</strong> - 1 point</p>
<p>How does 2D LiDAR differ from 3D LiDAR in terms of environmental perception for humanoid robots?</p>
<p>A) 2D LiDAR is for indoor use, 3D LiDAR is for outdoor use
B) 2D LiDAR measures distance in one plane (typically ground level), 3D LiDAR measures in multiple planes (volume)
C) 2D LiDAR is faster, 3D LiDAR is more accurate
D) 2D LiDAR is expensive, 3D LiDAR is cheap</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct Answer: B</strong></p><p><strong>Explanation</strong>: 2D LiDAR typically scans in a single horizontal plane and provides distance measurements in that plane only. 3D LiDAR scans in multiple planes to create a 3D point cloud, providing full volumetric information about the environment including height and depth.</p><p><em>Lesson Reference: 02-depth-sensing.md (LiDAR Technologies - 2D vs 3D)</em></p></div></div></details>
<hr>
<p><strong>Question 6 (Multiple Choice)</strong> - 1 point</p>
<p>Which ROS2 message type is used for 3D point cloud data from depth sensors?</p>
<p>A) sensor_msgs/PointCloud
B) sensor_msgs/Point3D
C) sensor_msgs/PointCloud2
D) sensor_msgs/LaserScan3D</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct Answer: C</strong></p><p><strong>Explanation</strong>: sensor_msgs/PointCloud2 is the standard ROS2 message for 3D point cloud data. It contains structured binary data representing 3D coordinates of points in space, along with optional color and other per-point information.</p><p><em>Lesson Reference: 02-depth-sensing.md (ROS2 Integration - sensor_msgs/PointCloud2)</em></p></div></div></details>
<hr>
<p><strong>Question 7 (Short Answer)</strong> - 2 points</p>
<p>Compare the advantages and disadvantages of LiDAR vs stereo vision for humanoid robot navigation. Provide one advantage and one disadvantage for each technology.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer &amp; Rubric</summary><div><div class="collapsibleContent_i85q"><p><strong>Sample Answer</strong>:</p><ul>
<li class=""><strong>LiDAR Advantages</strong>: Precise distance measurements, works in darkness, reliable in various lighting conditions</li>
<li class=""><strong>LiDAR Disadvantages</strong>: Cannot detect transparent surfaces, higher cost, sensitive to rain/fog</li>
<li class=""><strong>Stereo Vision Advantages</strong>: Provides color information, lower cost, works in most environments</li>
<li class=""><strong>Stereo Vision Disadvantages</strong>: Sensitive to lighting conditions, less accurate at longer distances, computationally intensive</li>
</ul><p><strong>Grading Rubric (2 points total)</strong>:</p><ul>
<li class=""><strong>2 points</strong>: Correctly identifies one advantage and one disadvantage for both technologies</li>
<li class=""><strong>1 point</strong>: Correctly identifies advantage/disadvantage for one technology OR partial understanding of both</li>
<li class=""><strong>0 points</strong>: Incorrect or irrelevant responses</li>
</ul><p><em>Lesson Reference: 02-depth-sensing.md (Depth Technology Comparison)</em></p></div></div></details>
<hr>
<p><strong>Question 8 (Short Answer)</strong> - 1 point</p>
<p>What is a &quot;point cloud&quot; in the context of depth sensing, and how does it differ from a regular 2D image?</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer &amp; Rubric</summary><div><div class="collapsibleContent_i85q"><p><strong>Sample Answer</strong>: A point cloud is a collection of 3D points in space, where each point has X, Y, and Z coordinates representing the 3D structure of objects in the environment. Unlike a 2D image which only has X, Y coordinates with color/intensity values, a point cloud provides full 3D spatial information about the scene.</p><p><strong>Grading Rubric (1 point)</strong>:</p><ul>
<li class=""><strong>1 point</strong>: Correctly explains point cloud as 3D points with X, Y, Z coordinates AND differentiates from 2D images</li>
<li class=""><strong>0 points</strong>: Incorrect or incomplete explanation</li>
</ul><p><em>Lesson Reference: 02-depth-sensing.md (Point Cloud Fundamentals)</em></p></div></div></details>
<hr>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="lesson-3-imu-and-proprioception-questions-9-11">Lesson 3: IMU and Proprioception (Questions 9-11)<a href="#lesson-3-imu-and-proprioception-questions-9-11" class="hash-link" aria-label="Direct link to Lesson 3: IMU and Proprioception (Questions 9-11)" title="Direct link to Lesson 3: IMU and Proprioception (Questions 9-11)" translate="no">â€‹</a></h3>
<p><strong>Question 9 (Multiple Choice)</strong> - 1 point</p>
<p>Which three sensors are typically combined in an IMU for humanoid robot balance?</p>
<p>A) Camera, gyroscope, magnetometer
B) Accelerometer, gyroscope, magnetometer
C) LiDAR, accelerometer, gyroscope
D) GPS, magnetometer, camera</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct Answer: B</strong></p><p><strong>Explanation</strong>: An IMU (Inertial Measurement Unit) combines three sensor types: accelerometer (measures linear acceleration and gravity), gyroscope (measures angular velocity/rotation rates), and magnetometer (measures magnetic field for heading). Together they provide comprehensive motion and orientation sensing.</p><p><em>Lesson Reference: 03-imu-proprioception.md (What Is an IMU?)</em></p></div></div></details>
<hr>
<p><strong>Question 10 (Multiple Choice)</strong> - 1 point</p>
<p>What does a gyroscope in an IMU measure for humanoid robot balance?</p>
<p>A) Linear acceleration and gravity
B) Angular velocity (rate of rotation)
C) Magnetic field direction
D) Distance to obstacles</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct Answer: B</strong></p><p><strong>Explanation</strong>: A gyroscope measures angular velocity - the rate at which the robot is rotating around any axis. This is crucial for balance control as it allows the robot to detect when it&#x27;s starting to tip or rotate unexpectedly.</p><p><em>Lesson Reference: 03-imu-proprioception.md (Gyroscope Function)</em></p></div></div></details>
<hr>
<p><strong>Question 11 (Short Answer)</strong> - 2 points</p>
<p>Explain how IMU data can be used to detect if a humanoid robot is falling. Describe the sensor readings that would indicate a fall is occurring.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer &amp; Rubric</summary><div><div class="collapsibleContent_i85q"><p><strong>Sample Answer</strong>: A humanoid robot falling would show characteristic IMU readings: (1) Accelerometer readings would show near-zero linear acceleration during freefall (as the robot and sensors accelerate together at gravity), followed by high acceleration during impact. (2) Gyroscope readings might show rapid, uncontrolled rotation during the fall. (3) The combination of these readings can be processed by fall detection algorithms that look for acceleration magnitude thresholds and rotation patterns that indicate loss of balance.</p><p><strong>Grading Rubric (2 points total)</strong>:</p><ul>
<li class=""><strong>2 points</strong>: Correctly explains freefall acceleration near zero AND impact acceleration AND mentions gyroscope rotation patterns</li>
<li class=""><strong>1 point</strong>: Correctly explains one or two aspects of fall detection</li>
<li class=""><strong>0 points</strong>: Incorrect or irrelevant explanation</li>
</ul><p><em>Lesson Reference: 03-imu-proprioception.md (Fall Detection with IMU)</em></p></div></div></details>
<hr>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="lesson-4-sensor-fusion-questions-12-13">Lesson 4: Sensor Fusion (Questions 12-13)<a href="#lesson-4-sensor-fusion-questions-12-13" class="hash-link" aria-label="Direct link to Lesson 4: Sensor Fusion (Questions 12-13)" title="Direct link to Lesson 4: Sensor Fusion (Questions 12-13)" translate="no">â€‹</a></h3>
<p><strong>Question 12 (Multiple Choice)</strong> - 1 point</p>
<p>Why is sensor fusion essential for humanoid robot perception?</p>
<p>A) It reduces the cost of individual sensors
B) It combines multiple sensors to overcome individual limitations and create more robust perception
C) It increases the size of the robot
D) It eliminates the need for any single sensor</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct Answer: B</strong></p><p><strong>Explanation</strong>: Sensor fusion combines data from multiple sensors to create more accurate, reliable, and complete information than any single sensor could provide. It allows robots to overcome limitations (e.g., cameras fail in darkness, LiDAR struggles with transparent surfaces) by intelligently integrating complementary strengths.</p><p><em>Lesson Reference: 04-sensor-fusion.md (What Is Sensor Fusion?)</em></p></div></div></details>
<hr>
<p><strong>Question 13 (Multiple Choice)</strong> - 1 point</p>
<p>What does a complementary filter do in sensor fusion?</p>
<p>A) It eliminates all sensor data
B) It combines gyroscope and accelerometer data with configurable time constants to provide drift-corrected orientation
C) It only uses camera data
D) It doubles the sensor data</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer</summary><div><div class="collapsibleContent_i85q"><p><strong>Correct Answer: B</strong></p><p><strong>Explanation</strong>: A complementary filter combines gyroscope data (good for short-term rotation tracking) with accelerometer data (good for long-term orientation reference) using configurable time constants. This provides drift-corrected orientation estimation by leveraging the strengths of each sensor type.</p><p><em>Lesson Reference: 04-sensor-fusion.md (Complementary Filtering)</em></p></div></div></details>
<hr>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="integration-questions-questions-14-15">Integration Questions (Questions 14-15)<a href="#integration-questions-questions-14-15" class="hash-link" aria-label="Direct link to Integration Questions (Questions 14-15)" title="Direct link to Integration Questions (Questions 14-15)" translate="no">â€‹</a></h3>
<p><strong>Question 14 (Short Answer)</strong> - 2 points</p>
<p>Design a minimal multi-sensor perception system for a humanoid robot that needs to navigate indoors, avoid obstacles, and maintain balance. Identify which sensors from Module 2 would be essential and explain why each is necessary.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer &amp; Rubric</summary><div><div class="collapsibleContent_i85q"><p><strong>Sample Answer</strong>: Essential sensors would include: (1) Stereo cameras for navigation scene understanding and human interaction, (2) 3D LiDAR for precise obstacle detection and mapping (works in darkness), (3) IMU for balance control and orientation estimation. Cameras provide visual context, LiDAR provides reliable distance measurements for navigation, and IMU provides internal motion sensing for balance. The fusion of all three creates robust perception that works in various lighting conditions and provides both external awareness and internal state knowledge.</p><p><strong>Grading Rubric (2 points total)</strong>:</p><ul>
<li class=""><strong>2 points</strong>: Identifies at least 2-3 appropriate sensors AND explains specific reasons for each in the context of the task</li>
<li class=""><strong>1 point</strong>: Identifies sensors but with limited explanation OR identifies only 1-2 sensors with good explanation</li>
<li class=""><strong>0 points</strong>: Incorrect sensor choices or irrelevant explanations</li>
</ul><p><em>Lesson Reference: 04-sensor-fusion.md (Multi-Sensor Integration)</em></p></div></div></details>
<hr>
<p><strong>Question 15 (Short Answer)</strong> - 2 points</p>
<p>Explain how a humanoid robot would use sensor fusion to navigate safely when transitioning from a well-lit indoor area to a dark hallway. What sensors would become more or less important in each environment?</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Reveal Answer &amp; Rubric</summary><div><div class="collapsibleContent_i85q"><p><strong>Sample Answer</strong>: In the well-lit area, cameras would provide excellent navigation information for obstacle detection and path planning. As the robot enters darkness, camera effectiveness decreases significantly. The fusion system would automatically reduce weighting of camera data while increasing reliance on LiDAR and depth sensors, which work independently of lighting. The IMU would continue providing balance and orientation information throughout the transition. The robot&#x27;s navigation system would adaptively combine available sensor data based on confidence levels, maintaining safe navigation throughout the lighting transition.</p><p><strong>Grading Rubric (2 points total)</strong>:</p><ul>
<li class=""><strong>2 points</strong>: Correctly explains sensor adaptation during lighting transition AND identifies which sensors become more/less important</li>
<li class=""><strong>1 point</strong>: Explains some aspects of sensor adaptation OR identifies some sensor changes</li>
<li class=""><strong>0 points</strong>: Incorrect or irrelevant explanation</li>
</ul><p><em>Lesson Reference: 04-sensor-fusion.md (Adaptive Sensor Weighting)</em></p></div></div></details>
<hr></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-tags-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/quiz">quiz</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/assessment">assessment</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/sensors">sensors</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/camera">camera</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/lidar">lidar</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/imu">imu</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/sensor-fusion">sensor-fusion</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/evaluation">evaluation</a></li></ul></div></div><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/06-quiz.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.summary"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Capstone Summary</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/isaac-ai-brain/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Module 3: The AI-Robot Brain (NVIDIA Isaacâ„¢)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#instructions" class="table-of-contents__link toc-highlight">Instructions</a></li><li><a href="#questions" class="table-of-contents__link toc-highlight">Questions</a><ul><li><a href="#lesson-1-camera-systems-questions-1-4" class="table-of-contents__link toc-highlight">Lesson 1: Camera Systems (Questions 1-4)</a></li><li><a href="#lesson-2-depth-sensing-questions-5-8" class="table-of-contents__link toc-highlight">Lesson 2: Depth Sensing (Questions 5-8)</a></li><li><a href="#lesson-3-imu-and-proprioception-questions-9-11" class="table-of-contents__link toc-highlight">Lesson 3: IMU and Proprioception (Questions 9-11)</a></li><li><a href="#lesson-4-sensor-fusion-questions-12-13" class="table-of-contents__link toc-highlight">Lesson 4: Sensor Fusion (Questions 12-13)</a></li><li><a href="#integration-questions-questions-14-15" class="table-of-contents__link toc-highlight">Integration Questions (Questions 14-15)</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Learn</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/">Textbook</a></li><li class="footer__item"><a class="footer__link-item" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/ros2-nervous-system/">Module 1: ROS2</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/anusbutt/hackathon-phase-01" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://linkedin.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">X (Twitter)<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://instagram.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://panaversity.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Panaversity<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2026 Physical AI & Humanoid Robotics Textbook. Built for Panaversity Hackathon.</div></div></div></footer><button class="floatingChatButton__Uyc" aria-label="Open AI Assistant" title="Ask questions about Physical AI &amp; Humanoid Robotics"><span class="chatIcon_GGtK">ðŸ’¬</span><span class="chatLabel_InO3">AI Assistant</span></button></div>
</body>
</html>