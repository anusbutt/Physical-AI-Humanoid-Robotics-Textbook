"use strict";(globalThis.webpackChunkbook_source=globalThis.webpackChunkbook_source||[]).push([[5754],{8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>a});var i=s(6540);const o={},r=i.createContext(o);function t(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),i.createElement(r.Provider,{value:n},e.children)}},9164:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Physical-AI-Humanoid-Robotics/sensors-perception/quiz","title":"Module 2 Quiz: Sensors and Perception for Humanoid Robots","description":"Assessment quiz covering camera systems, depth sensing, IMU sensors, and sensor fusion for humanoid robots. 15 questions testing comprehension across all four lessons. Passing score: 12/18 points (67%).","source":"@site/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/06-quiz.md","sourceDirName":"13-Physical-AI-Humanoid-Robotics/02-sensors-perception","slug":"/Physical-AI-Humanoid-Robotics/sensors-perception/quiz","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/quiz","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/06-quiz.md","tags":[{"inline":true,"label":"quiz","permalink":"/hackathon-phase-01/docs/tags/quiz"},{"inline":true,"label":"assessment","permalink":"/hackathon-phase-01/docs/tags/assessment"},{"inline":true,"label":"sensors","permalink":"/hackathon-phase-01/docs/tags/sensors"},{"inline":true,"label":"camera","permalink":"/hackathon-phase-01/docs/tags/camera"},{"inline":true,"label":"lidar","permalink":"/hackathon-phase-01/docs/tags/lidar"},{"inline":true,"label":"imu","permalink":"/hackathon-phase-01/docs/tags/imu"},{"inline":true,"label":"sensor-fusion","permalink":"/hackathon-phase-01/docs/tags/sensor-fusion"},{"inline":true,"label":"evaluation","permalink":"/hackathon-phase-01/docs/tags/evaluation"}],"version":"current","sidebarPosition":6,"frontMatter":{"id":"quiz","title":"Module 2 Quiz: Sensors and Perception for Humanoid Robots","sidebar_position":6,"description":"Assessment quiz covering camera systems, depth sensing, IMU sensors, and sensor fusion for humanoid robots. 15 questions testing comprehension across all four lessons. Passing score: 12/18 points (67%).","time_estimate":"45 minutes","difficulty_level":"beginner-intermediate","prerequisites":["01-camera-systems","02-depth-sensing","03-imu-proprioception","04-sensor-fusion"],"related_lessons":["01-camera-systems","02-depth-sensing","03-imu-proprioception","04-sensor-fusion","05-capstone-project"],"assessment_method":"quiz","skills":[{"name":"Sensor Comprehension","proficiency_level":"beginner-intermediate","category":"knowledge-assessment","bloom_level":"understand","digcomp_area":"technical-concepts","measurable_at_this_level":"demonstrate understanding of sensor concepts through quiz performance (12/18 points minimum)"},{"name":"Code Reading (sensor_msgs)","proficiency_level":"beginner","category":"programming","bloom_level":"understand","digcomp_area":"programming","measurable_at_this_level":"read and explain Python code for sensor_msgs integration"}],"learning_objectives":[{"objective":"Assess understanding of camera systems, types, and ROS2 integration","proficiency_level":"beginner","bloom_level":"understand","assessment_method":"questions 1-4 (Lesson 1)"},{"objective":"Assess understanding of depth sensing technologies (LiDAR, depth cameras) and ROS2 message formats","proficiency_level":"beginner","bloom_level":"apply","assessment_method":"questions 5-8 (Lesson 2)"},{"objective":"Assess understanding of IMU sensors and proprioception for balance control","proficiency_level":"beginner","bloom_level":"understand","assessment_method":"questions 9-11 (Lesson 3)"},{"objective":"Assess understanding of sensor fusion principles and multi-sensor integration","proficiency_level":"intermediate","bloom_level":"apply","assessment_method":"questions 12-13 (Lesson 4)"},{"objective":"Assess ability to integrate concepts across multiple lessons","proficiency_level":"intermediate","bloom_level":"apply","assessment_method":"questions 14-15 (integration)"}],"cognitive_load":{"new_concepts":0,"assessment":"assessment tool - no new concepts introduced, evaluates retention and comprehension from Lessons 1-4"},"differentiation":{"extension_for_advanced":"Challenge: Score 16+/18 points (89%+). Then complete hands-on implementation of sensor fusion algorithms using actual ROS2 sensor data.","remedial_for_struggling":"If scoring below 12/18, review specific lessons indicated in study guide, focus on one lesson at a time, then retake quiz."},"tags":["quiz","assessment","sensors","camera","lidar","imu","sensor-fusion","evaluation"],"generated_by":"manual","created":"2025-12-13","last_modified":"2025-12-13","ros2_version":"humble"},"sidebar":"tutorialSidebar","previous":{"title":"Capstone Summary","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.summary"},"next":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/isaac-ai-brain/"}}');var o=s(4848),r=s(8453);const t={id:"quiz",title:"Module 2 Quiz: Sensors and Perception for Humanoid Robots",sidebar_position:6,description:"Assessment quiz covering camera systems, depth sensing, IMU sensors, and sensor fusion for humanoid robots. 15 questions testing comprehension across all four lessons. Passing score: 12/18 points (67%).",time_estimate:"45 minutes",difficulty_level:"beginner-intermediate",prerequisites:["01-camera-systems","02-depth-sensing","03-imu-proprioception","04-sensor-fusion"],related_lessons:["01-camera-systems","02-depth-sensing","03-imu-proprioception","04-sensor-fusion","05-capstone-project"],assessment_method:"quiz",skills:[{name:"Sensor Comprehension",proficiency_level:"beginner-intermediate",category:"knowledge-assessment",bloom_level:"understand",digcomp_area:"technical-concepts",measurable_at_this_level:"demonstrate understanding of sensor concepts through quiz performance (12/18 points minimum)"},{name:"Code Reading (sensor_msgs)",proficiency_level:"beginner",category:"programming",bloom_level:"understand",digcomp_area:"programming",measurable_at_this_level:"read and explain Python code for sensor_msgs integration"}],learning_objectives:[{objective:"Assess understanding of camera systems, types, and ROS2 integration",proficiency_level:"beginner",bloom_level:"understand",assessment_method:"questions 1-4 (Lesson 1)"},{objective:"Assess understanding of depth sensing technologies (LiDAR, depth cameras) and ROS2 message formats",proficiency_level:"beginner",bloom_level:"apply",assessment_method:"questions 5-8 (Lesson 2)"},{objective:"Assess understanding of IMU sensors and proprioception for balance control",proficiency_level:"beginner",bloom_level:"understand",assessment_method:"questions 9-11 (Lesson 3)"},{objective:"Assess understanding of sensor fusion principles and multi-sensor integration",proficiency_level:"intermediate",bloom_level:"apply",assessment_method:"questions 12-13 (Lesson 4)"},{objective:"Assess ability to integrate concepts across multiple lessons",proficiency_level:"intermediate",bloom_level:"apply",assessment_method:"questions 14-15 (integration)"}],cognitive_load:{new_concepts:0,assessment:"assessment tool - no new concepts introduced, evaluates retention and comprehension from Lessons 1-4"},differentiation:{extension_for_advanced:"Challenge: Score 16+/18 points (89%+). Then complete hands-on implementation of sensor fusion algorithms using actual ROS2 sensor data.",remedial_for_struggling:"If scoring below 12/18, review specific lessons indicated in study guide, focus on one lesson at a time, then retake quiz."},tags:["quiz","assessment","sensors","camera","lidar","imu","sensor-fusion","evaluation"],generated_by:"manual",created:"2025-12-13",last_modified:"2025-12-13",ros2_version:"humble"},a="Module 2 Quiz: Sensors and Perception for Humanoid Robots",l={},c=[{value:"Instructions",id:"instructions",level:2},{value:"Questions",id:"questions",level:2},{value:"Lesson 1: Camera Systems (Questions 1-4)",id:"lesson-1-camera-systems-questions-1-4",level:3},{value:"Lesson 2: Depth Sensing (Questions 5-8)",id:"lesson-2-depth-sensing-questions-5-8",level:3},{value:"Lesson 3: IMU and Proprioception (Questions 9-11)",id:"lesson-3-imu-and-proprioception-questions-9-11",level:3},{value:"Lesson 4: Sensor Fusion (Questions 12-13)",id:"lesson-4-sensor-fusion-questions-12-13",level:3},{value:"Integration Questions (Questions 14-15)",id:"integration-questions-questions-14-15",level:3}];function d(e){const n={a:"a",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Details:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-2-quiz-sensors-and-perception-for-humanoid-robots",children:"Module 2 Quiz: Sensors and Perception for Humanoid Robots"})}),"\n",(0,o.jsx)(n.p,{children:"Test your understanding of camera systems, depth sensing, IMU sensors, and sensor fusion techniques for humanoid robots."}),"\n",(0,o.jsx)(n.h2,{id:"instructions",children:"Instructions"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Total Questions"}),": 15"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Passing Score"}),": 12/15 correct (80%)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Time Limit"}),": 45 minutes (recommended)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Question Types"}),": Multiple choice, short answer, code reading"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resources"}),": You may refer to lesson notes during the quiz"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Scoring"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Multiple choice: 1 point each"}),"\n",(0,o.jsx)(n.li,{children:"Short answer: 1-2 points each (partial credit available)"}),"\n",(0,o.jsx)(n.li,{children:"Code reading: 2 points each (with rubric)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Need to review?"})," Questions reference specific lessons. If you're unsure, revisit:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Questions 1-4: ",(0,o.jsx)(n.a,{href:"./camera-systems",children:"Lesson 1 - Camera Systems"})]}),"\n",(0,o.jsxs)(n.li,{children:["Questions 5-8: ",(0,o.jsx)(n.a,{href:"./depth-sensing",children:"Lesson 2 - Depth Sensing"})]}),"\n",(0,o.jsxs)(n.li,{children:["Questions 9-11: ",(0,o.jsx)(n.a,{href:"./imu-proprioception",children:"Lesson 3 - IMU & Proprioception"})]}),"\n",(0,o.jsxs)(n.li,{children:["Questions 12-13: ",(0,o.jsx)(n.a,{href:"./sensor-fusion",children:"Lesson 4 - Sensor Fusion"})]}),"\n",(0,o.jsx)(n.li,{children:"Questions 14-15: Integration across multiple lessons"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"questions",children:"Questions"}),"\n",(0,o.jsx)(n.h3,{id:"lesson-1-camera-systems-questions-1-4",children:"Lesson 1: Camera Systems (Questions 1-4)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 1 (Multiple Choice)"})," - 1 point"]}),"\n",(0,o.jsx)(n.p,{children:"What is the primary difference between monocular, stereo, and RGB-D cameras in terms of depth perception?"}),"\n",(0,o.jsx)(n.p,{children:"A) Monocular cameras can see in 3D, stereo cameras are 2D, RGB-D cameras are color only\r\nB) Monocular cameras provide no depth, stereo cameras estimate depth through triangulation, RGB-D cameras provide direct depth measurements\r\nC) Monocular cameras are for indoor use, stereo cameras for outdoor, RGB-D cameras for underwater\r\nD) Monocular cameras are expensive, stereo cameras are cheap, RGB-D cameras are free"}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer"}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Correct Answer: B"})}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Explanation"}),": Monocular cameras provide only 2D images with no depth information. Stereo cameras use two lenses to capture slightly different views, allowing depth estimation through triangulation. RGB-D cameras (like Kinect) provide both color (RGB) and direct depth measurements (D) from structured light or time-of-flight sensors."]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 01-camera-systems.md (Camera Types for Humanoid Robots)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 2 (Multiple Choice)"})," - 1 point"]}),"\n",(0,o.jsx)(n.p,{children:"Which ROS2 message type is used for standard camera image data?"}),"\n",(0,o.jsx)(n.p,{children:"A) sensor_msgs/Image\r\nB) sensor_msgs/Camera\r\nC) sensor_msgs/Video\r\nD) sensor_msgs/CameraImage"}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer"}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Correct Answer: A"})}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Explanation"}),": The sensor_msgs/Image message type is the standard ROS2 message for camera image data. It contains the raw pixel data along with metadata like encoding, height, width, and step size."]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 01-camera-systems.md (ROS2 Integration - sensor_msgs/Image)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 3 (Short Answer)"})," - 2 points"]}),"\n",(0,o.jsx)(n.p,{children:"Explain the trade-offs between placing cameras on a humanoid robot's head, chest, and wrists. For each location, describe one advantage and one disadvantage for humanoid robot tasks."}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer & Rubric"}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Sample Answer"}),":"]}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Head"}),": Advantage - matches human eye perspective for natural interaction and navigation. Disadvantage - limited view of hands during manipulation tasks."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Chest"}),": Advantage - good view of ground and obstacles for navigation. Disadvantage - limited view of upper environment and human faces during interaction."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Wrists"}),": Advantage - provides visual feedback during manipulation tasks. Disadvantage - limited field of view and constantly moving with arm motion."]}),"\n"]}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Grading Rubric (2 points total)"}),":"]}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"2 points"}),": Correctly identifies advantages and disadvantages for all three locations with specific robot tasks"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"1 point"}),": Correctly identifies advantages/disadvantages for 1-2 locations OR has partial understanding"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"0 points"}),": Incorrect or irrelevant responses"]}),"\n"]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 01-camera-systems.md (Camera Placement Strategies)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 4 (Short Answer)"})," - 1 point"]}),"\n",(0,o.jsx)(n.p,{children:'What does the "field of view" (FOV) parameter indicate for a camera, and why is it important for humanoid robot navigation?'}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer & Rubric"}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Sample Answer"}),": Field of view indicates the angular extent of the scene that a camera can capture. For humanoid robots, FOV is important because it determines how much of the environment the robot can see at once, affecting obstacle detection, navigation safety, and situational awareness."]}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Grading Rubric (1 point)"}),":"]}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"1 point"}),": Correctly explains FOV as angular extent AND relates to robot navigation/environment awareness"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"0 points"}),": Incorrect or incomplete explanation"]}),"\n"]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 01-camera-systems.md (Camera Parameters - Field of View)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"lesson-2-depth-sensing-questions-5-8",children:"Lesson 2: Depth Sensing (Questions 5-8)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 5 (Multiple Choice)"})," - 1 point"]}),"\n",(0,o.jsx)(n.p,{children:"How does 2D LiDAR differ from 3D LiDAR in terms of environmental perception for humanoid robots?"}),"\n",(0,o.jsx)(n.p,{children:"A) 2D LiDAR is for indoor use, 3D LiDAR is for outdoor use\r\nB) 2D LiDAR measures distance in one plane (typically ground level), 3D LiDAR measures in multiple planes (volume)\r\nC) 2D LiDAR is faster, 3D LiDAR is more accurate\r\nD) 2D LiDAR is expensive, 3D LiDAR is cheap"}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer"}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Correct Answer: B"})}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Explanation"}),": 2D LiDAR typically scans in a single horizontal plane and provides distance measurements in that plane only. 3D LiDAR scans in multiple planes to create a 3D point cloud, providing full volumetric information about the environment including height and depth."]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 02-depth-sensing.md (LiDAR Technologies - 2D vs 3D)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 6 (Multiple Choice)"})," - 1 point"]}),"\n",(0,o.jsx)(n.p,{children:"Which ROS2 message type is used for 3D point cloud data from depth sensors?"}),"\n",(0,o.jsx)(n.p,{children:"A) sensor_msgs/PointCloud\r\nB) sensor_msgs/Point3D\r\nC) sensor_msgs/PointCloud2\r\nD) sensor_msgs/LaserScan3D"}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer"}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Correct Answer: C"})}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Explanation"}),": sensor_msgs/PointCloud2 is the standard ROS2 message for 3D point cloud data. It contains structured binary data representing 3D coordinates of points in space, along with optional color and other per-point information."]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 02-depth-sensing.md (ROS2 Integration - sensor_msgs/PointCloud2)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 7 (Short Answer)"})," - 2 points"]}),"\n",(0,o.jsx)(n.p,{children:"Compare the advantages and disadvantages of LiDAR vs stereo vision for humanoid robot navigation. Provide one advantage and one disadvantage for each technology."}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer & Rubric"}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Sample Answer"}),":"]}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LiDAR Advantages"}),": Precise distance measurements, works in darkness, reliable in various lighting conditions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"LiDAR Disadvantages"}),": Cannot detect transparent surfaces, higher cost, sensitive to rain/fog"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Stereo Vision Advantages"}),": Provides color information, lower cost, works in most environments"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Stereo Vision Disadvantages"}),": Sensitive to lighting conditions, less accurate at longer distances, computationally intensive"]}),"\n"]}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Grading Rubric (2 points total)"}),":"]}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"2 points"}),": Correctly identifies one advantage and one disadvantage for both technologies"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"1 point"}),": Correctly identifies advantage/disadvantage for one technology OR partial understanding of both"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"0 points"}),": Incorrect or irrelevant responses"]}),"\n"]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 02-depth-sensing.md (Depth Technology Comparison)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 8 (Short Answer)"})," - 1 point"]}),"\n",(0,o.jsx)(n.p,{children:'What is a "point cloud" in the context of depth sensing, and how does it differ from a regular 2D image?'}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer & Rubric"}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Sample Answer"}),": A point cloud is a collection of 3D points in space, where each point has X, Y, and Z coordinates representing the 3D structure of objects in the environment. Unlike a 2D image which only has X, Y coordinates with color/intensity values, a point cloud provides full 3D spatial information about the scene."]}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Grading Rubric (1 point)"}),":"]}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"1 point"}),": Correctly explains point cloud as 3D points with X, Y, Z coordinates AND differentiates from 2D images"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"0 points"}),": Incorrect or incomplete explanation"]}),"\n"]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 02-depth-sensing.md (Point Cloud Fundamentals)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"lesson-3-imu-and-proprioception-questions-9-11",children:"Lesson 3: IMU and Proprioception (Questions 9-11)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 9 (Multiple Choice)"})," - 1 point"]}),"\n",(0,o.jsx)(n.p,{children:"Which three sensors are typically combined in an IMU for humanoid robot balance?"}),"\n",(0,o.jsx)(n.p,{children:"A) Camera, gyroscope, magnetometer\r\nB) Accelerometer, gyroscope, magnetometer\r\nC) LiDAR, accelerometer, gyroscope\r\nD) GPS, magnetometer, camera"}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer"}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Correct Answer: B"})}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Explanation"}),": An IMU (Inertial Measurement Unit) combines three sensor types: accelerometer (measures linear acceleration and gravity), gyroscope (measures angular velocity/rotation rates), and magnetometer (measures magnetic field for heading). Together they provide comprehensive motion and orientation sensing."]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 03-imu-proprioception.md (What Is an IMU?)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 10 (Multiple Choice)"})," - 1 point"]}),"\n",(0,o.jsx)(n.p,{children:"What does a gyroscope in an IMU measure for humanoid robot balance?"}),"\n",(0,o.jsx)(n.p,{children:"A) Linear acceleration and gravity\r\nB) Angular velocity (rate of rotation)\r\nC) Magnetic field direction\r\nD) Distance to obstacles"}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer"}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Correct Answer: B"})}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Explanation"}),": A gyroscope measures angular velocity - the rate at which the robot is rotating around any axis. This is crucial for balance control as it allows the robot to detect when it's starting to tip or rotate unexpectedly."]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 03-imu-proprioception.md (Gyroscope Function)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 11 (Short Answer)"})," - 2 points"]}),"\n",(0,o.jsx)(n.p,{children:"Explain how IMU data can be used to detect if a humanoid robot is falling. Describe the sensor readings that would indicate a fall is occurring."}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer & Rubric"}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Sample Answer"}),": A humanoid robot falling would show characteristic IMU readings: (1) Accelerometer readings would show near-zero linear acceleration during freefall (as the robot and sensors accelerate together at gravity), followed by high acceleration during impact. (2) Gyroscope readings might show rapid, uncontrolled rotation during the fall. (3) The combination of these readings can be processed by fall detection algorithms that look for acceleration magnitude thresholds and rotation patterns that indicate loss of balance."]}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Grading Rubric (2 points total)"}),":"]}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"2 points"}),": Correctly explains freefall acceleration near zero AND impact acceleration AND mentions gyroscope rotation patterns"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"1 point"}),": Correctly explains one or two aspects of fall detection"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"0 points"}),": Incorrect or irrelevant explanation"]}),"\n"]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 03-imu-proprioception.md (Fall Detection with IMU)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"lesson-4-sensor-fusion-questions-12-13",children:"Lesson 4: Sensor Fusion (Questions 12-13)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 12 (Multiple Choice)"})," - 1 point"]}),"\n",(0,o.jsx)(n.p,{children:"Why is sensor fusion essential for humanoid robot perception?"}),"\n",(0,o.jsx)(n.p,{children:"A) It reduces the cost of individual sensors\r\nB) It combines multiple sensors to overcome individual limitations and create more robust perception\r\nC) It increases the size of the robot\r\nD) It eliminates the need for any single sensor"}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer"}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Correct Answer: B"})}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Explanation"}),": Sensor fusion combines data from multiple sensors to create more accurate, reliable, and complete information than any single sensor could provide. It allows robots to overcome limitations (e.g., cameras fail in darkness, LiDAR struggles with transparent surfaces) by intelligently integrating complementary strengths."]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 04-sensor-fusion.md (What Is Sensor Fusion?)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 13 (Multiple Choice)"})," - 1 point"]}),"\n",(0,o.jsx)(n.p,{children:"What does a complementary filter do in sensor fusion?"}),"\n",(0,o.jsx)(n.p,{children:"A) It eliminates all sensor data\r\nB) It combines gyroscope and accelerometer data with configurable time constants to provide drift-corrected orientation\r\nC) It only uses camera data\r\nD) It doubles the sensor data"}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer"}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Correct Answer: B"})}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Explanation"}),": A complementary filter combines gyroscope data (good for short-term rotation tracking) with accelerometer data (good for long-term orientation reference) using configurable time constants. This provides drift-corrected orientation estimation by leveraging the strengths of each sensor type."]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 04-sensor-fusion.md (Complementary Filtering)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"integration-questions-questions-14-15",children:"Integration Questions (Questions 14-15)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 14 (Short Answer)"})," - 2 points"]}),"\n",(0,o.jsx)(n.p,{children:"Design a minimal multi-sensor perception system for a humanoid robot that needs to navigate indoors, avoid obstacles, and maintain balance. Identify which sensors from Module 2 would be essential and explain why each is necessary."}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer & Rubric"}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Sample Answer"}),": Essential sensors would include: (1) Stereo cameras for navigation scene understanding and human interaction, (2) 3D LiDAR for precise obstacle detection and mapping (works in darkness), (3) IMU for balance control and orientation estimation. Cameras provide visual context, LiDAR provides reliable distance measurements for navigation, and IMU provides internal motion sensing for balance. The fusion of all three creates robust perception that works in various lighting conditions and provides both external awareness and internal state knowledge."]}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Grading Rubric (2 points total)"}),":"]}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"2 points"}),": Identifies at least 2-3 appropriate sensors AND explains specific reasons for each in the context of the task"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"1 point"}),": Identifies sensors but with limited explanation OR identifies only 1-2 sensors with good explanation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"0 points"}),": Incorrect sensor choices or irrelevant explanations"]}),"\n"]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 04-sensor-fusion.md (Multi-Sensor Integration)"})})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Question 15 (Short Answer)"})," - 2 points"]}),"\n",(0,o.jsx)(n.p,{children:"Explain how a humanoid robot would use sensor fusion to navigate safely when transitioning from a well-lit indoor area to a dark hallway. What sensors would become more or less important in each environment?"}),"\n",(0,o.jsxs)(s,{children:[(0,o.jsx)("summary",{children:"Reveal Answer & Rubric"}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Sample Answer"}),": In the well-lit area, cameras would provide excellent navigation information for obstacle detection and path planning. As the robot enters darkness, camera effectiveness decreases significantly. The fusion system would automatically reduce weighting of camera data while increasing reliance on LiDAR and depth sensors, which work independently of lighting. The IMU would continue providing balance and orientation information throughout the transition. The robot's navigation system would adaptively combine available sensor data based on confidence levels, maintaining safe navigation throughout the lighting transition."]}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Grading Rubric (2 points total)"}),":"]}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"2 points"}),": Correctly explains sensor adaptation during lighting transition AND identifies which sensors become more/less important"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"1 point"}),": Explains some aspects of sensor adaptation OR identifies some sensor changes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"0 points"}),": Incorrect or irrelevant explanation"]}),"\n"]}),(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Lesson Reference: 04-sensor-fusion.md (Adaptive Sensor Weighting)"})})]}),"\n",(0,o.jsx)(n.hr,{})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);