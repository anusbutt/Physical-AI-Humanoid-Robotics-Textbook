"use strict";(globalThis.webpackChunkbook_source=globalThis.webpackChunkbook_source||[]).push([[5396],{2772:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Physical-AI-Humanoid-Robotics/vision-language-action/cognitive-planning.summary","title":"Summary - Lesson 2 - Cognitive Planning with LLMs","description":"Module: Module 4 - Vision-Language-Action (VLA)","source":"@site/docs/13-Physical-AI-Humanoid-Robotics/04-vision-language-action/02-cognitive-planning.summary.md","sourceDirName":"13-Physical-AI-Humanoid-Robotics/04-vision-language-action","slug":"/Physical-AI-Humanoid-Robotics/vision-language-action/cognitive-planning.summary","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/cognitive-planning.summary","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/04-vision-language-action/02-cognitive-planning.summary.md","tags":[{"inline":true,"label":"llm","permalink":"/hackathon-phase-01/docs/tags/llm"},{"inline":true,"label":"cognitive-planning","permalink":"/hackathon-phase-01/docs/tags/cognitive-planning"},{"inline":true,"label":"task-decomposition","permalink":"/hackathon-phase-01/docs/tags/task-decomposition"},{"inline":true,"label":"robotics","permalink":"/hackathon-phase-01/docs/tags/robotics"},{"inline":true,"label":"natural-language","permalink":"/hackathon-phase-01/docs/tags/natural-language"}],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Summary - Lesson 2 - Cognitive Planning with LLMs","sidebar_position":2,"skills":["Large Language Models","Task Decomposition","Natural Language Understanding","Prompt Engineering","ROS2 Action Planning"],"learning_objectives":["Understand how LLMs decompose high-level natural language commands into action sequences","Learn effective prompting strategies for robotics task planning","Implement task decomposition algorithms for natural language to ROS2 actions","Handle ambiguous or underspecified commands with LLMs","Design error recovery strategies for LLM-generated action sequences"],"cognitive_load":6,"differentiation":"AI Colearning, Expert Insight, Practice Exercise","tags":["llm","cognitive-planning","task-decomposition","robotics","natural-language"],"created":"2025-12-23","last_modified":"2025-12-23","ros2_version":"humble"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2 - Cognitive Planning with LLMs","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/cognitive-planning"},"next":{"title":"Lesson 3 - Vision-Language Integration","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/vision-language-integration"}}');var t=i(4848),a=i(8453);const o={title:"Summary - Lesson 2 - Cognitive Planning with LLMs",sidebar_position:2,skills:["Large Language Models","Task Decomposition","Natural Language Understanding","Prompt Engineering","ROS2 Action Planning"],learning_objectives:["Understand how LLMs decompose high-level natural language commands into action sequences","Learn effective prompting strategies for robotics task planning","Implement task decomposition algorithms for natural language to ROS2 actions","Handle ambiguous or underspecified commands with LLMs","Design error recovery strategies for LLM-generated action sequences"],cognitive_load:6,differentiation:"AI Colearning, Expert Insight, Practice Exercise",tags:["llm","cognitive-planning","task-decomposition","robotics","natural-language"],created:"2025-12-23",last_modified:"2025-12-23",ros2_version:"humble"},r="Summary: Lesson 2 - Cognitive Planning with LLMs",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Key Concepts Covered",id:"key-concepts-covered",level:2},{value:"Cognitive Planning Architecture",id:"cognitive-planning-architecture",level:3},{value:"LLM Prompting Strategies",id:"llm-prompting-strategies",level:3},{value:"Task Decomposition Methods",id:"task-decomposition-methods",level:3},{value:"Grounding in Physical Reality",id:"grounding-in-physical-reality",level:3},{value:"Error Handling &amp; Recovery",id:"error-handling--recovery",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"\ud83d\udcac AI Colearning Prompt",id:"-ai-colearning-prompt",level:2},{value:"\ud83c\udf93 Expert Insight",id:"-expert-insight",level:2},{value:"\ud83e\udd1d Practice Exercise",id:"-practice-exercise",level:2},{value:"Example Application",id:"example-application",level:3},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Technical Corrections Applied",id:"technical-corrections-applied",level:2},{value:"\u2705 Module Completion Checklist",id:"-module-completion-checklist",level:2}];function d(n){const e={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"summary-lesson-2---cognitive-planning-with-llms",children:"Summary: Lesson 2 - Cognitive Planning with LLMs"})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Module"}),": Module 4 - Vision-Language-Action (VLA)\r\n",(0,t.jsx)(e.strong,{children:"Lesson"}),": 02-cognitive-planning.md\r\n",(0,t.jsx)(e.strong,{children:"Target Audience"}),": CS students with Python + Modules 1-3 (ROS2, Sensors, Isaac) knowledge\r\n",(0,t.jsx)(e.strong,{children:"Estimated Time"}),": 45-55 minutes\r\n",(0,t.jsx)(e.strong,{children:"Difficulty"}),": Intermediate"]}),"\n",(0,t.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this lesson, students will be able to:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Understand"})," how LLMs decompose high-level natural language commands into action sequences, including the cognitive planning process"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Apply"})," effective prompting strategies for robotics task planning, including Chain-of-Thought techniques"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Analyze"})," the challenges of grounding language in physical reality for robotic execution"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Evaluate"})," task decomposition hierarchies and their mapping to ROS2 action sequences"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Create"})," error recovery strategies for LLM-generated action sequences in robotics"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"key-concepts-covered",children:"Key Concepts Covered"}),"\n",(0,t.jsx)(e.h3,{id:"cognitive-planning-architecture",children:"Cognitive Planning Architecture"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Natural Language Understanding"}),": Interpreting high-level human commands and intentions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task Decomposition"}),": Breaking complex goals into hierarchical subtasks"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"World Modeling"}),": Maintaining context about environment and object states"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action Mapping"}),": Connecting abstract concepts to concrete ROS2 actions"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"llm-prompting-strategies",children:"LLM Prompting Strategies"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Chain-of-Thought (CoT)"}),": Step-by-step reasoning for reliable planning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Few-Shot Examples"}),": Providing examples to guide LLM behavior"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"System Context"}),": Providing robot capabilities and constraints"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Output Formatting"}),": Structuring LLM responses for robotic execution"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"task-decomposition-methods",children:"Task Decomposition Methods"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hierarchical Planning"}),": Breaking complex tasks into manageable subtasks"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Spatial Reasoning"}),": Understanding object relationships and locations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Constraint Handling"}),": Respecting robot capabilities and environmental constraints"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sequential vs Parallel"}),": Determining which tasks can be executed simultaneously"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"grounding-in-physical-reality",children:"Grounding in Physical Reality"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robot Capabilities"}),": Understanding payload limits, reach constraints, mobility"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Object Affordances"}),": What can be done with different types of objects"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Environmental Constraints"}),": Navigable spaces, safety considerations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Validation Mechanisms"}),": Checking LLM plans for feasibility"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"error-handling--recovery",children:"Error Handling & Recovery"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Plan Validation"}),": Checking LLM outputs before execution"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Clarification Requests"}),": Asking for clarification when commands are ambiguous"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Fallback Strategies"}),": Handling plan failures gracefully"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Human Intervention"}),": When to request human assistance"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Cognitive Planning Bridges Intent and Action"}),": LLMs translate high-level human goals into executable robotic behaviors, enabling natural human-robot interaction."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Chain-of-Thought Improves Reliability"}),": Step-by-step reasoning prompts lead to more reliable and interpretable planning than direct command translation."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Grounding is Critical"}),": LLM plans must be validated against robot capabilities and physical constraints to ensure feasibility."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Hierarchical Decomposition is Essential"}),": Complex tasks must be broken down into manageable subtasks that map to primitive robot actions."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Error Handling is Mandatory"}),": Robust systems include validation, clarification, and recovery mechanisms for LLM-generated plans."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"-ai-colearning-prompt",children:"\ud83d\udcac AI Colearning Prompt"}),"\n",(0,t.jsx)(e.p,{children:'Ask Claude to demonstrate how "Clean the room" gets decomposed into robotic actions, considering intermediate reasoning steps important for cognitive planning.'}),"\n",(0,t.jsx)(e.h2,{id:"-expert-insight",children:"\ud83c\udf93 Expert Insight"}),"\n",(0,t.jsx)(e.p,{children:"LLMs have limitations in robotics applications including hallucinations, spatial reasoning challenges, and probabilistic outputs. Robust systems must include validation mechanisms to catch these issues before execution."}),"\n",(0,t.jsx)(e.h2,{id:"-practice-exercise",children:"\ud83e\udd1d Practice Exercise"}),"\n",(0,t.jsx)(e.p,{children:'Analyze "Set up for a meeting" command and break it into ROS2 action sequences, considering objects needed, their locations, setup order, and clarifying questions.'}),"\n",(0,t.jsx)(e.h3,{id:"example-application",children:"Example Application"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Scenario"}),': Robot receives "Clean the living room"']}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"LLM decomposes into hierarchical plan: navigate \u2192 survey \u2192 categorize objects \u2192 execute cleaning actions"}),"\n",(0,t.jsx)(e.li,{children:"Each step maps to specific ROS2 action servers (navigation, perception, manipulation)"}),"\n",(0,t.jsx)(e.li,{children:"System validates plans against robot capabilities before execution"}),"\n",(0,t.jsx)(e.li,{children:"Error handling manages unexpected obstacles or failures"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,t.jsx)(e.p,{children:"Students demonstrate mastery when they can:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Explain the cognitive planning pipeline from natural language to ROS2 actions (LLM \u2192 decomposition \u2192 mapping \u2192 execution)"}),"\n",(0,t.jsx)(e.li,{children:"Design effective prompting strategies for robotics applications with appropriate Chain-of-Thought elements"}),"\n",(0,t.jsx)(e.li,{children:"Implement task decomposition hierarchies that map effectively to robot capabilities"}),"\n",(0,t.jsx)(e.li,{children:"Analyze the challenges of grounding LLM outputs in physical reality"}),"\n",(0,t.jsx)(e.li,{children:"Design error handling and recovery strategies for LLM-generated action sequences"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate the feasibility of LLM plans before execution"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"technical-corrections-applied",children:"Technical Corrections Applied"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Prompting Strategy Clarity"})," (Line 45): Added detailed explanation of Chain-of-Thought prompting and its benefits for robotics"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Grounding Emphasis"})," (Lines 60, 85): Clarified the critical importance of validating LLM outputs against physical constraints"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Error Handling Integration"})," (Line 70): Emphasized the necessity of validation mechanisms in cognitive planning systems"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Practical Examples"}),": Added detailed living room cleaning scenario to illustrate complete cognitive planning operation"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"-module-completion-checklist",children:"\u2705 Module Completion Checklist"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"\u2705 Lesson Content: Complete with 7-section structure (What Is \u2192 Why Matters \u2192 Key Principles \u2192 Practical Example \u2192 Summary \u2192 Next Steps)"}),"\n",(0,t.jsx)(e.li,{children:"\u2705 Frontmatter: 13 fields properly configured"}),"\n",(0,t.jsx)(e.li,{children:"\u2705 Callouts: 1 AI Colearning, 1 Expert Insight, 1 Practice Exercise"}),"\n",(0,t.jsx)(e.li,{children:"\u2705 Summary: Paired .summary.md file created"}),"\n",(0,t.jsx)(e.li,{children:"\u2705 Technical Accuracy: Validated for robotics applications"}),"\n",(0,t.jsx)(e.li,{children:"\u2705 Differentiation: Appropriate for CS students with Modules 1-3 knowledge"}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>r});var s=i(6540);const t={},a=s.createContext(t);function o(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),s.createElement(a.Provider,{value:e},n.children)}}}]);