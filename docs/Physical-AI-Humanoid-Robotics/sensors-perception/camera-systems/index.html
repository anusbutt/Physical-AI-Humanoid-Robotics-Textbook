<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Lesson 1: Camera Systems and Computer Vision | Physical AI &amp; Humanoid Robotics Textbook</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://anusbutt.github.io/hackathon-phase-01/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://anusbutt.github.io/hackathon-phase-01/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Lesson 1: Camera Systems and Computer Vision | Physical AI &amp; Humanoid Robotics Textbook"><meta data-rh="true" name="description" content="What Is a Camera in Robotics?"><meta data-rh="true" property="og:description" content="What Is a Camera in Robotics?"><link data-rh="true" rel="icon" href="/hackathon-phase-01/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems"><link data-rh="true" rel="alternate" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems" hreflang="en"><link data-rh="true" rel="alternate" href="https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Physical AI & Humanoid Robotics","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"},{"@type":"ListItem","position":2,"name":"Module 2: Sensors and Perception for Humanoid Robots","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"},{"@type":"ListItem","position":3,"name":"Lesson 1: Camera Systems and Computer Vision","item":"https://anusbutt.github.io/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems"}]}</script><link rel="alternate" type="application/rss+xml" href="/hackathon-phase-01/blog/rss.xml" title="Physical AI &amp; Humanoid Robotics Textbook RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/hackathon-phase-01/blog/atom.xml" title="Physical AI &amp; Humanoid Robotics Textbook Atom Feed"><link rel="stylesheet" href="/hackathon-phase-01/assets/css/styles.5903934b.css">
<script src="/hackathon-phase-01/assets/js/runtime~main.94161a1c.js" defer="defer"></script>
<script src="/hackathon-phase-01/assets/js/main.ab534ac3.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/hackathon-phase-01/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/hackathon-phase-01/"><div class="navbar__logo"><img src="/hackathon-phase-01/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/hackathon-phase-01/img/logo.svg" alt="Physical AI Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics Textbook</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/">Textbook</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/anusbutt/hackathon-phase-01" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"><span title="Physical AI &amp; Humanoid Robotics" class="categoryLinkLabel_W154">Physical AI &amp; Humanoid Robotics</span></a><button aria-label="Collapse sidebar category &#x27;Physical AI &amp; Humanoid Robotics&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/ros2-nervous-system/"><span title="Module 1: The Robotic Nervous System (ROS 2)" class="categoryLinkLabel_W154">Module 1: The Robotic Nervous System (ROS 2)</span></a><button aria-label="Expand sidebar category &#x27;Module 1: The Robotic Nervous System (ROS 2)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"><span title="Module 2: Sensors and Perception for Humanoid Robots" class="categoryLinkLabel_W154">Module 2: Sensors and Perception for Humanoid Robots</span></a><button aria-label="Collapse sidebar category &#x27;Module 2: Sensors and Perception for Humanoid Robots&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems"><span title="Lesson 1: Camera Systems and Computer Vision" class="linkLabel_WmDU">Lesson 1: Camera Systems and Computer Vision</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems.summary"><span title="Summary: Lesson 1 - Camera Systems and Computer Vision" class="linkLabel_WmDU">Summary: Lesson 1 - Camera Systems and Computer Vision</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing"><span title="Lesson 2: Depth Sensing Technologies" class="linkLabel_WmDU">Lesson 2: Depth Sensing Technologies</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/depth-sensing.summary"><span title="Summary: Lesson 2 - Depth Sensing Technologies" class="linkLabel_WmDU">Summary: Lesson 2 - Depth Sensing Technologies</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception"><span title="IMU &amp; Proprioception" class="linkLabel_WmDU">IMU &amp; Proprioception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/imu-proprioception.summary"><span title="Summary: Lesson 3 - IMU and Proprioception" class="linkLabel_WmDU">Summary: Lesson 3 - IMU and Proprioception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion"><span title="Sensor Fusion" class="linkLabel_WmDU">Sensor Fusion</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/sensor-fusion.summary"><span title="Lesson 4 Summary: Sensor Fusion Techniques for Humanoid Robots" class="linkLabel_WmDU">Lesson 4 Summary: Sensor Fusion Techniques for Humanoid Robots</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.architecture"><span title="Multi-Sensor Fusion Capstone: Architecture Documentation" class="linkLabel_WmDU">Multi-Sensor Fusion Capstone: Architecture Documentation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.casestudies"><span title="Case Studies: Real-World Multi-Sensor Integration" class="linkLabel_WmDU">Case Studies: Real-World Multi-Sensor Integration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content"><span title="Module 2 Capstone Project: Integrated Sensor System Design" class="linkLabel_WmDU">Module 2 Capstone Project: Integrated Sensor System Design</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project"><span title="Capstone Project: Design a Multi-Sensor Humanoid Perception System" class="linkLabel_WmDU">Capstone Project: Design a Multi-Sensor Humanoid Perception System</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.summary"><span title="Capstone Summary" class="linkLabel_WmDU">Capstone Summary</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/quiz"><span title="Module 2 Quiz: Sensors and Perception for Humanoid Robots" class="linkLabel_WmDU">Module 2 Quiz: Sensors and Perception for Humanoid Robots</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/isaac-ai-brain/"><span title="Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)" class="categoryLinkLabel_W154">Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)</span></a><button aria-label="Expand sidebar category &#x27;Module 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/vision-language-action/"><span title="Module 4: Vision-Language-Action (VLA)" class="categoryLinkLabel_W154">Module 4: Vision-Language-Action (VLA)</span></a><button aria-label="Expand sidebar category &#x27;Module 4: Vision-Language-Action (VLA)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/hackathon-phase-01/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/"><span>Physical AI &amp; Humanoid Robotics</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"><span>Module 2: Sensors and Perception for Humanoid Robots</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Lesson 1: Camera Systems and Computer Vision</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Lesson 1: Camera Systems and Computer Vision</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-a-camera-in-robotics">What Is a Camera in Robotics?<a href="#what-is-a-camera-in-robotics" class="hash-link" aria-label="Direct link to What Is a Camera in Robotics?" title="Direct link to What Is a Camera in Robotics?" translate="no">‚Äã</a></h2>
<p>When you watch a humanoid robot navigate a crowded room, reach for a coffee mug, or recognize a person&#x27;s face, you&#x27;re witnessing the power of camera systems at work. Cameras serve as the primary visual sensors that give robots the ability to perceive their environment, transforming light into digital data that algorithms can interpret. Just as human eyes capture photons and convert them into neural signals, robotic cameras capture images and convert them into pixel arrays that computer vision algorithms process.</p>
<p>In the context of humanoid robotics, a camera is a sensor device that captures visual information from the environment and encodes it as digital image data. This data consists of <strong>pixels</strong> (picture elements), each representing a tiny sample of color or brightness at a specific location in the visual field. The <strong>resolution</strong> of a camera‚Äîtypically expressed as width √ó height in pixels (e.g., 1920√ó1080)‚Äîdetermines how much spatial detail the robot can perceive. The <strong>frame rate</strong>, measured in frames per second (fps), controls how frequently the camera captures new images, with 30 fps being common for general tasks and 60+ fps necessary for fast-moving scenarios like catching objects or maintaining balance during dynamic motion.</p>
<p>Two additional parameters define what a camera can see and how it delivers data. The <strong>field of view</strong> (FoV) specifies the angular extent of the observable world, ranging from narrow (telephoto, ~30¬∞) to wide-angle (fisheye, 180¬∞+). Finally, <strong>image encoding</strong> refers to the format in which pixel data is stored‚Äîcommon encodings include RGB (red-green-blue color), BGR (blue-green-red, used by OpenCV), grayscale (single intensity channel), and specialized formats like floating-point depth maps. Understanding these five foundational concepts‚Äîpixels, resolution, frame rate, field of view, and image encoding‚Äîis essential for designing and programming vision systems for humanoid robots.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="why-camera-systems-matter-for-physical-ai">Why Camera Systems Matter for Physical AI<a href="#why-camera-systems-matter-for-physical-ai" class="hash-link" aria-label="Direct link to Why Camera Systems Matter for Physical AI" title="Direct link to Why Camera Systems Matter for Physical AI" translate="no">‚Äã</a></h2>
<p>Camera systems are the cornerstone of physical AI because they enable robots to perform three critical categories of tasks: manipulation, navigation, and human interaction. For manipulation tasks like grasping a wrench or assembling components, cameras provide the semantic understanding necessary to identify objects, estimate their poses, and guide end-effectors with precision. Navigation systems rely on cameras to detect obstacles, recognize landmarks, and build spatial maps of environments. Human-robot interaction becomes possible when cameras enable facial recognition, gesture detection, and gaze tracking‚Äîallowing humanoids to respond naturally to people in shared spaces.</p>
<p>Consider three leading humanoid platforms that exemplify camera-centric design. <strong>Boston Dynamics&#x27; Atlas</strong> uses multiple cameras in its head and hands to perform parkour, tool manipulation, and terrain assessment‚Äîtasks that would be impossible without real-time visual feedback. <strong>Tesla Optimus</strong>, designed for manufacturing and domestic environments, relies exclusively on camera-based vision (no lidar) to navigate factories and recognize objects, mirroring the computer vision approach used in Tesla&#x27;s autonomous vehicles. <strong>Agility Robotics&#x27; Digit</strong>, built for logistics and delivery, employs stereo cameras to detect stairs, avoid humans, and place packages safely‚Äîall while maintaining balance on two legs.</p>
<p>Without cameras, humanoid robots would be functionally blind. While other sensors like lidar provide accurate distance measurements and force sensors detect contact, cameras uniquely deliver semantic information about the world. A camera can distinguish between a coffee mug and a water bottle, recognize a stop sign, or detect whether a person is waving. This semantic richness comes at a trade-off: cameras provide less accurate depth information compared to active sensors like lidar or time-of-flight (ToF) cameras, and they struggle in low-light conditions where infrared or ultrasonic sensors excel. However, the interpretability of camera data‚Äîcombined with advances in deep learning for computer vision‚Äîmakes cameras indispensable for robots that must operate in human-centric environments.</p>
<p>The integration of cameras with modern physical AI systems creates a feedback loop: cameras capture the world, neural networks interpret the visual data, and robot control systems execute actions based on that understanding. This closed-loop vision-action pipeline is what transforms a mechanical system into an intelligent agent capable of learning from experience and adapting to novel situations.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="-case-study-boston-dynamics-atlas---multi-camera-perception-system">üìä Case Study: Boston Dynamics Atlas - Multi-Camera Perception System<a href="#-case-study-boston-dynamics-atlas---multi-camera-perception-system" class="hash-link" aria-label="Direct link to üìä Case Study: Boston Dynamics Atlas - Multi-Camera Perception System" title="Direct link to üìä Case Study: Boston Dynamics Atlas - Multi-Camera Perception System" translate="no">‚Äã</a></h3>
<p><strong>Robot</strong>: Atlas (Boston Dynamics)
<strong>Camera System</strong>: Stereo cameras in head, RGB cameras in wrists
<strong>Application</strong>: Autonomous parkour, tool manipulation, terrain navigation</p>
<p>Atlas employs a sophisticated multi-camera architecture designed for operation in unstructured environments. The head-mounted stereo camera pair (baseline approximately 10cm) provides depth perception for navigation and obstacle detection at ranges up to 8 meters, enabling Atlas to identify footholds on stairs and detect gaps during parkour maneuvers. The stereo system operates at 30 fps with 752√ó480 resolution per camera, balancing computational load against the need for real-time depth estimation during dynamic locomotion.</p>
<p>Complementing the head cameras, Atlas features wrist-mounted RGB cameras (monocular, ~1280√ó720) positioned to maintain visual contact with manipulation targets even when the robot&#x27;s hands move outside the head camera&#x27;s field of view. These eye-in-hand cameras support visual servoing for tasks like tool grasping and valve turning, where millimeter-level precision matters. The wrist cameras run at lower frame rates (10-15 fps) to reduce computational overhead, since manipulation tasks tolerate higher latency than balance control.</p>
<p><strong>Key Takeaway</strong>: Multi-camera systems enable task specialization‚Äîhead cameras prioritize depth for locomotion, while wrist cameras prioritize color detail for manipulation, each optimized for distinct computational and spatial requirements.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="-case-study-tesla-optimus---vision-only-navigation">üìä Case Study: Tesla Optimus - Vision-Only Navigation<a href="#-case-study-tesla-optimus---vision-only-navigation" class="hash-link" aria-label="Direct link to üìä Case Study: Tesla Optimus - Vision-Only Navigation" title="Direct link to üìä Case Study: Tesla Optimus - Vision-Only Navigation" translate="no">‚Äã</a></h3>
<p><strong>Robot</strong>: Optimus (Tesla)
<strong>Camera System</strong>: Eight monocular cameras (similar to Tesla vehicle FSD architecture)
<strong>Application</strong>: Factory floor navigation, bin picking, object recognition</p>
<p>Tesla Optimus deliberately avoids lidar and RGB-D sensors, relying exclusively on monocular cameras for all perception tasks‚Äîa design philosophy inherited from Tesla&#x27;s autonomous vehicle program. The robot uses eight cameras positioned around its head and torso to achieve near-360¬∞ coverage, mirroring the camera placement in Tesla cars. Each camera captures 1280√ó960 resolution at 36 fps, feeding a neural network trained on Tesla&#x27;s massive driving dataset to perform depth estimation, object detection, and semantic segmentation simultaneously.</p>
<p>The vision-only approach presents unique challenges: without active depth sensing, Optimus must infer distance from monocular cues like object size, texture gradients, and parallax motion. Tesla addresses this through large-scale data collection‚Äîthousands of hours of teleoperated robot demonstrations provide ground-truth depth labels for training neural networks to predict metric depth from single images. This strategy trades sensor cost and outdoor reliability (cameras work in sunlight, unlike IR-based depth sensors) against computational complexity and the need for extensive training data.</p>
<p><strong>Key Takeaway</strong>: Monocular camera arrays can replace active depth sensors when combined with powerful neural network architectures and large training datasets, demonstrating that software innovation can compensate for hardware limitations in perception system design.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="-ai-colearning-prompt">üí¨ AI Colearning Prompt<a href="#-ai-colearning-prompt" class="hash-link" aria-label="Direct link to üí¨ AI Colearning Prompt" title="Direct link to üí¨ AI Colearning Prompt" translate="no">‚Äã</a></h3>
<blockquote>
<p><strong>Ask your AI assistant</strong>: &quot;Why do stereo cameras need synchronized image capture to calculate depth accurately? What happens if the left and right camera frames are captured at slightly different times while the robot is moving?&quot;</p>
<p>This prompt helps you understand temporal synchronization requirements in multi-camera systems. Your AI assistant can explain how motion between unsynchronized frames causes depth estimation errors, and why hardware-triggered stereo cameras (where both sensors capture simultaneously) produce more accurate depth maps than software-synchronized systems, especially critical for fast-moving humanoid robots.</p>
</blockquote>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-principles">Key Principles<a href="#key-principles" class="hash-link" aria-label="Direct link to Key Principles" title="Direct link to Key Principles" translate="no">‚Äã</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="31-camera-types-and-trade-offs">3.1 Camera Types and Trade-offs<a href="#31-camera-types-and-trade-offs" class="hash-link" aria-label="Direct link to 3.1 Camera Types and Trade-offs" title="Direct link to 3.1 Camera Types and Trade-offs" translate="no">‚Äã</a></h3>
<p>Humanoid robots employ three primary camera types, each with distinct advantages and limitations shaped by the physics of depth perception.</p>
<p><strong>Monocular cameras</strong> use a single lens to capture 2D images, similar to taking a photograph with your phone. Their primary advantage is simplicity: they&#x27;re lightweight, inexpensive, and require minimal computational overhead. Tesla Optimus uses monocular cameras for object detection and scene understanding, relying on deep learning models to infer depth from visual cues like texture gradients and object size. However, monocular cameras inherently lack depth information‚Äîthey cannot directly measure how far away objects are without additional processing or motion (e.g., structure from motion). This makes them suitable for classification tasks (identifying objects) but challenging for precise manipulation or obstacle avoidance.</p>
<p><strong>Stereo cameras</strong> mimic human binocular vision by using two synchronized cameras separated by a baseline distance (typically 6-12 cm for humanoid heads). By comparing the left and right images and identifying corresponding pixels, stereo algorithms compute depth through triangulation. The depth accuracy depends on the baseline: wider baselines provide better depth precision at long ranges but lose accuracy for nearby objects. Stereo cameras work passively (no active illumination required) and are effective outdoors, making them popular for navigation tasks. Boston Dynamics&#x27; Atlas uses stereo vision for terrain mapping during locomotion. The primary disadvantage is computational cost: stereo matching algorithms must solve the correspondence problem for thousands of pixels in real-time, and performance degrades in textureless environments (e.g., blank walls).</p>
<p><strong>RGB-D cameras</strong> (RGB plus depth) actively measure distance using either structured light projection (e.g., Microsoft Kinect) or time-of-flight (ToF) technology (e.g., Intel RealSense D435). These cameras output both color images and per-pixel depth maps without requiring correspondence solving. Structured light systems project infrared patterns and measure distortion, while ToF cameras emit infrared pulses and measure round-trip time. RGB-D cameras excel at indoor grasping tasks because they provide immediate, dense depth information for object pose estimation. Their limitations include restricted outdoor performance (sunlight interferes with infrared), limited range (typically 0.3-10 meters), and higher power consumption compared to passive cameras.</p>
<p>Here&#x27;s a comparison of the three camera types:</p>
<table><thead><tr><th>Feature</th><th>Monocular</th><th>Stereo</th><th>RGB-D</th></tr></thead><tbody><tr><td>Depth Info</td><td>None (2D)</td><td>Triangulation</td><td>Active (IR/ToF)</td></tr><tr><td>Range</td><td>N/A</td><td>0.5-10m</td><td>0.3-10m</td></tr><tr><td>Advantages</td><td>Simple, low cost</td><td>Passive depth</td><td>No computation for depth</td></tr><tr><td>Disadvantages</td><td>No depth</td><td>Requires calibration</td><td>Limited outdoor use</td></tr><tr><td>Humanoid Use</td><td>Object detection</td><td>Navigation</td><td>Indoor grasping</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="32-camera-parameters-and-their-effects">3.2 Camera Parameters and Their Effects<a href="#32-camera-parameters-and-their-effects" class="hash-link" aria-label="Direct link to 3.2 Camera Parameters and Their Effects" title="Direct link to 3.2 Camera Parameters and Their Effects" translate="no">‚Äã</a></h3>
<p>Selecting camera parameters requires balancing competing priorities: resolution, field of view, and frame rate each impose trade-offs on computational resources, weight, and performance.</p>
<p><strong>Resolution</strong> determines spatial detail. Common resolutions range from VGA (640√ó480) for low-latency applications to 4K (3840√ó2160) for inspection tasks. Higher resolution enables finer object detection (e.g., reading small text or detecting distant objects) but quadratically increases data volume and processing time. For humanoid robots, 640√ó480 or 1280√ó720 represents a practical sweet spot: sufficient detail for navigation and manipulation without overwhelming onboard computers. Tesla Optimus reportedly uses 1280√ó960 cameras to balance detail and inference speed.</p>
<p><strong>Field of view</strong> defines the angular coverage of the camera, typically expressed in degrees horizontally. Narrow fields of view (30-50¬∞, telephoto lenses) provide magnified detail at distance but sacrifice peripheral awareness. Wide fields of view (90-120¬∞, wide-angle lenses) capture more context but distort edges and reduce central detail. The relationship between focal length and FoV is inverse: shorter focal lengths yield wider views. For humanoid head-mounted cameras, 60-90¬∞ horizontal FoV is common, matching human peripheral vision. Manipulation cameras on wrists often use narrower 40-60¬∞ FoV to focus on the workspace directly ahead.</p>
<p><strong>Frame rate</strong> controls temporal resolution. Standard rates include 30 fps (sufficient for walking and general navigation), 60 fps (enables smooth tracking of moving objects), and 120+ fps (required for high-speed manipulation or catching). Higher frame rates increase data bandwidth linearly and reduce exposure time (potentially requiring more lighting). For balance-critical tasks where visual feedback informs control loops, 60 fps or higher minimizes latency between perception and action.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="33-camera-placement-on-humanoid-robots">3.3 Camera Placement on Humanoid Robots<a href="#33-camera-placement-on-humanoid-robots" class="hash-link" aria-label="Direct link to 3.3 Camera Placement on Humanoid Robots" title="Direct link to 3.3 Camera Placement on Humanoid Robots" translate="no">‚Äã</a></h3>
<p>Where you mount cameras fundamentally shapes what tasks a humanoid can perform. Three canonical locations each serve distinct purposes.</p>
<p><strong>Head-mounted cameras</strong> (typically stereo pairs or RGB-D) provide the robot&#x27;s primary situational awareness, analogous to human eyes. Positioned 1.5-1.8 meters above ground on adult-scale humanoids, they offer a commanding view for navigation, person detection, and scene understanding. The head&#x27;s ability to pan and tilt (via neck actuators) extends the effective field of view and allows the robot to track moving targets or inspect specific regions. The primary disadvantage is that head cameras cannot see the robot&#x27;s own hands during manipulation tasks unless the head continuously tracks the hands‚Äîcreating a coordination overhead.</p>
<p><strong>Wrist-mounted cameras</strong> (often monocular or small RGB-D sensors) solve the manipulation visibility problem by maintaining a constant view of the end-effector workspace. These &quot;eye-in-hand&quot; cameras enable visual servoing (adjusting grasp approach based on real-time feedback) and verification (confirming object contact). Boston Dynamics&#x27; Atlas uses wrist cameras to guide tool grasping. The limitation is that wrist cameras provide a restricted, task-focused view and do not contribute to global navigation or scene awareness.</p>
<p><strong>Chest-mounted cameras</strong> represent a compromise: positioned at torso height with a fixed forward orientation, they maintain a stable viewpoint unaffected by head motion while still capturing the hands during reaching tasks. Some humanoid designs use chest cameras for odometry (tracking motion by monitoring floor features) or as a fallback navigation sensor. However, chest cameras sacrifice the flexibility of head-mounted systems and may be occluded by the robot&#x27;s own arms.</p>
<p>Many modern humanoids employ all three camera placements simultaneously, fusing data across sensors. This multi-camera strategy mirrors human vision-motor coordination: your eyes (head cameras) survey the scene, your peripheral vision (chest cameras) monitors obstacles, and you glance at your hands (wrist cameras) when precision matters.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="34-image-data-representation-in-ros2">3.4 Image Data Representation in ROS2<a href="#34-image-data-representation-in-ros2" class="hash-link" aria-label="Direct link to 3.4 Image Data Representation in ROS2" title="Direct link to 3.4 Image Data Representation in ROS2" translate="no">‚Äã</a></h3>
<p>In ROS2, camera data flows through the system as <code>sensor_msgs/Image</code> messages‚Äîa standardized format that separates image metadata from raw pixel data. Understanding this message structure is essential for subscribing to camera topics and integrating vision algorithms.</p>
<p>The <code>sensor_msgs/Image</code> message contains seven fields:</p>
<ul>
<li class=""><strong>header</strong>: A standard ROS2 header including a timestamp (when the image was captured) and frame_id (the coordinate frame of the camera, e.g., &quot;camera_optical_frame&quot;). Timestamps enable synchronization across multiple sensors.</li>
<li class=""><strong>height</strong> and <strong>width</strong>: Integer values specifying image dimensions in pixels.</li>
<li class=""><strong>encoding</strong>: A string describing the pixel format, such as &quot;rgb8&quot; (8-bit RGB), &quot;bgr8&quot; (8-bit BGR), &quot;mono8&quot; (grayscale), or &quot;32FC1&quot; (32-bit floating-point depth map). This field determines how to interpret the raw data array.</li>
<li class=""><strong>is_bigendian</strong>: A boolean indicating byte order (rarely needed on modern little-endian systems).</li>
<li class=""><strong>step</strong>: The number of bytes per row, accounting for potential padding. For an 8-bit RGB image of width W, step = W √ó 3 (three bytes per pixel).</li>
<li class=""><strong>data</strong>: A flat byte array containing the actual pixel values in row-major order (left-to-right, top-to-bottom).</li>
</ul>
<p><strong>Color space encodings</strong> warrant special attention because different libraries use different conventions. &quot;rgb8&quot; places red in the first byte, green in the second, blue in the third. &quot;bgr8&quot; reverses this order‚Äîa quirk inherited from OpenCV, which defaults to BGR. Grayscale encodings like &quot;mono8&quot; use a single byte per pixel. Floating-point encodings like &quot;32FC1&quot; store depth values in meters, common for RGB-D cameras.</p>
<p><strong>Memory layout</strong> follows row-major order: pixel (0,0) is the top-left corner, and pixels are stored row-by-row. To access the pixel at row <code>r</code>, column <code>c</code> in an &quot;rgb8&quot; image, compute the byte offset as <code>offset = r * step + c * 3</code>, then read three consecutive bytes for red, green, and blue channels.</p>
<p>This standardized representation allows ROS2 nodes written in different languages (Python, C++) to exchange image data seamlessly. When you subscribe to a camera topic, you receive these <code>sensor_msgs/Image</code> messages at the configured frame rate, ready for processing.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="35-image-flow-through-ros2-nodes">3.5 Image Flow Through ROS2 Nodes<a href="#35-image-flow-through-ros2-nodes" class="hash-link" aria-label="Direct link to 3.5 Image Flow Through ROS2 Nodes" title="Direct link to 3.5 Image Flow Through ROS2 Nodes" translate="no">‚Äã</a></h3>
<p>Camera data in ROS2 follows a publisher-subscriber pattern that decouples image acquisition from processing. A <strong>camera driver node</strong> (e.g., <code>usb_cam_node</code> for USB cameras or vendor-specific drivers for industrial cameras) publishes <code>sensor_msgs/Image</code> messages on a topic like <code>/camera/image_raw</code>. Downstream nodes‚Äîyour vision algorithms, display tools, or recording utilities‚Äîsubscribe to this topic and receive each new frame via callback functions.</p>
<p>Accompanying the image topic, most camera drivers publish a <strong>sensor_msgs/CameraInfo</strong> message on <code>/camera/camera_info</code>. This message contains intrinsic camera parameters (focal length, principal point, distortion coefficients) necessary for tasks like undistorting images or projecting 3D points into the image plane. Stereo algorithms require synchronized CameraInfo from both left and right cameras.</p>
<p><strong>Quality of Service (QoS) settings</strong> govern message delivery reliability. For camera topics, common practice uses &quot;best effort&quot; reliability (tolerates dropped frames to minimize latency) with a moderate queue depth (e.g., 10 messages). This configuration ensures real-time performance: if processing lags behind the camera frame rate, old frames are discarded rather than queuing indefinitely. For critical applications like recording, you might use &quot;reliable&quot; delivery at the cost of increased latency.</p>
<p>The image flow pipeline typically looks like this:</p>
<ol>
<li class="">Camera driver captures frame from hardware</li>
<li class="">Driver publishes <code>sensor_msgs/Image</code> on <code>/camera/image_raw</code></li>
<li class="">Vision node receives message via callback (e.g., <code>image_callback</code>)</li>
<li class="">Node processes image (e.g., object detection with OpenCV/PyTorch)</li>
<li class="">Node publishes results (e.g., bounding boxes, detected classes) on output topics</li>
</ol>
<p>This modular design allows you to swap camera hardware (changing only the driver node) or upgrade vision algorithms (changing only the processing node) without rewriting the entire system.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="-expert-insight-rgb-vs-bgr-color-space-confusion">üéì Expert Insight: RGB vs BGR Color Space Confusion<a href="#-expert-insight-rgb-vs-bgr-color-space-confusion" class="hash-link" aria-label="Direct link to üéì Expert Insight: RGB vs BGR Color Space Confusion" title="Direct link to üéì Expert Insight: RGB vs BGR Color Space Confusion" translate="no">‚Äã</a></h3>
<blockquote>
<p><strong>Common Pitfall</strong>: One of the most frequent errors in ROS2 vision pipelines is forgetting that OpenCV uses BGR (Blue-Green-Red) color channel ordering by default, while most other libraries and ROS2 Image messages use RGB (Red-Green-Blue). When you subscribe to an &quot;rgb8&quot; encoded image and pass it directly to OpenCV functions like <code>cv2.imshow()</code>, colors will appear inverted (reds become blues, blues become reds).</p>
<p><strong>Best practice</strong>: Always check the encoding field in your image callback. If working with OpenCV, either convert RGB to BGR using <code>cv2.cvtColor(img, cv2.COLOR_RGB2BGR)</code> or explicitly request &quot;bgr8&quot; encoding when creating your camera driver. This simple check prevents hours of debugging &quot;why does my red ball appear blue?&quot;</p>
</blockquote>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="-ai-colearning-prompt-1">üí¨ AI Colearning Prompt<a href="#-ai-colearning-prompt-1" class="hash-link" aria-label="Direct link to üí¨ AI Colearning Prompt" title="Direct link to üí¨ AI Colearning Prompt" translate="no">‚Äã</a></h3>
<blockquote>
<p><strong>Ask your AI assistant</strong>: &quot;Explain the relationship between field of view and focal length using a binoculars analogy. Why do wide-angle lenses have shorter focal lengths?&quot;</p>
<p>Understanding the inverse relationship between focal length and field of view is crucial for selecting the right lens for a task. Your AI assistant can clarify this optical principle by comparing camera lenses to adjustable binoculars: zooming in (increasing focal length) narrows your view but magnifies distant objects, while zooming out (decreasing focal length) widens your view but reduces magnification. This analogy helps build intuition for why humanoid robots use wide-angle lenses for navigation (maximum awareness) and telephoto lenses for inspection tasks (detailed examination at distance).</p>
</blockquote>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="embedded-callouts">Embedded Callouts<a href="#embedded-callouts" class="hash-link" aria-label="Direct link to Embedded Callouts" title="Direct link to Embedded Callouts" translate="no">‚Äã</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="-common-pitfall-resolution-isnt-everything">üéì Common Pitfall: Resolution Isn&#x27;t Everything<a href="#-common-pitfall-resolution-isnt-everything" class="hash-link" aria-label="Direct link to üéì Common Pitfall: Resolution Isn&#x27;t Everything" title="Direct link to üéì Common Pitfall: Resolution Isn&#x27;t Everything" translate="no">‚Äã</a></h3>
<blockquote>
<p><strong>Resolution sweet spot for humanoids</strong>: Beginners often assume higher resolution always improves performance. In reality, 640√ó480 resolution is often superior to 4K for real-time humanoid applications. Why? Quadrupling resolution (e.g., 640√ó480 to 1280√ó960) quadruples pixel count, data transfer bandwidth, and processing time, but rarely quadruples task performance. For navigation and manipulation, the limiting factor is usually algorithm latency, not spatial detail. Modern object detectors (YOLO, Faster R-CNN) are trained on ~640-pixel images and perform well at this scale. Only specialized tasks‚Äîreading small text, inspecting fine defects, or detecting distant objects‚Äîjustify resolutions above 1280√ó720 on resource-constrained robot hardware.</p>
</blockquote>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="-hands-on-collaboration-exercise-multi-camera-system-design">ü§ù Hands-On Collaboration Exercise: Multi-Camera System Design<a href="#-hands-on-collaboration-exercise-multi-camera-system-design" class="hash-link" aria-label="Direct link to ü§ù Hands-On Collaboration Exercise: Multi-Camera System Design" title="Direct link to ü§ù Hands-On Collaboration Exercise: Multi-Camera System Design" translate="no">‚Äã</a></h3>
<blockquote>
<p><strong>Design challenge for you and your AI assistant</strong>: Imagine you&#x27;re designing the vision system for a humanoid delivery robot that must navigate office buildings, ride elevators, and hand packages to people. The robot has three available camera mounts (head, chest, wrists) and a computational budget for processing three camera streams.</p>
<p>Work with your AI assistant to decide: Which camera types (monocular, stereo, RGB-D) would you place at each location, and why? Consider these competing requirements: (1) safe navigation requires wide FoV and depth sensing, (2) object recognition (detecting package labels) requires color and resolution, (3) human interaction requires face detection and gaze estimation, and (4) hand-off tasks require precise depth at close range.</p>
<p>There&#x27;s no single &quot;correct&quot; answer‚Äîthe goal is to articulate trade-offs and defend your choices. Your AI assistant can role-play as a hardware engineer, challenging your decisions with budget and power constraints. This exercise mirrors real-world system design discussions on humanoid robotics teams.</p>
</blockquote>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="practical-example-subscribing-to-camera-images">Practical Example: Subscribing to Camera Images<a href="#practical-example-subscribing-to-camera-images" class="hash-link" aria-label="Direct link to Practical Example: Subscribing to Camera Images" title="Direct link to Practical Example: Subscribing to Camera Images" translate="no">‚Äã</a></h2>
<p>The following Python code demonstrates the fundamental pattern for receiving camera images in a ROS2 node. This subscriber node connects to a camera driver&#x27;s output topic and logs metadata about each received frame.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> rclpy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> rclpy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">node </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Node</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> sensor_msgs</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">msg </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">CameraSubscriber</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Node</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;Subscribes to camera images and logs image metadata.&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;camera_subscriber&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Subscribe to the raw image topic (typical ROS2 camera driver output).</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Image messages arrive asynchronously; ROS2 queues them in order.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">subscription </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">create_subscription</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Image</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string" style="color:#e3116c">&#x27;/camera/image_raw&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">image_callback</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token number" style="color:#36acaa">10</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># QoS queue depth: keep latest 10 messages; drop older ones</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">image_callback</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> msg</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> Image</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;Process incoming camera image and log metadata.</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="display:inline-block;color:#e3116c"></span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        This callback is invoked by the ROS2 executor each time a new frame</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        arrives on &#x27;/camera/image_raw&#x27;. The msg parameter is a sensor_msgs/Image</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        object containing all metadata and pixel data for a single frame.</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Access image dimensions and format from ROS2 message fields</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_logger</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">info</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string-interpolation string" style="color:#e3116c">f&#x27;Image received: </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">msg</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">width</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">x</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">msg</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">height</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">, &#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string-interpolation string" style="color:#e3116c">f&#x27;encoding=</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">msg</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">encoding</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">, timestamp=</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">msg</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">header</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">stamp</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">sec</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Image processing (e.g., OpenCV operations) would go here.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Example: convert to NumPy with cv_bridge.imgmsg_to_cv2(msg, desired_encoding=&#x27;passthrough&#x27;)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># or manually: np.frombuffer(msg.data, dtype=np.uint8).reshape(msg.height, msg.width, 3)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Note: rclpy.init(), spin(), shutdown() omitted for conceptual clarity</span><br></span></code></pre></div></div>
<p>The <code>image_callback</code> method executes every time a new frame arrives on the <code>/camera/image_raw</code> topic. Inside this callback, you have access to the complete <code>sensor_msgs/Image</code> message, including dimensions (<code>msg.width</code>, <code>msg.height</code>), pixel format (<code>msg.encoding</code>), timestamp (<code>msg.header.stamp</code>), and raw pixel data (<code>msg.data</code>). This example logs metadata to verify the subscription is working‚Äîa crucial debugging step when integrating new cameras.</p>
<p>The <code>sensor_msgs/Image</code> message contains everything needed to reconstruct the image: if the encoding is &quot;bgr8&quot; (common for USB cameras), you can convert <code>msg.data</code> to a NumPy array and pass it to OpenCV functions like <code>cv2.imshow()</code> for display or <code>cv2.Canny()</code> for edge detection. For humanoid vision pipelines, the callback typically performs feature extraction (e.g., detecting faces with a neural network), publishes results to downstream nodes (e.g., control systems), or records images for offline analysis.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="accessing-camera-calibration-parameters-with-camerainfo">Accessing Camera Calibration Parameters with CameraInfo<a href="#accessing-camera-calibration-parameters-with-camerainfo" class="hash-link" aria-label="Direct link to Accessing Camera Calibration Parameters with CameraInfo" title="Direct link to Accessing Camera Calibration Parameters with CameraInfo" translate="no">‚Äã</a></h3>
<p>Alongside the image topic, ROS2 camera drivers publish <strong>sensor_msgs/CameraInfo</strong> messages containing intrinsic camera parameters‚Äîfocal length, principal point, and distortion coefficients. These parameters are essential for geometric tasks: undistorting images, projecting 3D world points into the image plane, or estimating object poses from image coordinates. The following example demonstrates subscribing to camera calibration data and extracting key parameters like focal length and field of view.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> rclpy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> rclpy</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">node </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Node</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> sensor_msgs</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">msg </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> CameraInfo</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> math</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">class</span><span class="token plain"> </span><span class="token class-name">CameraInfoSubscriber</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Node</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;Subscribes to camera calibration info and extracts optical parameters.&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token builtin">super</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">__init__</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;camera_info_subscriber&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Subscribe to the camera_info topic published alongside image_raw.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># CameraInfo messages are typically published at lower frequency (1-10 Hz)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># since calibration parameters remain constant during operation.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">subscription </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">create_subscription</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            CameraInfo</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string" style="color:#e3116c">&#x27;/camera/camera_info&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">info_callback</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token number" style="color:#36acaa">10</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">info_callback</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">self</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> msg</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> CameraInfo</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">None</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token triple-quoted-string string" style="color:#e3116c">&quot;&quot;&quot;Extract camera parameters from intrinsic calibration matrix.</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="display:inline-block;color:#e3116c"></span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        The K matrix (intrinsic matrix) is a 3x3 array flattened to 9 elements:</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        [fx,  0, cx,</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">         0,  fy, cy,</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">         0,   0,  1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        where fx/fy are focal lengths (pixels) and cx/cy define the principal point.</span><br></span><span class="token-line" style="color:#393A34"><span class="token triple-quoted-string string" style="color:#e3116c">        &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Extract intrinsic parameters from the K matrix (row-major storage)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        fx</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">float</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">k</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Focal length in x (pixels)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        fy</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">float</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">k</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">4</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Focal length in y (pixels)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        cx</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">float</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">k</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Principal point x (image center, pixels)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        cy</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">float</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">k</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">5</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># Principal point y (image center, pixels)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Calculate horizontal field of view from focal length and image width.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># This accounts for actual image sensor dimensions, not just resolution.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        fov_x_rad</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">float</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2.0</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> math</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">atan</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">msg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">width </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">2.0</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> fx</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        fov_x_deg</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token builtin">float</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> fov_x_rad </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">180.0</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> math</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">pi</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Log calibration parameters for verification during integration</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_logger</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">info</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string-interpolation string" style="color:#e3116c">f&#x27;Camera calibration: </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">msg</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">width</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">x</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">msg</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">.</span><span class="token string-interpolation interpolation">height</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">, &#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string-interpolation string" style="color:#e3116c">f&#x27;focal_length_x=</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">fx</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">:</span><span class="token string-interpolation interpolation format-spec">.1f</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">px, FOV=</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">fov_x_deg</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">:</span><span class="token string-interpolation interpolation format-spec">.1f</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">¬∞, &#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token string-interpolation string" style="color:#e3116c">f&#x27;principal_point=(</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">cx</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">:</span><span class="token string-interpolation interpolation format-spec">.1f</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">, </span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">{</span><span class="token string-interpolation interpolation">cy</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">:</span><span class="token string-interpolation interpolation format-spec">.1f</span><span class="token string-interpolation interpolation punctuation" style="color:#393A34">}</span><span class="token string-interpolation string" style="color:#e3116c">)&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre></div></div>
<p>This CameraInfo subscriber demonstrates how to extract the intrinsic camera matrix (K matrix) and compute the field of view‚Äîessential for tasks like rectifying distorted images or determining if a detected object at a specific pixel location falls within the manipulator&#x27;s reachable workspace. The K matrix is the foundation of camera projection: multiplying a 3D world point by K yields the pixel location where that point appears in the image, enabling geometric reasoning about objects in the scene.</p>
<p>This subscriber pattern is foundational: whether you&#x27;re building a simple image logger or a sophisticated multi-stage vision pipeline with object tracking, pose estimation, and scene reconstruction, every ROS2 vision node begins by subscribing to camera topics and implementing robust callbacks that process image data in real-time.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="-practice-exercise-inspecting-live-camera-topics">ü§ù Practice Exercise: Inspecting Live Camera Topics<a href="#-practice-exercise-inspecting-live-camera-topics" class="hash-link" aria-label="Direct link to ü§ù Practice Exercise: Inspecting Live Camera Topics" title="Direct link to ü§ù Practice Exercise: Inspecting Live Camera Topics" translate="no">‚Äã</a></h3>
<blockquote>
<p><strong>Challenge</strong>: If you have access to a ROS2 system with a camera (physical hardware, Gazebo simulation, or rosbag playback), use command-line tools to inspect camera topics:</p>
<ol>
<li class="">List all active topics: <code>ros2 topic list</code></li>
<li class="">Find the camera image topic (usually <code>/camera/image_raw</code> or similar)</li>
<li class="">Display the message type: <code>ros2 topic info /camera/image_raw</code></li>
<li class="">Echo one message to see its structure: <code>ros2 topic echo /camera/image_raw --once</code></li>
<li class="">Check the publishing rate: <code>ros2 topic hz /camera/image_raw</code></li>
<li class="">Inspect CameraInfo: <code>ros2 topic echo /camera/camera_info --once</code></li>
</ol>
<p><strong>Goal</strong>: Extract the image width, height, encoding, and camera&#x27;s horizontal field of view (calculate from the K matrix focal length). Compare the actual frame rate to the camera&#x27;s specification.</p>
<p><strong>Hint</strong>: For step 6, look for the <code>k</code> array in the output. The first element <code>k[0]</code> is the focal length in pixels. Use the FOV formula from the CameraInfoSubscriber code example.</p>
</blockquote>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary" translate="no">‚Äã</a></h2>
<ul>
<li class=""><strong>Cameras transform light into digital pixel arrays</strong>, providing humanoid robots with semantic understanding of their environment through five key parameters: pixels, resolution, frame rate, field of view, and image encoding.</li>
<li class=""><strong>Three camera types serve different roles</strong>: monocular cameras offer simplicity for 2D tasks, stereo cameras provide passive depth through triangulation for navigation, and RGB-D cameras deliver active depth sensing for precise indoor manipulation.</li>
<li class=""><strong>Camera placement determines capabilities</strong>: head-mounted cameras enable situational awareness and navigation, wrist-mounted cameras support manipulation tasks with visual servoing, and chest-mounted cameras offer a stable intermediate viewpoint.</li>
<li class=""><strong>ROS2 standardizes image data as sensor_msgs/Image messages</strong> with separate fields for dimensions, encoding (rgb8, bgr8, mono8, etc.), timestamps, and raw pixel data stored in row-major order‚Äîenabling language-agnostic image exchange across nodes.</li>
<li class=""><strong>The publisher-subscriber pattern decouples image acquisition from processing</strong>, allowing camera driver nodes to publish frames on topics like <code>/camera/image_raw</code> while vision algorithms subscribe and process images via callbacks, using &quot;best effort&quot; QoS for real-time performance.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="next-steps">Next Steps<a href="#next-steps" class="hash-link" aria-label="Direct link to Next Steps" title="Direct link to Next Steps" translate="no">‚Äã</a></h2>
<p>Now that you understand how cameras capture and represent visual information in ROS2, the natural next question is: how do we accurately measure distance to objects in 3D space? While monocular cameras lack depth and stereo cameras require computational correspondence solving, dedicated depth sensing technologies offer direct distance measurements critical for navigation and manipulation.</p>
<p>In Lesson 2, we&#x27;ll explore depth sensing technologies‚Äîincluding time-of-flight sensors, structured light systems, and lidar‚Äîthat complement camera systems by providing millimeter-accurate range data. You&#x27;ll learn how to fuse depth information with camera images to create rich 3D representations of the robot&#x27;s workspace, enabling tasks like obstacle avoidance, grasp planning, and terrain mapping.</p>
<p>Continue to <a class="" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/02-depth-sensing">Lesson 2: Depth Sensing Technologies</a> to build on your camera fundamentals and unlock 3D perception capabilities.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-tags-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/ros-2">ros2</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/sensors">sensors</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/camera">camera</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/computer-vision">computer-vision</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/hackathon-phase-01/docs/tags/humanoid-robotics">humanoid-robotics</a></li></ul></div></div><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/01-camera-systems.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Module 2: Sensors and Perception for Humanoid Robots</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/camera-systems.summary"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Summary: Lesson 1 - Camera Systems and Computer Vision</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-a-camera-in-robotics" class="table-of-contents__link toc-highlight">What Is a Camera in Robotics?</a></li><li><a href="#why-camera-systems-matter-for-physical-ai" class="table-of-contents__link toc-highlight">Why Camera Systems Matter for Physical AI</a><ul><li><a href="#-case-study-boston-dynamics-atlas---multi-camera-perception-system" class="table-of-contents__link toc-highlight">üìä Case Study: Boston Dynamics Atlas - Multi-Camera Perception System</a></li><li><a href="#-case-study-tesla-optimus---vision-only-navigation" class="table-of-contents__link toc-highlight">üìä Case Study: Tesla Optimus - Vision-Only Navigation</a></li><li><a href="#-ai-colearning-prompt" class="table-of-contents__link toc-highlight">üí¨ AI Colearning Prompt</a></li></ul></li><li><a href="#key-principles" class="table-of-contents__link toc-highlight">Key Principles</a><ul><li><a href="#31-camera-types-and-trade-offs" class="table-of-contents__link toc-highlight">3.1 Camera Types and Trade-offs</a></li><li><a href="#32-camera-parameters-and-their-effects" class="table-of-contents__link toc-highlight">3.2 Camera Parameters and Their Effects</a></li><li><a href="#33-camera-placement-on-humanoid-robots" class="table-of-contents__link toc-highlight">3.3 Camera Placement on Humanoid Robots</a></li><li><a href="#34-image-data-representation-in-ros2" class="table-of-contents__link toc-highlight">3.4 Image Data Representation in ROS2</a></li><li><a href="#35-image-flow-through-ros2-nodes" class="table-of-contents__link toc-highlight">3.5 Image Flow Through ROS2 Nodes</a></li><li><a href="#-expert-insight-rgb-vs-bgr-color-space-confusion" class="table-of-contents__link toc-highlight">üéì Expert Insight: RGB vs BGR Color Space Confusion</a></li><li><a href="#-ai-colearning-prompt-1" class="table-of-contents__link toc-highlight">üí¨ AI Colearning Prompt</a></li></ul></li><li><a href="#embedded-callouts" class="table-of-contents__link toc-highlight">Embedded Callouts</a><ul><li><a href="#-common-pitfall-resolution-isnt-everything" class="table-of-contents__link toc-highlight">üéì Common Pitfall: Resolution Isn&#39;t Everything</a></li><li><a href="#-hands-on-collaboration-exercise-multi-camera-system-design" class="table-of-contents__link toc-highlight">ü§ù Hands-On Collaboration Exercise: Multi-Camera System Design</a></li></ul></li><li><a href="#practical-example-subscribing-to-camera-images" class="table-of-contents__link toc-highlight">Practical Example: Subscribing to Camera Images</a><ul><li><a href="#accessing-camera-calibration-parameters-with-camerainfo" class="table-of-contents__link toc-highlight">Accessing Camera Calibration Parameters with CameraInfo</a></li><li><a href="#-practice-exercise-inspecting-live-camera-topics" class="table-of-contents__link toc-highlight">ü§ù Practice Exercise: Inspecting Live Camera Topics</a></li></ul></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#next-steps" class="table-of-contents__link toc-highlight">Next Steps</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Learn</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/">Textbook</a></li><li class="footer__item"><a class="footer__link-item" href="/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/ros2-nervous-system/">Module 1: ROS2</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/anusbutt/hackathon-phase-01" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://linkedin.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">X (Twitter)<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://instagram.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://panaversity.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Panaversity<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2026 Physical AI & Humanoid Robotics Textbook. Built for Panaversity Hackathon.</div></div></div></footer><button class="floatingChatButton__Uyc" aria-label="Open AI Assistant" title="Ask questions about Physical AI &amp; Humanoid Robotics"><span class="chatIcon_GGtK">üí¨</span><span class="chatLabel_InO3">AI Assistant</span></button></div>
</body>
</html>