"use strict";(globalThis.webpackChunkbook_source=globalThis.webpackChunkbook_source||[]).push([[366],{8385:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.casestudies","title":"Case Studies: Real-World Multi-Sensor Integration","description":"These case studies showcase how leading humanoid robots integrate camera systems, depth sensors, IMUs, and sensor fusion to solve real-world challenges. Use these examples as design inspiration for your capstone project, not as blueprints to copy. Each robot made specific trade-offs based on their operational requirements, cost constraints, and technical priorities.","source":"@site/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/05-capstone-project.casestudies.md","sourceDirName":"13-Physical-AI-Humanoid-Robotics/02-sensors-perception","slug":"/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.casestudies","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.casestudies","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/13-Physical-AI-Humanoid-Robotics/02-sensors-perception/05-capstone-project.casestudies.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Multi-Sensor Fusion Capstone: Architecture Documentation","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.architecture"},"next":{"title":"Module 2 Capstone Project: Integrated Sensor System Design","permalink":"/hackathon-phase-01/docs/Physical-AI-Humanoid-Robotics/sensors-perception/capstone-project.content"}}');var r=n(4848),o=n(8453);const t={},a="Case Studies: Real-World Multi-Sensor Integration",l={},c=[{value:"\ud83d\udcca Case Study 1: Boston Dynamics Atlas - Multi-Sensor SLAM for Dynamic Environments",id:"-case-study-1-boston-dynamics-atlas---multi-sensor-slam-for-dynamic-environments",level:2},{value:"Robot Overview",id:"robot-overview",level:3},{value:"Sensor Configuration",id:"sensor-configuration",level:3},{value:"Integration Approach",id:"integration-approach",level:3},{value:"Lessons Applied",id:"lessons-applied",level:3},{value:"Design Insights",id:"design-insights",level:3},{value:"\ud83d\udcca Case Study 2: Agility Robotics Digit - Visual-Inertial Odometry for GPS-Free Indoor Navigation",id:"-case-study-2-agility-robotics-digit---visual-inertial-odometry-for-gps-free-indoor-navigation",level:2},{value:"Robot Overview",id:"robot-overview-1",level:3},{value:"Sensor Configuration",id:"sensor-configuration-1",level:3},{value:"Integration Approach",id:"integration-approach-1",level:3},{value:"Lessons Applied",id:"lessons-applied-1",level:3},{value:"Design Insights",id:"design-insights-1",level:3},{value:"\ud83d\udcca Case Study 3: Tesla Optimus - Multi-Camera Fusion for Cost-Optimized Manipulation",id:"-case-study-3-tesla-optimus---multi-camera-fusion-for-cost-optimized-manipulation",level:2},{value:"Robot Overview",id:"robot-overview-2",level:3},{value:"Sensor Configuration",id:"sensor-configuration-2",level:3},{value:"Integration Approach",id:"integration-approach-2",level:3},{value:"Lessons Applied",id:"lessons-applied-2",level:3},{value:"Design Insights",id:"design-insights-2",level:3},{value:"\ud83d\udcca Case Study 4: Comparative Analysis - Sensor Suite Trade-offs",id:"-case-study-4-comparative-analysis---sensor-suite-trade-offs",level:2},{value:"Key Takeaway for Capstone Design",id:"key-takeaway-for-capstone-design",level:3},{value:"Summary: Applying Case Studies to Your Capstone",id:"summary-applying-case-studies-to-your-capstone",level:2}];function d(e){const s={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.header,{children:(0,r.jsx)(s.h1,{id:"case-studies-real-world-multi-sensor-integration",children:"Case Studies: Real-World Multi-Sensor Integration"})}),"\n",(0,r.jsx)(s.p,{children:"These case studies showcase how leading humanoid robots integrate camera systems, depth sensors, IMUs, and sensor fusion to solve real-world challenges. Use these examples as design inspiration for your capstone project, not as blueprints to copy. Each robot made specific trade-offs based on their operational requirements, cost constraints, and technical priorities."}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"-case-study-1-boston-dynamics-atlas---multi-sensor-slam-for-dynamic-environments",children:"\ud83d\udcca Case Study 1: Boston Dynamics Atlas - Multi-Sensor SLAM for Dynamic Environments"}),"\n",(0,r.jsx)(s.h3,{id:"robot-overview",children:"Robot Overview"}),"\n",(0,r.jsx)(s.p,{children:"Boston Dynamics Atlas is designed for navigation and manipulation in disaster response, construction sites, and unstructured outdoor environments. Atlas must handle rough terrain, dynamic obstacles (moving debris, humans), and perform complex whole-body manipulation tasks like lifting objects and opening doors."}),"\n",(0,r.jsx)(s.h3,{id:"sensor-configuration",children:"Sensor Configuration"}),"\n",(0,r.jsxs)(s.table,{children:[(0,r.jsx)(s.thead,{children:(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Sensor Type"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Model/Technology"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Specifications"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Placement"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Update Rate"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Estimated Cost"})})]})}),(0,r.jsxs)(s.tbody,{children:[(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"Stereo Cameras"}),(0,r.jsx)(s.td,{children:"Carnegie Robotics Multisense SL"}),(0,r.jsx)(s.td,{children:"2MP CMOS, Bayer filter, 60\xb0 FOV"}),(0,r.jsx)(s.td,{children:"Head (pan-tilt mount)"}),(0,r.jsx)(s.td,{children:"30 FPS"}),(0,r.jsx)(s.td,{children:"$15,000+"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"LiDAR"}),(0,r.jsx)(s.td,{children:"Hokuyo UTM-30LX-EW (in Multisense SL)"}),(0,r.jsx)(s.td,{children:"270\xb0 planar, 30m range, \xb130mm accuracy"}),(0,r.jsx)(s.td,{children:"Head (rotating spindle at 30 RPM)"}),(0,r.jsx)(s.td,{children:"40 Hz (1,081 points/scan)"}),(0,r.jsx)(s.td,{children:"Included in Multisense"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"RGB-D Depth Camera"}),(0,r.jsx)(s.td,{children:"Time-of-Flight (ToF) sensors"}),(0,r.jsx)(s.td,{children:"640\xd7480 depth resolution, 0.5-5m range"}),(0,r.jsx)(s.td,{children:"Chest/torso (fixed)"}),(0,r.jsx)(s.td,{children:"30 Hz"}),(0,r.jsx)(s.td,{children:"$500-1,500"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"IMU"}),(0,r.jsx)(s.td,{children:"High-precision MEMS IMU (9-axis)"}),(0,r.jsx)(s.td,{children:"Accelerometer, gyroscope, magnetometer"}),(0,r.jsx)(s.td,{children:"Torso (center of mass)"}),(0,r.jsx)(s.td,{children:"1,000 Hz"}),(0,r.jsx)(s.td,{children:"$2,000-5,000"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"Joint Encoders"}),(0,r.jsx)(s.td,{children:"Absolute and incremental encoders"}),(0,r.jsx)(s.td,{children:"Sub-degree accuracy, 28 DOF"}),(0,r.jsx)(s.td,{children:"All joints (28 total)"}),(0,r.jsx)(s.td,{children:"4,000 Hz"}),(0,r.jsx)(s.td,{children:"$500-1,000 each"})]})]})]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Total Sensor Suite Cost"}),": Approximately $25,000-35,000 (excluding joint encoders)"]}),"\n",(0,r.jsx)(s.h3,{id:"integration-approach",children:"Integration Approach"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Sensor Fusion Algorithm"}),": Atlas uses a tightly-coupled LiDAR-Inertial-Visual SLAM system that fuses:"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"LiDAR point clouds"})," (from rotating Hokuyo) for 3D structure mapping and obstacle detection at 5-30m range"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Stereo camera images"})," for visual features, texture-based localization, and object recognition"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"IMU data"})," at 1 kHz for orientation tracking, motion prediction, and dynamic balance control"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Joint encoder feedback"})," at 4 kHz for proprioceptive state estimation (limb positions, center-of-mass calculation)"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"ROS2 Integration"})," (conceptual, Atlas uses proprietary middleware but principles apply):"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/multisense/left/image_raw"})," (sensor_msgs/Image): Left stereo camera for visual odometry"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/multisense/lidar_scan"})," (sensor_msgs/LaserScan): Planar LiDAR scan, rotated to build 3D point cloud"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/torso/imu"})," (sensor_msgs/Imu): High-rate orientation, angular velocity, linear acceleration"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/joint_states"})," (sensor_msgs/JointState): 28 DOF encoder positions for balance computation"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/slam/odometry"})," (nav_msgs/Odometry): Fused state estimate (position, orientation, velocity) from EKF"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Key Design Decision"}),": The Multisense SL sensor head combines stereo cameras and a rotating LiDAR in a single unit, simplifying calibration and reducing sensor mounting complexity. The rotating LiDAR builds a 3D point cloud by sweeping a 2D scan line, achieving 360\xb0 coverage with mechanical rotation."]}),"\n",(0,r.jsx)(s.h3,{id:"lessons-applied",children:"Lessons Applied"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"From Lesson 1 (Camera Systems)"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Stereo vision"})," provides passive depth estimation for nearby obstacles (0.5-10m) without IR interference in outdoor sunlight"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Active perception"}),': Pan-tilt head allows Atlas to "look" at objects of interest, adjusting FOV dynamically']}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"From Lesson 2 (Depth Sensing)"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"LiDAR for outdoor reliability"}),": Unlike RGB-D sensors, LiDAR works in direct sunlight and provides long-range (30m) obstacle detection"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Rotating vs. solid-state"}),": Mechanical rotation achieves 360\xb0 coverage with a single planar LiDAR, trading mechanical complexity for cost savings vs. 3D LiDAR ($100k+)"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"From Lesson 3 (IMU and Proprioception)"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"High-rate IMU (1 kHz)"})," enables dynamic balance control during parkour, backflips, and rough terrain walking"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Joint encoders as proprioception"}),": 4 kHz encoder feedback provides accurate limb position for center-of-mass estimation, critical for balance"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"From Lesson 4 (Sensor Fusion)"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Tightly-coupled SLAM"}),": LiDAR, camera, and IMU data are fused in a single optimization framework, improving robustness vs. loosely-coupled systems"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Redundancy strategy"}),": If camera fails (e.g., mud on lens), LiDAR + IMU provide basic navigation; if LiDAR fails, stereo vision + IMU enable short-range operation"]}),"\n"]}),"\n",(0,r.jsx)(s.h3,{id:"design-insights",children:"Design Insights"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Sensor Placement Philosophy"}),': Head-mounted sensors (Multisense SL) provide wide FOV and active perception via pan-tilt, but create occlusion when manipulating objects near the chest. Atlas supplements with chest-mounted ToF depth camera for "blind spot" coverage during manipulation tasks.']}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Failure Handling"}),": Atlas prioritizes redundancy for safety-critical tasks:"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Dual depth modalities"})," (stereo + LiDAR): If one fails, the other provides fallback"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"IMU drift correction"}),": Visual and LiDAR loop closure re-initializes IMU bias every 10-30 seconds"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Emergency stop"}),": If all perception fails, IMU-only balance control attempts safe shutdown (crouch, then sit)"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Relevance to Capstone Scenarios"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Scenario B (Outdoor Delivery)"}),": Atlas' LiDAR + stereo camera approach is ideal for outdoor navigation with long-range obstacle detection"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Scenario D (Warehouse)"}),": Multi-sensor redundancy ensures high reliability for safety-critical environments"]}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"-case-study-2-agility-robotics-digit---visual-inertial-odometry-for-gps-free-indoor-navigation",children:"\ud83d\udcca Case Study 2: Agility Robotics Digit - Visual-Inertial Odometry for GPS-Free Indoor Navigation"}),"\n",(0,r.jsx)(s.h3,{id:"robot-overview-1",children:"Robot Overview"}),"\n",(0,r.jsx)(s.p,{children:"Agility Robotics Digit is designed for warehouse logistics and last-mile delivery in indoor and semi-structured outdoor environments (sidewalks, building entrances). Digit prioritizes autonomous navigation without GPS, using Visual-Inertial Odometry (VIO) for localization. The robot must carry packages up to 16 kg while navigating through doorways, elevators, and around human workers."}),"\n",(0,r.jsx)(s.h3,{id:"sensor-configuration-1",children:"Sensor Configuration"}),"\n",(0,r.jsxs)(s.table,{children:[(0,r.jsx)(s.thead,{children:(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Sensor Type"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Model/Technology"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Specifications"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Placement"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Update Rate"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Estimated Cost"})})]})}),(0,r.jsxs)(s.tbody,{children:[(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"RGB-D Depth Cameras"}),(0,r.jsx)(s.td,{children:"Intel RealSense D435i (\xd74)"}),(0,r.jsx)(s.td,{children:"Stereo + IR depth, 1280\xd7720 RGB, 1280\xd7720 depth, IMU integrated"}),(0,r.jsx)(s.td,{children:"Head (forward), torso (left/right/rear)"}),(0,r.jsx)(s.td,{children:"30 FPS (RGB), 90 FPS (depth)"}),(0,r.jsx)(s.td,{children:"$200-300 each ($800-1,200 total)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"LiDAR"}),(0,r.jsx)(s.td,{children:"2D planar LiDAR (model unspecified)"}),(0,r.jsx)(s.td,{children:"270\xb0 FOV, 30m range"}),(0,r.jsx)(s.td,{children:"Chest (horizontal plane)"}),(0,r.jsx)(s.td,{children:"10-20 Hz"}),(0,r.jsx)(s.td,{children:"$1,500-5,000"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"IMU"}),(0,r.jsx)(s.td,{children:"Bosch BMI055 (integrated in D435i) + MEMS IMU"}),(0,r.jsx)(s.td,{children:"6-axis (accel + gyro), \xb116g, \xb12000\xb0/s"}),(0,r.jsx)(s.td,{children:"Torso (center) + cameras"}),(0,r.jsx)(s.td,{children:"200 Hz (torso), 200 Hz (D435i)"}),(0,r.jsx)(s.td,{children:"$50-100 (integrated in D435i)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"Absolute Encoders"}),(0,r.jsx)(s.td,{children:"Joint position sensors"}),(0,r.jsx)(s.td,{children:"Proprioceptive feedback for 20 DOF legs/arms"}),(0,r.jsx)(s.td,{children:"All leg and arm joints"}),(0,r.jsx)(s.td,{children:"100+ Hz"}),(0,r.jsx)(s.td,{children:"$300-500 each"})]})]})]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Total Sensor Suite Cost"}),": Approximately $5,000-10,000 (excluding encoders)"]}),"\n",(0,r.jsx)(s.h3,{id:"integration-approach-1",children:"Integration Approach"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Sensor Fusion Algorithm"}),": Digit uses ",(0,r.jsx)(s.strong,{children:"Visual-Inertial Odometry (VIO)"})," combining:"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Four Intel RealSense D435i cameras"})," provide 360\xb0 RGB-D coverage and integrated IMU data"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Stereo visual features"})," from multiple cameras enable robust visual odometry with redundancy (if one camera fails, others compensate)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"IMU data"})," fuses with visual odometry to handle rapid motion, camera occlusion, and reduce drift during dynamic walking"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"LiDAR"})," supplements for obstacle detection and loop closure detection in large warehouses"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"ROS2 Integration"})," (Digit uses proprietary software, but ROS2 equivalent would be):"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/camera/front/color/image_raw"}),", ",(0,r.jsx)(s.code,{children:"/camera/left/color/image_raw"}),", etc. (sensor_msgs/Image): 4 cameras for multi-view VIO"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/camera/front/depth/image_rect_raw"})," (sensor_msgs/Image): Depth images for obstacle avoidance"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/camera/front/imu"})," (sensor_msgs/Imu): Integrated IMU from D435i cameras"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/torso/imu"})," (sensor_msgs/Imu): High-precision torso IMU for balance control"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/scan"})," (sensor_msgs/LaserScan): 2D LiDAR for obstacle detection"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/odometry/vio"})," (nav_msgs/Odometry): Fused VIO output combining cameras + IMU"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Key Design Decision"}),": Digit uses ",(0,r.jsx)(s.strong,{children:"four RealSense D435i cameras"})," instead of a single high-end camera or 3D LiDAR. This provides:"]}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Redundancy"}),": If one camera's view is blocked (e.g., carrying a large package), others maintain localization"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"360\xb0 coverage"}),": Front, left, right, rear cameras eliminate blind spots"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Cost efficiency"}),": Four $250 cameras ($1,000 total) vs. one 3D LiDAR ($8,000+)"]}),"\n"]}),"\n",(0,r.jsx)(s.h3,{id:"lessons-applied-1",children:"Lessons Applied"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"From Lesson 1 (Camera Systems)"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Multi-camera visual odometry"}),": Tracks features across overlapping camera views, increasing robustness vs. single stereo pair"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"RGB-D integration"}),": D435i combines RGB camera with active IR stereo depth, enabling operation in low-light warehouses"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"From Lesson 2 (Depth Sensing)"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Active IR stereo"}),": D435i projects IR pattern for texture-less surfaces (white walls, uniform floors), overcoming passive stereo limitations"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Depth + LiDAR complementarity"}),": RGB-D provides dense depth (0.3-3m) for close-range manipulation; LiDAR provides sparse long-range (5-30m) for navigation"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"From Lesson 3 (IMU and Proprioception)"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"VIO requires tight camera-IMU synchronization"}),": D435i integrates IMU on camera PCB, ensuring hardware time-synchronization (<1 ms jitter)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Balance control vs. navigation IMU"}),": Torso IMU (200 Hz) for dynamic balance; camera IMUs (200 Hz) for VIO"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"From Lesson 4 (Sensor Fusion)"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"VIO (Visual-Inertial Odometry)"}),": Fuses visual features with IMU using Extended Kalman Filter (EKF) or graph optimization (e.g., VINS-Mono algorithm)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Loop closure"}),": When Digit revisits a location, visual recognition triggers re-localization, correcting accumulated drift"]}),"\n"]}),"\n",(0,r.jsx)(s.h3,{id:"design-insights-1",children:"Design Insights"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"GPS-Free Localization Philosophy"}),": Digit operates in environments where GPS is unavailable (indoor warehouses, building interiors). VIO provides drift-bounded localization:"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Short-term (<1 min)"}),": IMU-predicted motion between camera frames (<0.1% drift)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Medium-term (1-10 min)"}),": Visual odometry bounds IMU drift to <1% of distance traveled"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Long-term (10+ min)"}),": Loop closure re-initializes position when revisiting known areas"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Failure Handling"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Single camera failure"}),": VIO continues with remaining 3 cameras at reduced accuracy (\xb15 cm \u2192 \xb110 cm localization error)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"IMU drift"}),": If visual features are lost (e.g., uniform white hallway), IMU dead-reckoning for <5 seconds until features reappear"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Complete vision failure"}),": Fall back to joint encoders + IMU for basic balance, stop autonomous navigation, request manual intervention"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Sensor Placement Strategy"}),": RealSense cameras placed at torso height (not head) to minimize occlusion during package carrying. Forward and side cameras provide frontal FOV overlap for robust stereo matching."]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Relevance to Capstone Scenarios"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Scenario A (Home Assistant)"}),": Digit's VIO approach ideal for GPS-free indoor navigation with cost-effective RGB-D cameras"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Scenario C (Human Interaction)"}),": Multi-camera setup provides 360\xb0 awareness for safe human proximity"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Scenario D (Warehouse)"}),": Proven VIO in real warehouse deployments (Amazon, GXO Logistics)"]}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"-case-study-3-tesla-optimus---multi-camera-fusion-for-cost-optimized-manipulation",children:"\ud83d\udcca Case Study 3: Tesla Optimus - Multi-Camera Fusion for Cost-Optimized Manipulation"}),"\n",(0,r.jsx)(s.h3,{id:"robot-overview-2",children:"Robot Overview"}),"\n",(0,r.jsx)(s.p,{children:"Tesla Optimus (also known as Tesla Bot) is designed for general-purpose humanoid tasks in factories, homes, and service environments. Optimus prioritizes cost reduction by leveraging Tesla's automotive Full Self-Driving (FSD) computer and camera-only perception (no LiDAR). The robot must perform manipulation tasks (assembly, picking, placing) and navigate indoor/outdoor environments."}),"\n",(0,r.jsx)(s.h3,{id:"sensor-configuration-2",children:"Sensor Configuration"}),"\n",(0,r.jsxs)(s.table,{children:[(0,r.jsx)(s.thead,{children:(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Sensor Type"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Model/Technology"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Specifications"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Placement"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Update Rate"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Estimated Cost"})})]})}),(0,r.jsxs)(s.tbody,{children:[(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"Head Cameras"}),(0,r.jsx)(s.td,{children:"8\xd7 automotive-grade cameras"}),(0,r.jsx)(s.td,{children:"1280\xd7960 resolution, wide/standard/narrow FOV mix"}),(0,r.jsx)(s.td,{children:"Head (3\xd7 forward, 2\xd7 side, 1\xd7 rear, 2\xd7 downward)"}),(0,r.jsx)(s.td,{children:"30-60 FPS"}),(0,r.jsx)(s.td,{children:"$50-200 each ($400-1,600 total)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"Wrist Cameras"}),(0,r.jsx)(s.td,{children:"2\xd7 high-resolution cameras"}),(0,r.jsx)(s.td,{children:"1920\xd71080 or higher for manipulation"}),(0,r.jsx)(s.td,{children:"Wrists (gripper-mounted)"}),(0,r.jsx)(s.td,{children:"30 FPS"}),(0,r.jsx)(s.td,{children:"$100-300 each ($200-600 total)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"IMU"}),(0,r.jsx)(s.td,{children:"6-axis MEMS IMU"}),(0,r.jsx)(s.td,{children:"\xb18g accel, \xb11000\xb0/s gyro"}),(0,r.jsx)(s.td,{children:"Torso (center of mass)"}),(0,r.jsx)(s.td,{children:"100-200 Hz"}),(0,r.jsx)(s.td,{children:"$20-50"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"Joint Force/Torque Sensors"}),(0,r.jsx)(s.td,{children:"Actuator-integrated sensors"}),(0,r.jsx)(s.td,{children:"Measure joint torque for compliant control"}),(0,r.jsx)(s.td,{children:"All 28+ joints"}),(0,r.jsx)(s.td,{children:"200+ Hz"}),(0,r.jsx)(s.td,{children:"Integrated in actuators"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:"Foot Force Sensors"}),(0,r.jsx)(s.td,{children:"Pressure sensors or load cells"}),(0,r.jsx)(s.td,{children:"Ground contact force for gait stability"}),(0,r.jsx)(s.td,{children:"Both feet"}),(0,r.jsx)(s.td,{children:"200+ Hz"}),(0,r.jsx)(s.td,{children:"$50-200 each"})]})]})]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Total Sensor Suite Cost"}),": Approximately $1,000-3,000 (excluding actuator-integrated sensors)"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Notable Omission"}),": ",(0,r.jsx)(s.strong,{children:"No LiDAR or dedicated depth sensors"}),". Depth estimated via monocular depth prediction neural networks."]}),"\n",(0,r.jsx)(s.h3,{id:"integration-approach-2",children:"Integration Approach"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Sensor Fusion Algorithm"}),": Optimus uses ",(0,r.jsx)(s.strong,{children:"multi-camera neural network fusion"})," running on Tesla FSD computer:"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Eight automotive cameras"})," provide overlapping 360\xb0 coverage for navigation and obstacle avoidance"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Monocular depth estimation"}),": Neural networks trained on millions of driving scenarios predict depth from single camera images"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Multi-camera occupancy grid"}),": Fuses 8 camera views into 3D occupancy map (similar to Tesla's BEV - Bird's Eye View - representation for cars)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Wrist cameras"})," for manipulation: Visual servoing during grasping, object pose estimation"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"IMU + vision fusion"}),": IMU provides motion prediction between camera frames; neural network compensates for camera motion"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"ROS2 Integration"})," (conceptual for educational purposes):"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/camera/head_forward_wide/image_raw"}),", ",(0,r.jsx)(s.code,{children:"/camera/head_forward_narrow/image_raw"}),", etc. (sensor_msgs/Image): 8 head cameras"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/camera/wrist_left/image_raw"}),", ",(0,r.jsx)(s.code,{children:"/camera/wrist_right/image_raw"})," (sensor_msgs/Image): Gripper-mounted cameras"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/imu/data"})," (sensor_msgs/Imu): Torso IMU for balance and motion prediction"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/joint_states"})," (sensor_msgs/JointState): Actuator positions and torques"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/depth/prediction"})," (sensor_msgs/Image): Neural network monocular depth prediction (not sensor_msgs/PointCloud2, as depth is estimated, not measured)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.code,{children:"/perception/occupancy_grid"})," (nav_msgs/OccupancyGrid): Fused 3D obstacle map from multi-camera neural network"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Key Design Decision"}),": ",(0,r.jsx)(s.strong,{children:"Vision-only perception"})," (no LiDAR) reduces sensor cost from $8,000-20,000 (3D LiDAR) to <$2,000 (cameras only). Trade-off: depth estimation is less accurate (\xb110-20 cm at 5m vs. \xb12 cm for LiDAR) but sufficient for navigation and manipulation tasks in structured environments."]}),"\n",(0,r.jsx)(s.h3,{id:"lessons-applied-2",children:"Lessons Applied"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"From Lesson 1 (Camera Systems)"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Multi-camera stereo"}),": Overlapping FOV cameras enable stereo depth triangulation (e.g., forward-wide + forward-narrow cameras)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Wrist-mounted cameras"}),": Eye-in-hand configuration provides unoccluded view of grasped objects until contact"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"From Lesson 2 (Depth Sensing)"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Monocular depth neural networks"}),": Trained on massive datasets (Tesla FSD data), predict depth without structured light or ToF hardware"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Trade-off acceptance"}),": Lower depth accuracy acceptable for navigation (collision avoidance requires ~10 cm precision, not <1 cm)"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"From Lesson 3 (IMU and Proprioception)"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Force/torque sensing"}),": Joint actuators measure applied forces, enabling compliant manipulation (e.g., gentle handoff, adaptive grip)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Foot force sensors"}),": Ground reaction forces for zero-moment point (ZMP) balance control"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"From Lesson 4 (Sensor Fusion)"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Neural network as fusion algorithm"}),": Instead of traditional EKF/UKF, Tesla uses transformers to fuse 8 camera views + IMU into unified 3D occupancy representation"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Temporal fusion"}),": Neural network processes camera sequences (not single frames), implicitly learning visual odometry"]}),"\n"]}),"\n",(0,r.jsx)(s.h3,{id:"design-insights-2",children:"Design Insights"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Cost Reduction Philosophy"}),": Optimus targets consumer/factory price points ($20,000-30,000 estimated), requiring aggressive sensor cost reduction:"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"No LiDAR"}),": Saves $8,000-20,000 per robot"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Automotive-grade cameras"}),": Leverage Tesla's supply chain ($50-200 per camera vs. $500-2,000 for robotics-grade cameras)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"FSD computer reuse"}),": Amortizes R&D costs across automotive and robotics platforms"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Wrist Camera Benefits"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Unoccluded view"}),": Unlike head cameras, wrist cameras maintain view of object until gripper closes"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Close-range precision"}),": 1080p cameras at 0.2-0.5m provide sub-millimeter pixel resolution for grasp refinement"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Lighting control"}),": Wrist-mounted LED ring illuminates workspace, reducing sensitivity to ambient lighting"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Failure Handling"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Camera redundancy"}),": 8 head cameras mean 1-2 camera failures still allow degraded navigation (7 cameras \u2192 reduced FOV coverage)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Monocular depth uncertainty"}),": Neural network outputs confidence scores; low-confidence depth estimates trigger cautious behavior (slow down, use other cameras)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"IMU drift"}),": Visual odometry from multiple cameras re-initializes IMU bias every 1-2 seconds"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Computational Strategy"}),": FSD computer (custom ASIC) runs neural networks at 144 TOPS (tera operations per second), enabling real-time processing of 8 camera streams + depth prediction + object detection. Lower-cost robots cannot afford this compute power, making LiDAR + simpler algorithms more practical."]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Relevance to Capstone Scenarios"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Scenario A (Home Assistant)"}),": Vision-only approach viable for indoor tasks if compute budget allows neural network depth estimation"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Scenario C (Human Interaction)"}),": Wrist cameras essential for safe object handoff and manipulation"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Cost-constrained designs"}),": Demonstrates trade-off between sensor hardware (LiDAR) vs. computation (neural networks for depth estimation)"]}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"-case-study-4-comparative-analysis---sensor-suite-trade-offs",children:"\ud83d\udcca Case Study 4: Comparative Analysis - Sensor Suite Trade-offs"}),"\n",(0,r.jsx)(s.p,{children:"This table summarizes key design decisions across the three case studies, highlighting how different operational requirements drive sensor selection."}),"\n",(0,r.jsxs)(s.table,{children:[(0,r.jsx)(s.thead,{children:(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Criterion"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Atlas (Boston Dynamics)"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Digit (Agility Robotics)"})}),(0,r.jsx)(s.th,{children:(0,r.jsx)(s.strong,{children:"Optimus (Tesla)"})})]})}),(0,r.jsxs)(s.tbody,{children:[(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Primary Environment"})}),(0,r.jsx)(s.td,{children:"Outdoor/disaster (unstructured)"}),(0,r.jsx)(s.td,{children:"Indoor warehouse (semi-structured)"}),(0,r.jsx)(s.td,{children:"Indoor factory/home (structured)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Depth Technology"})}),(0,r.jsx)(s.td,{children:"LiDAR (rotating 2D) + Stereo"}),(0,r.jsx)(s.td,{children:"RGB-D (active IR stereo) + LiDAR"}),(0,r.jsx)(s.td,{children:"Monocular depth neural networks"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Depth Sensor Cost"})}),(0,r.jsx)(s.td,{children:"$15,000+ (Multisense SL)"}),(0,r.jsx)(s.td,{children:"$1,000-1,500 (4\xd7 RealSense D435i + LiDAR)"}),(0,r.jsx)(s.td,{children:"$0 (cameras only, depth estimated)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Camera Count"})}),(0,r.jsx)(s.td,{children:"2 (stereo pair)"}),(0,r.jsx)(s.td,{children:"8 (4\xd7 RGB-D cameras = 8 image streams)"}),(0,r.jsx)(s.td,{children:"10 (8 head + 2 wrist)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"IMU Update Rate"})}),(0,r.jsx)(s.td,{children:"1,000 Hz (high-precision)"}),(0,r.jsx)(s.td,{children:"200 Hz (torso + integrated)"}),(0,r.jsx)(s.td,{children:"100-200 Hz (standard MEMS)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Fusion Algorithm"})}),(0,r.jsx)(s.td,{children:"Tightly-coupled LiDAR-Visual-Inertial SLAM"}),(0,r.jsx)(s.td,{children:"VIO (Visual-Inertial Odometry) with loop closure"}),(0,r.jsx)(s.td,{children:"Multi-camera neural network fusion (transformer-based)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Localization Drift"})}),(0,r.jsx)(s.td,{children:"<0.1% (LiDAR + loop closure)"}),(0,r.jsx)(s.td,{children:"<1% (VIO with loop closure)"}),(0,r.jsx)(s.td,{children:"<2% (monocular depth less accurate)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Compute Platform"})}),(0,r.jsx)(s.td,{children:"Custom high-performance CPU/GPU"}),(0,r.jsx)(s.td,{children:"Dual Intel i7 CPUs + optional Jetson"}),(0,r.jsx)(s.td,{children:"Tesla FSD computer (144 TOPS custom ASIC)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Estimated Sensor Cost"})}),(0,r.jsx)(s.td,{children:"$25,000-35,000"}),(0,r.jsx)(s.td,{children:"$5,000-10,000"}),(0,r.jsx)(s.td,{children:"$1,000-3,000"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Cost Optimization"})}),(0,r.jsx)(s.td,{children:"Performance-first (disaster response requires reliability)"}),(0,r.jsx)(s.td,{children:"Balanced (warehouse profitability requires cost control)"}),(0,r.jsx)(s.td,{children:"Cost-first (consumer/factory scale requires low price)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Failure Redundancy"})}),(0,r.jsx)(s.td,{children:"Dual depth modalities (stereo + LiDAR)"}),(0,r.jsx)(s.td,{children:"Multi-camera redundancy (4 cameras, lose 1-2 OK)"}),(0,r.jsx)(s.td,{children:"8 head cameras, partial failure tolerated"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Outdoor Sunlight"})}),(0,r.jsx)(s.td,{children:"LiDAR handles sunlight well"}),(0,r.jsx)(s.td,{children:"RGB-D IR pattern may wash out, relies on LiDAR fallback"}),(0,r.jsx)(s.td,{children:"Monocular depth neural networks handle sunlight (trained on outdoor driving)"})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.strong,{children:"Applicable Capstone Scenarios"})}),(0,r.jsx)(s.td,{children:"Scenario B (Outdoor Delivery), Scenario D (Warehouse)"}),(0,r.jsx)(s.td,{children:"Scenario A (Home Assistant), Scenario D (Warehouse)"}),(0,r.jsx)(s.td,{children:"Scenario A (Home Assistant), Scenario C (Human Interaction)"})]})]})]}),"\n",(0,r.jsx)(s.h3,{id:"key-takeaway-for-capstone-design",children:"Key Takeaway for Capstone Design"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:'There is no "best" sensor suite\u2014only appropriate trade-offs for specific scenarios'}),":"]}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"If outdoor operation is required"})," (Scenario B): LiDAR is nearly mandatory due to sunlight interference with IR-based RGB-D sensors. Atlas demonstrates this principle."]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"If indoor GPS-free navigation is required"})," (Scenarios A, C, D): VIO (camera + IMU fusion) provides drift-bounded localization. Digit proves this works at commercial scale."]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"If cost is paramount and compute is available"})," (Scenario A with consumer budget): Monocular depth estimation can replace hardware depth sensors, as Optimus demonstrates. However, this requires significant neural network training and inference compute."]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"If manipulation precision is critical"})," (Scenarios A, C): Wrist-mounted cameras (Optimus approach) provide unoccluded view of objects during grasping. Head cameras alone create blind spots when arms reach forward."]}),"\n"]}),"\n",(0,r.jsxs)(s.li,{children:["\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"If safety is critical"})," (Scenario C: human interaction): Redundant depth sensing (e.g., Atlas' stereo + LiDAR) ensures zero collision risk even if one sensor fails."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:["When designing your capstone project, ",(0,r.jsx)(s.strong,{children:"justify sensor choices with explicit trade-offs"})," (cost vs. accuracy, indoor vs. outdoor, compute vs. hardware). Reference these case studies to demonstrate awareness of real-world design decisions."]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"summary-applying-case-studies-to-your-capstone",children:"Summary: Applying Case Studies to Your Capstone"}),"\n",(0,r.jsxs)(s.p,{children:["Use these case studies as ",(0,r.jsx)(s.strong,{children:"conceptual references"}),", not blueprints:"]}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Atlas shows"}),": When reliability and outdoor operation are paramount, invest in LiDAR and redundant sensors (Scenarios B, D)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Digit shows"}),": VIO with multiple RGB-D cameras is viable for indoor logistics at moderate cost (Scenarios A, D)"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Optimus shows"}),": Vision-only approaches can work if neural network compute is available and accuracy requirements are relaxed (Scenarios A, C)"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Design Process"}),":"]}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsx)(s.li,{children:"Identify your scenario constraints (indoor vs. outdoor, cost budget, precision requirements)"}),"\n",(0,r.jsx)(s.li,{children:"Map constraints to sensor technologies from Lessons 1-4"}),"\n",(0,r.jsx)(s.li,{children:'Reference case studies for real-world validation (e.g., "Digit uses VIO for warehouse navigation, so VIO is appropriate for Scenario D")'}),"\n",(0,r.jsx)(s.li,{children:'Justify trade-offs explicitly (e.g., "Chose stereo cameras over RGB-D because Scenario B requires outdoor operation where sunlight interferes with IR depth sensors, as seen in Atlas\' LiDAR-first design")'}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"What Makes a Strong Capstone Design"}),":"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["Sensor choices ",(0,r.jsx)(s.strong,{children:"reference specific limitations"}),' from Lessons 1-4 (e.g., "RGB-D fails on glass surfaces, Lesson 2 Section 3.3")']}),"\n",(0,r.jsxs)(s.li,{children:["Fusion strategy ",(0,r.jsx)(s.strong,{children:"matches sensor characteristics"}),' (e.g., "VIO appropriate for camera + IMU, not LiDAR + magnetometer")']}),"\n",(0,r.jsxs)(s.li,{children:["Failure modes ",(0,r.jsx)(s.strong,{children:"demonstrate awareness"}),' of real-world issues (e.g., "Atlas handles camera mud occlusion with LiDAR fallback")']}),"\n",(0,r.jsxs)(s.li,{children:["ROS2 architecture is ",(0,r.jsx)(s.strong,{children:"realistic and implementable"})," (message types, topic names, QoS policies correct)"]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:["These case studies provide the ",(0,r.jsx)(s.strong,{children:"evidence base"})," for your design justifications. When explaining sensor selection, cite these robots as proof that your approach works in practice."]})]})}function h(e={}){const{wrapper:s}={...(0,o.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>t,x:()=>a});var i=n(6540);const r={},o=i.createContext(r);function t(e){const s=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),i.createElement(o.Provider,{value:s},e.children)}}}]);